[
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.deregister_matplotlib_converters # pandas.plotting. deregister_matplotlib_converters ( ) [source] # Remove pandas formatters and converters. Removes the custom converters added by register() . This attempts to set the state of the registry back to the state before pandas registered its own units. Converters for pandasâ own types like Timestamp and Period are removed completely. Converters for types pandas overwrites, like datetime.datetime , are restored to their original value. See also register_matplotlib_converters Register pandas formatters and converters with matplotlib. Examples The following line is done automatically by pandas so the plot can be rendered: >>> pd . plotting . register_matplotlib_converters () >>> df = pd . DataFrame ({ 'ts' : pd . period_range ( '2020' , periods = 2 , freq = 'M' ), ... 'y' : [ 1 , 2 ] ... }) >>> plot = df . plot . line ( x = 'ts' , y = 'y' ) Unsetting the register manually an error will be raised: >>> pd . set_option ( \"plotting.matplotlib.register_converters\" , ... False ) >>> df . plot . line ( x = 'ts' , y = 'y' ) Traceback (most recent call last): TypeError : float() argument must be a string or a real number, not 'Period' previous pandas.plotting.boxplot next pandas.plotting.lag_plot On this page deregister_matplotlib_converters() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.deregister_matplotlib_converters.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Cookbook Cookbook # This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request . Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users. Idioms # These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: In [1]: df = pd . DataFrame ( ...: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ...: ) ...: In [2]: df Out[2]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 if-thenâ¦ # An if-then on one column In [3]: df . loc [ df . AAA >= 5 , \"BBB\" ] = - 1 In [4]: df Out[4]: AAA BBB CCC 0 4 10 100 1 5 -1 50 2 6 -1 -30 3 7 -1 -50 An if-then with assignment to 2 columns: In [5]: df . loc [ df . AAA >= 5 , [ \"BBB\" , \"CCC\" ]] = 555 In [6]: df Out[6]: AAA BBB CCC 0 4 10 100 1 5 555 555 2 6 555 555 3 7 555 555 Add another line with different logic, to do the -else In [7]: df . loc [ df . AAA < 5 , [ \"BBB\" , \"CCC\" ]] = 2000 In [8]: df Out[8]: AAA BBB CCC 0 4 2000 2000 1 5 555 555 2 6 555 555 3 7 555 555 Or use pandas where after youâve set up a mask In [9]: df_mask = pd . DataFrame ( ...: { \"AAA\" : [ True ] * 4 , \"BBB\" : [ False ] * 4 , \"CCC\" : [ True , False ] * 2 } ...: ) ...: In [10]: df . where ( df_mask , - 1000 ) Out[10]: AAA BBB CCC 0 4 -1000 2000 1 5 -1000 -1000 2 6 -1000 555 3 7 -1000 -1000 if-then-else using NumPyâs where() In [11]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ....: ) ....: In [12]: df Out[12]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 In [13]: df [ \"logic\" ] = np . where ( df [ \"AAA\" ] > 5 , \"high\" , \"low\" ) In [14]: df Out[14]: AAA BBB CCC logic 0 4 10 100 low 1 5 20 50 low 2 6 30 -30 high 3 7 40 -50 high Splitting # Split a frame with a boolean criterion In [15]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ....: ) ....: In [16]: df Out[16]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 In [17]: df [ df . AAA <= 5 ] Out[17]: AAA BBB CCC 0 4 10 100 1 5 20 50 In [18]: df [ df . AAA > 5 ] Out[18]: AAA BBB CCC 2 6 30 -30 3 7 40 -50 Building criteria # Select with multi-column criteria In [19]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ....: ) ....: In [20]: df Out[20]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 â¦and (without assignment returns a Series) In [21]: df . loc [( df [ \"BBB\" ] < 25 ) & ( df [ \"CCC\" ] >= - 40 ), \"AAA\" ] Out[21]: 0 4 1 5 Name: AAA, dtype: int64 â¦or (without assignment returns a Series) In [22]: df . loc [( df [ \"BBB\" ] > 25 ) | ( df [ \"CCC\" ] >= - 40 ), \"AAA\" ] Out[22]: 0 4 1 5 2 6 3 7 Name: AAA, dtype: int64 â¦or (with assignment modifies the DataFrame.) In [23]: df . loc [( df [ \"BBB\" ] > 25 ) | ( df [ \"CCC\" ] >= 75 ), \"AAA\" ] = 999 In [24]: df Out[24]: AAA BBB CCC 0 999 10 100 1 5 20 50 2 999 30 -30 3 999 40 -50 Select rows with data closest to certain value using argsort In [25]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ....: ) ....: In [26]: df Out[26]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 In [27]: aValue = 43.0 In [28]: df . loc [( df . CCC - aValue ) . abs () . argsort ()] Out[28]: AAA BBB CCC 1 5 20 50 0 4 10 100 2 6 30 -30 3 7 40 -50 Dynamically reduce a list of criteria using a binary operators In [29]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ....: ) ....: In [30]: df Out[30]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 In [31]: Crit1 = df . AAA <= 5.5 In [32]: Crit2 = df . BBB == 10.0 In [33]: Crit3 = df . CCC > - 40.0 One could hard code: In [34]: AllCrit = Crit1 & Crit2 & Crit3 â¦Or it can be done with a list of dynamically built criteria In [35]: import functools In [36]: CritList = [ Crit1 , Crit2 , Crit3 ] In [37]: AllCrit = functools . reduce ( lambda x , y : x & y , CritList ) In [38]: df [ AllCrit ] Out[38]: AAA BBB CCC 0 4 10 100 Selection # Dataframes # The indexing docs. Using both row labels and value conditionals In [39]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ....: ) ....: In [40]: df Out[40]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 In [41]: df [( df . AAA <= 6 ) & ( df . index . isin ([ 0 , 2 , 4 ]))] Out[41]: AAA BBB CCC 0 4 10 100 2 6 30 -30 Use loc for label-oriented slicing and iloc positional slicing GH 2904 In [42]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]}, ....: index = [ \"foo\" , \"bar\" , \"boo\" , \"kar\" ], ....: ) ....: There are 2 explicit slicing methods, with a third general case Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions) In [43]: df . loc [ \"bar\" : \"kar\" ] # Label Out[43]: AAA BBB CCC bar 5 20 50 boo 6 30 -30 kar 7 40 -50 # Generic In [44]: df [ 0 : 3 ] Out[44]: AAA BBB CCC foo 4 10 100 bar 5 20 50 boo 6 30 -30 In [45]: df [ \"bar\" : \"kar\" ] Out[45]: AAA BBB CCC bar 5 20 50 boo 6 30 -30 kar 7 40 -50 Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. In [46]: data = { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} In [47]: df2 = pd . DataFrame ( data = data , index = [ 1 , 2 , 3 , 4 ]) # Note index starts at 1. In [48]: df2 . iloc [ 1 : 3 ] # Position-oriented Out[48]: AAA BBB CCC 2 5 20 50 3 6 30 -30 In [49]: df2 . loc [ 1 : 3 ] # Label-oriented Out[49]: AAA BBB CCC 1 4 10 100 2 5 20 50 3 6 30 -30 Using inverse operator (~) to take the complement of a mask In [50]: df = pd . DataFrame ( ....: { \"AAA\" : [ 4 , 5 , 6 , 7 ], \"BBB\" : [ 10 , 20 , 30 , 40 ], \"CCC\" : [ 100 , 50 , - 30 , - 50 ]} ....: ) ....: In [51]: df Out[51]: AAA BBB CCC 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 In [52]: df [ ~ (( df . AAA <= 6 ) & ( df . index . isin ([ 0 , 2 , 4 ])))] Out[52]: AAA BBB CCC 1 5 20 50 3 7 40 -50 New columns # Efficiently and dynamically creating new columns using DataFrame.map (previously named applymap) In [53]: df = pd . DataFrame ({ \"AAA\" : [ 1 , 2 , 1 , 3 ], \"BBB\" : [ 1 , 1 , 2 , 2 ], \"CCC\" : [ 2 , 1 , 3 , 1 ]}) In [54]: df Out[54]: AAA BBB CCC 0 1 1 2 1 2 1 1 2 1 2 3 3 3 2 1 In [55]: source_cols = df . columns # Or some subset would work too In [56]: new_cols = [ str ( x ) + \"_cat\" for x in source_cols ] In [57]: categories = { 1 : \"Alpha\" , 2 : \"Beta\" , 3 : \"Charlie\" } In [58]: df [ new_cols ] = df [ source_cols ] . map ( categories . get ) In [59]: df Out[59]: AAA BBB CCC AAA_cat BBB_cat CCC_cat 0 1 1 2 Alpha Alpha Beta 1 2 1 1 Beta Alpha Alpha 2 1 2 3 Alpha Beta Charlie 3 3 2 1 Charlie Beta Alpha Keep other columns when using min() with groupby In [60]: df = pd . DataFrame ( ....: { \"AAA\" : [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 ], \"BBB\" : [ 2 , 1 , 3 , 4 , 5 , 1 , 2 , 3 ]} ....: ) ....: In [61]: df Out[61]: AAA BBB 0 1 2 1 1 1 2 1 3 3 2 4 4 2 5 5 2 1 6 3 2 7 3 3 Method 1 : idxmin() to get the index of the minimums In [62]: df . loc [ df . groupby ( \"AAA\" )[ \"BBB\" ] . idxmin ()] Out[62]: AAA BBB 1 1 1 5 2 1 6 3 2 Method 2 : sort then take first of each In [63]: df . sort_values ( by = \"BBB\" ) . groupby ( \"AAA\" , as_index = False ) . first () Out[63]: AAA BBB 0 1 1 1 2 1 2 3 2 Notice the same results, with the exception of the index. Multiindexing # The multindexing docs. Creating a MultiIndex from a labeled frame In [64]: df = pd . DataFrame ( ....: { ....: \"row\" : [ 0 , 1 , 2 ], ....: \"One_X\" : [ 1.1 , 1.1 , 1.1 ], ....: \"One_Y\" : [ 1.2 , 1.2 , 1.2 ], ....: \"Two_X\" : [ 1.11 , 1.11 , 1.11 ], ....: \"Two_Y\" : [ 1.22 , 1.22 , 1.22 ], ....: } ....: ) ....: In [65]: df Out[65]: row One_X One_Y Two_X Two_Y 0 0 1.1 1.2 1.11 1.22 1 1 1.1 1.2 1.11 1.22 2 2 1.1 1.2 1.11 1.22 # As Labelled Index In [66]: df = df . set_index ( \"row\" ) In [67]: df Out[67]: One_X One_Y Two_X Two_Y row 0 1.1 1.2 1.11 1.22 1 1.1 1.2 1.11 1.22 2 1.1 1.2 1.11 1.22 # With Hierarchical Columns In [68]: df . columns = pd . MultiIndex . from_tuples ([ tuple ( c . split ( \"_\" )) for c in df . columns ]) In [69]: df Out[69]: One Two X Y X Y row 0 1.1 1.2 1.11 1.22 1 1.1 1.2 1.11 1.22 2 1.1 1.2 1.11 1.22 # Now stack & Reset In [70]: df = df . stack ( 0 , future_stack = True ) . reset_index ( 1 ) In [71]: df Out[71]: level_1 X Y row 0 One 1.10 1.20 0 Two 1.11 1.22 1 One 1.10 1.20 1 Two 1.11 1.22 2 One 1.10 1.20 2 Two 1.11 1.22 # And fix the labels (Notice the label 'level_1' got added automatically) In [72]: df . columns = [ \"Sample\" , \"All_X\" , \"All_Y\" ] In [73]: df Out[73]: Sample All_X All_Y row 0 One 1.10 1.20 0 Two 1.11 1.22 1 One 1.10 1.20 1 Two 1.11 1.22 2 One 1.10 1.20 2 Two 1.11 1.22 Arithmetic # Performing arithmetic with a MultiIndex that needs broadcasting In [74]: cols = pd . MultiIndex . from_tuples ( ....: [( x , y ) for x in [ \"A\" , \"B\" , \"C\" ] for y in [ \"O\" , \"I\" ]] ....: ) ....: In [75]: df = pd . DataFrame ( np . random . randn ( 2 , 6 ), index = [ \"n\" , \"m\" ], columns = cols ) In [76]: df Out[76]: A B C O I O I O I n 0.469112 -0.282863 -1.509059 -1.135632 1.212112 -0.173215 m 0.119209 -1.044236 -0.861849 -2.104569 -0.494929 1.071804 In [77]: df = df . div ( df [ \"C\" ], level = 1 ) In [78]: df Out[78]: A B C O I O I O I n 0.387021 1.633022 -1.244983 6.556214 1.0 1.0 m -0.240860 -0.974279 1.741358 -1.963577 1.0 1.0 Slicing # Slicing a MultiIndex with xs In [79]: coords = [( \"AA\" , \"one\" ), ( \"AA\" , \"six\" ), ( \"BB\" , \"one\" ), ( \"BB\" , \"two\" ), ( \"BB\" , \"six\" )] In [80]: index = pd . MultiIndex . from_tuples ( coords ) In [81]: df = pd . DataFrame ([ 11 , 22 , 33 , 44 , 55 ], index , [ \"MyData\" ]) In [82]: df Out[82]: MyData AA one 11 six 22 BB one 33 two 44 six 55 To take the cross section of the 1st level and 1st axis the index: # Note : level and axis are optional, and default to zero In [83]: df . xs ( \"BB\" , level = 0 , axis = 0 ) Out[83]: MyData one 33 two 44 six 55 â¦and now the 2nd level of the 1st axis. In [84]: df . xs ( \"six\" , level = 1 , axis = 0 ) Out[84]: MyData AA 22 BB 55 Slicing a MultiIndex with xs, method #2 In [85]: import itertools In [86]: index = list ( itertools . product ([ \"Ada\" , \"Quinn\" , \"Violet\" ], [ \"Comp\" , \"Math\" , \"Sci\" ])) In [87]: headr = list ( itertools . product ([ \"Exams\" , \"Labs\" ], [ \"I\" , \"II\" ])) In [88]: indx = pd . MultiIndex . from_tuples ( index , names = [ \"Student\" , \"Course\" ]) In [89]: cols = pd . MultiIndex . from_tuples ( headr ) # Notice these are un-named In [90]: data = [[ 70 + x + y + ( x * y ) % 3 for x in range ( 4 )] for y in range ( 9 )] In [91]: df = pd . DataFrame ( data , indx , cols ) In [92]: df Out[92]: Exams Labs I II I II Student Course Ada Comp 70 71 72 73 Math 71 73 75 74 Sci 72 75 75 75 Quinn Comp 73 74 75 76 Math 74 76 78 77 Sci 75 78 78 78 Violet Comp 76 77 78 79 Math 77 79 81 80 Sci 78 81 81 81 In [93]: All = slice ( None ) In [94]: df . loc [ \"Violet\" ] Out[94]: Exams Labs I II I II Course Comp 76 77 78 79 Math 77 79 81 80 Sci 78 81 81 81 In [95]: df . loc [( All , \"Math\" ), All ] Out[95]: Exams Labs I II I II Student Course Ada Math 71 73 75 74 Quinn Math 74 76 78 77 Violet Math 77 79 81 80 In [96]: df . loc [( slice ( \"Ada\" , \"Quinn\" ), \"Math\" ), All ] Out[96]: Exams Labs I II I II Student Course Ada Math 71 73 75 74 Quinn Math 74 76 78 77 In [97]: df . loc [( All , \"Math\" ), ( \"Exams\" )] Out[97]: I II Student Course Ada Math 71 73 Quinn Math 74 76 Violet Math 77 79 In [98]: df . loc [( All , \"Math\" ), ( All , \"II\" )] Out[98]: Exams Labs II II Student Course Ada Math 73 74 Quinn Math 76 77 Violet Math 79 80 Setting portions of a MultiIndex with xs Sorting # Sort by specific column or an ordered list of columns, with a MultiIndex In [99]: df . sort_values ( by = ( \"Labs\" , \"II\" ), ascending = False ) Out[99]: Exams Labs I II I II Student Course Violet Sci 78 81 81 81 Math 77 79 81 80 Comp 76 77 78 79 Quinn Sci 75 78 78 78 Math 74 76 78 77 Comp 73 74 75 76 Ada Sci 72 75 75 75 Math 71 73 75 74 Comp 70 71 72 73 Partial selection, the need for sortedness GH 2995 Levels # Prepending a level to a multiindex Flatten Hierarchical columns Missing data # The missing data docs. Fill forward a reversed timeseries In [100]: df = pd . DataFrame ( .....: np . random . randn ( 6 , 1 ), .....: index = pd . date_range ( \"2013-08-01\" , periods = 6 , freq = \"B\" ), .....: columns = list ( \"A\" ), .....: ) .....: In [101]: df . loc [ df . index [ 3 ], \"A\" ] = np . nan In [102]: df Out[102]: A 2013-08-01 0.721555 2013-08-02 -0.706771 2013-08-05 -1.039575 2013-08-06 NaN 2013-08-07 -0.424972 2013-08-08 0.567020 In [103]: df . bfill () Out[103]: A 2013-08-01 0.721555 2013-08-02 -0.706771 2013-08-05 -1.039575 2013-08-06 -0.424972 2013-08-07 -0.424972 2013-08-08 0.567020 cumsum reset at NaN values Replace # Using replace with backrefs Grouping # The grouping docs. Basic grouping with apply Unlike agg, applyâs callable is passed a sub-DataFrame which gives you access to all the columns In [104]: df = pd . DataFrame ( .....: { .....: \"animal\" : \"cat dog cat fish dog cat cat\" . split (), .....: \"size\" : list ( \"SSMMMLL\" ), .....: \"weight\" : [ 8 , 10 , 11 , 1 , 20 , 12 , 12 ], .....: \"adult\" : [ False ] * 5 + [ True ] * 2 , .....: } .....: ) .....: In [105]: df Out[105]: animal size weight adult 0 cat S 8 False 1 dog S 10 False 2 cat M 11 False 3 fish M 1 False 4 dog M 20 False 5 cat L 12 True 6 cat L 12 True # List the size of the animals with the highest weight. In [106]: df . groupby ( \"animal\" ) . apply ( lambda subf : subf [ \"size\" ][ subf [ \"weight\" ] . idxmax ()], include_groups = False ) Out[106]: animal cat L dog M fish M dtype: object Using get_group In [107]: gb = df . groupby ( \"animal\" ) In [108]: gb . get_group ( \"cat\" ) Out[108]: animal size weight adult 0 cat S 8 False 2 cat M 11 False 5 cat L 12 True 6 cat L 12 True Apply to different items in a group In [109]: def GrowUp ( x ): .....: avg_weight = sum ( x [ x [ \"size\" ] == \"S\" ] . weight * 1.5 ) .....: avg_weight += sum ( x [ x [ \"size\" ] == \"M\" ] . weight * 1.25 ) .....: avg_weight += sum ( x [ x [ \"size\" ] == \"L\" ] . weight ) .....: avg_weight /= len ( x ) .....: return pd . Series ([ \"L\" , avg_weight , True ], index = [ \"size\" , \"weight\" , \"adult\" ]) .....: In [110]: expected_df = gb . apply ( GrowUp , include_groups = False ) In [111]: expected_df Out[111]: size weight adult animal cat L 12.4375 True dog L 20.0000 True fish L 1.2500 True Expanding apply In [112]: S = pd . Series ([ i / 100.0 for i in range ( 1 , 11 )]) In [113]: def cum_ret ( x , y ): .....: return x * ( 1 + y ) .....: In [114]: def red ( x ): .....: return functools . reduce ( cum_ret , x , 1.0 ) .....: In [115]: S . expanding () . apply ( red , raw = True ) Out[115]: 0 1.010000 1 1.030200 2 1.061106 3 1.103550 4 1.158728 5 1.228251 6 1.314229 7 1.419367 8 1.547110 9 1.701821 dtype: float64 Replacing some values with mean of the rest of a group In [116]: df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 2 , 2 ], \"B\" : [ 1 , - 1 , 1 , 2 ]}) In [117]: gb = df . groupby ( \"A\" ) In [118]: def replace ( g ): .....: mask = g < 0 .....: return g . where ( ~ mask , g [ ~ mask ] . mean ()) .....: In [119]: gb . transform ( replace ) Out[119]: B 0 1 1 1 2 1 3 2 Sort groups by aggregated data In [120]: df = pd . DataFrame ( .....: { .....: \"code\" : [ \"foo\" , \"bar\" , \"baz\" ] * 2 , .....: \"data\" : [ 0.16 , - 0.21 , 0.33 , 0.45 , - 0.59 , 0.62 ], .....: \"flag\" : [ False , True ] * 3 , .....: } .....: ) .....: In [121]: code_groups = df . groupby ( \"code\" ) In [122]: agg_n_sort_order = code_groups [[ \"data\" ]] . transform ( \"sum\" ) . sort_values ( by = \"data\" ) In [123]: sorted_df = df . loc [ agg_n_sort_order . index ] In [124]: sorted_df Out[124]: code data flag 1 bar -0.21 True 4 bar -0.59 False 0 foo 0.16 False 3 foo 0.45 True 2 baz 0.33 False 5 baz 0.62 True Create multiple aggregated columns In [125]: rng = pd . date_range ( start = \"2014-10-07\" , periods = 10 , freq = \"2min\" ) In [126]: ts = pd . Series ( data = list ( range ( 10 )), index = rng ) In [127]: def MyCust ( x ): .....: if len ( x ) > 2 : .....: return x . iloc [ 1 ] * 1.234 .....: return pd . NaT .....: In [128]: mhc = { \"Mean\" : \"mean\" , \"Max\" : \"max\" , \"Custom\" : MyCust } In [129]: ts . resample ( \"5min\" ) . apply ( mhc ) Out[129]: Mean Max Custom 2014-10-07 00:00:00 1.0 2 1.234 2014-10-07 00:05:00 3.5 4 NaT 2014-10-07 00:10:00 6.0 7 7.404 2014-10-07 00:15:00 8.5 9 NaT In [130]: ts Out[130]: 2014-10-07 00:00:00 0 2014-10-07 00:02:00 1 2014-10-07 00:04:00 2 2014-10-07 00:06:00 3 2014-10-07 00:08:00 4 2014-10-07 00:10:00 5 2014-10-07 00:12:00 6 2014-10-07 00:14:00 7 2014-10-07 00:16:00 8 2014-10-07 00:18:00 9 Freq: 2min, dtype: int64 Create a value counts column and reassign back to the DataFrame In [131]: df = pd . DataFrame ( .....: { \"Color\" : \"Red Red Red Blue\" . split (), \"Value\" : [ 100 , 150 , 50 , 50 ]} .....: ) .....: In [132]: df Out[132]: Color Value 0 Red 100 1 Red 150 2 Red 50 3 Blue 50 In [133]: df [ \"Counts\" ] = df . groupby ([ \"Color\" ]) . transform ( len ) In [134]: df Out[134]: Color Value Counts 0 Red 100 3 1 Red 150 3 2 Red 50 3 3 Blue 50 1 Shift groups of the values in a column based on the index In [135]: df = pd . DataFrame ( .....: { \"line_race\" : [ 10 , 10 , 8 , 10 , 10 , 8 ], \"beyer\" : [ 99 , 102 , 103 , 103 , 88 , 100 ]}, .....: index = [ .....: \"Last Gunfighter\" , .....: \"Last Gunfighter\" , .....: \"Last Gunfighter\" , .....: \"Paynter\" , .....: \"Paynter\" , .....: \"Paynter\" , .....: ], .....: ) .....: In [136]: df Out[136]: line_race beyer Last Gunfighter 10 99 Last Gunfighter 10 102 Last Gunfighter 8 103 Paynter 10 103 Paynter 10 88 Paynter 8 100 In [137]: df [ \"beyer_shifted\" ] = df . groupby ( level = 0 )[ \"beyer\" ] . shift ( 1 ) In [138]: df Out[138]: line_race beyer beyer_shifted Last Gunfighter 10 99 NaN Last Gunfighter 10 102 99.0 Last Gunfighter 8 103 102.0 Paynter 10 103 NaN Paynter 10 88 103.0 Paynter 8 100 88.0 Select row with maximum value from each group In [139]: df = pd . DataFrame ( .....: { .....: \"host\" : [ \"other\" , \"other\" , \"that\" , \"this\" , \"this\" ], .....: \"service\" : [ \"mail\" , \"web\" , \"mail\" , \"mail\" , \"web\" ], .....: \"no\" : [ 1 , 2 , 1 , 2 , 1 ], .....: } .....: ) . set_index ([ \"host\" , \"service\" ]) .....: In [140]: mask = df . groupby ( level = 0 ) . agg ( \"idxmax\" ) In [141]: df_count = df . loc [ mask [ \"no\" ]] . reset_index () In [142]: df_count Out[142]: host service no 0 other web 2 1 that mail 1 2 this mail 2 Grouping like Pythonâs itertools.groupby In [143]: df = pd . DataFrame ([ 0 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 ], columns = [ \"A\" ]) In [144]: df [ \"A\" ] . groupby (( df [ \"A\" ] != df [ \"A\" ] . shift ()) . cumsum ()) . groups Out[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]} In [145]: df [ \"A\" ] . groupby (( df [ \"A\" ] != df [ \"A\" ] . shift ()) . cumsum ()) . cumsum () Out[145]: 0 0 1 1 2 0 3 1 4 2 5 3 6 0 7 1 8 2 Name: A, dtype: int64 Expanding data # Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval Splitting # Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. In [146]: df = pd . DataFrame ( .....: data = { .....: \"Case\" : [ \"A\" , \"A\" , \"A\" , \"B\" , \"A\" , \"A\" , \"B\" , \"A\" , \"A\" ], .....: \"Data\" : np . random . randn ( 9 ), .....: } .....: ) .....: In [147]: dfs = list ( .....: zip ( .....: * df . groupby ( .....: ( 1 * ( df [ \"Case\" ] == \"B\" )) .....: . cumsum () .....: . rolling ( window = 3 , min_periods = 1 ) .....: . median () .....: ) .....: ) .....: )[ - 1 ] .....: In [148]: dfs [ 0 ] Out[148]: Case Data 0 A 0.276232 1 A -1.087401 2 A -0.673690 3 B 0.113648 In [149]: dfs [ 1 ] Out[149]: Case Data 4 A -1.478427 5 A 0.524988 6 B 0.404705 In [150]: dfs [ 2 ] Out[150]: Case Data 7 A 0.577046 8 A -1.715002 Pivot # The Pivot docs. Partial sums and subtotals In [151]: df = pd . DataFrame ( .....: data = { .....: \"Province\" : [ \"ON\" , \"QC\" , \"BC\" , \"AL\" , \"AL\" , \"MN\" , \"ON\" ], .....: \"City\" : [ .....: \"Toronto\" , .....: \"Montreal\" , .....: \"Vancouver\" , .....: \"Calgary\" , .....: \"Edmonton\" , .....: \"Winnipeg\" , .....: \"Windsor\" , .....: ], .....: \"Sales\" : [ 13 , 6 , 16 , 8 , 4 , 3 , 1 ], .....: } .....: ) .....: In [152]: table = pd . pivot_table ( .....: df , .....: values = [ \"Sales\" ], .....: index = [ \"Province\" ], .....: columns = [ \"City\" ], .....: aggfunc = \"sum\" , .....: margins = True , .....: ) .....: In [153]: table . stack ( \"City\" , future_stack = True ) Out[153]: Sales Province City AL Calgary 8.0 Edmonton 4.0 Montreal NaN Toronto NaN Vancouver NaN ... ... All Toronto 13.0 Vancouver 16.0 Windsor 1.0 Winnipeg 3.0 All 51.0 [48 rows x 1 columns] Frequency table like plyr in R In [154]: grades = [ 48 , 99 , 75 , 80 , 42 , 80 , 72 , 68 , 36 , 78 ] In [155]: df = pd . DataFrame ( .....: { .....: \"ID\" : [ \"x %d \" % r for r in range ( 10 )], .....: \"Gender\" : [ \"F\" , \"M\" , \"F\" , \"M\" , \"F\" , \"M\" , \"F\" , \"M\" , \"M\" , \"M\" ], .....: \"ExamYear\" : [ .....: \"2007\" , .....: \"2007\" , .....: \"2007\" , .....: \"2008\" , .....: \"2008\" , .....: \"2008\" , .....: \"2008\" , .....: \"2009\" , .....: \"2009\" , .....: \"2009\" , .....: ], .....: \"Class\" : [ .....: \"algebra\" , .....: \"stats\" , .....: \"bio\" , .....: \"algebra\" , .....: \"algebra\" , .....: \"stats\" , .....: \"stats\" , .....: \"algebra\" , .....: \"bio\" , .....: \"bio\" , .....: ], .....: \"Participated\" : [ .....: \"yes\" , .....: \"yes\" , .....: \"yes\" , .....: \"yes\" , .....: \"no\" , .....: \"yes\" , .....: \"yes\" , .....: \"yes\" , .....: \"yes\" , .....: \"yes\" , .....: ], .....: \"Passed\" : [ \"yes\" if x > 50 else \"no\" for x in grades ], .....: \"Employed\" : [ .....: True , .....: True , .....: True , .....: False , .....: False , .....: False , .....: False , .....: True , .....: True , .....: False , .....: ], .....: \"Grade\" : grades , .....: } .....: ) .....: In [156]: df . groupby ( \"ExamYear\" ) . agg ( .....: { .....: \"Participated\" : lambda x : x . value_counts ()[ \"yes\" ], .....: \"Passed\" : lambda x : sum ( x == \"yes\" ), .....: \"Employed\" : lambda x : sum ( x ), .....: \"Grade\" : lambda x : sum ( x ) / len ( x ), .....: } .....: ) .....: Out[156]: Participated Passed Employed Grade ExamYear 2007 3 2 3 74.000000 2008 3 3 0 68.500000 2009 3 2 2 60.666667 Plot pandas DataFrame with year over year data To create year and month cross tabulation: In [157]: df = pd . DataFrame ( .....: { \"value\" : np . random . randn ( 36 )}, .....: index = pd . date_range ( \"2011-01-01\" , freq = \"ME\" , periods = 36 ), .....: ) .....: In [158]: pd . pivot_table ( .....: df , index = df . index . month , columns = df . index . year , values = \"value\" , aggfunc = \"sum\" .....: ) .....: Out[158]: 2011 2012 2013 1 -1.039268 -0.968914 2.565646 2 -0.370647 -1.294524 1.431256 3 -1.157892 0.413738 1.340309 4 -1.344312 0.276662 -1.170299 5 0.844885 -0.472035 -0.226169 6 1.075770 -0.013960 0.410835 7 -0.109050 -0.362543 0.813850 8 1.643563 -0.006154 0.132003 9 -1.469388 -0.923061 -0.827317 10 0.357021 0.895717 -0.076467 11 -0.674600 0.805244 -1.187678 12 -1.776904 -1.206412 1.130127 Apply # Rolling apply to organize - Turning embedded lists into a MultiIndex frame In [159]: df = pd . DataFrame ( .....: data = { .....: \"A\" : [[ 2 , 4 , 8 , 16 ], [ 100 , 200 ], [ 10 , 20 , 30 ]], .....: \"B\" : [[ \"a\" , \"b\" , \"c\" ], [ \"jj\" , \"kk\" ], [ \"ccc\" ]], .....: }, .....: index = [ \"I\" , \"II\" , \"III\" ], .....: ) .....: In [160]: def SeriesFromSubList ( aList ): .....: return pd . Series ( aList ) .....: In [161]: df_orgz = pd . concat ( .....: { ind : row . apply ( SeriesFromSubList ) for ind , row in df . iterrows ()} .....: ) .....: In [162]: df_orgz Out[162]: 0 1 2 3 I A 2 4 8 16.0 B a b c NaN II A 100 200 NaN NaN B jj kk NaN NaN III A 10 20.0 30.0 NaN B ccc NaN NaN NaN Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned In [163]: df = pd . DataFrame ( .....: data = np . random . randn ( 2000 , 2 ) / 10000 , .....: index = pd . date_range ( \"2001-01-01\" , periods = 2000 ), .....: columns = [ \"A\" , \"B\" ], .....: ) .....: In [164]: df Out[164]: A B 2001-01-01 -0.000144 -0.000141 2001-01-02 0.000161 0.000102 2001-01-03 0.000057 0.000088 2001-01-04 -0.000221 0.000097 2001-01-05 -0.000201 -0.000041 ... ... ... 2006-06-19 0.000040 -0.000235 2006-06-20 -0.000123 -0.000021 2006-06-21 -0.000113 0.000114 2006-06-22 0.000136 0.000109 2006-06-23 0.000027 0.000030 [2000 rows x 2 columns] In [165]: def gm ( df , const ): .....: v = (((( df [ \"A\" ] + df [ \"B\" ]) + 1 ) . cumprod ()) - 1 ) * const .....: return v . iloc [ - 1 ] .....: In [166]: s = pd . Series ( .....: { .....: df . index [ i ]: gm ( df . iloc [ i : min ( i + 51 , len ( df ) - 1 )], 5 ) .....: for i in range ( len ( df ) - 50 ) .....: } .....: ) .....: In [167]: s Out[167]: 2001-01-01 0.000930 2001-01-02 0.002615 2001-01-03 0.001281 2001-01-04 0.001117 2001-01-05 0.002772 ... 2006-04-30 0.003296 2006-05-01 0.002629 2006-05-02 0.002081 2006-05-03 0.004247 2006-05-04 0.003928 Length: 1950, dtype: float64 Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) In [168]: rng = pd . date_range ( start = \"2014-01-01\" , periods = 100 ) In [169]: df = pd . DataFrame ( .....: { .....: \"Open\" : np . random . randn ( len ( rng )), .....: \"Close\" : np . random . randn ( len ( rng )), .....: \"Volume\" : np . random . randint ( 100 , 2000 , len ( rng )), .....: }, .....: index = rng , .....: ) .....: In [170]: df Out[170]: Open Close Volume 2014-01-01 -1.611353 -0.492885 1219 2014-01-02 -3.000951 0.445794 1054 2014-01-03 -0.138359 -0.076081 1381 2014-01-04 0.301568 1.198259 1253 2014-01-05 0.276381 -0.669831 1728 ... ... ... ... 2014-04-06 -0.040338 0.937843 1188 2014-04-07 0.359661 -0.285908 1864 2014-04-08 0.060978 1.714814 941 2014-04-09 1.759055 -0.455942 1065 2014-04-10 0.138185 -1.147008 1453 [100 rows x 3 columns] In [171]: def vwap ( bars ): .....: return ( bars . Close * bars . Volume ) . sum () / bars . Volume . sum () .....: In [172]: window = 5 In [173]: s = pd . concat ( .....: [ .....: ( pd . Series ( vwap ( df . iloc [ i : i + window ]), index = [ df . index [ i + window ]])) .....: for i in range ( len ( df ) - window ) .....: ] .....: ) .....: In [174]: s . round ( 2 ) Out[174]: 2014-01-06 0.02 2014-01-07 0.11 2014-01-08 0.10 2014-01-09 0.07 2014-01-10 -0.29 ... 2014-04-06 -0.63 2014-04-07 -0.02 2014-04-08 -0.03 2014-04-09 0.34 2014-04-10 0.29 Length: 95, dtype: float64 Timeseries # Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex In [175]: dates = pd . date_range ( \"2000-01-01\" , periods = 5 ) In [176]: dates . to_period ( freq = \"M\" ) . to_timestamp () Out[176]: DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01'], dtype='datetime64[ns]', freq=None) Resampling # The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH 3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby Merge # The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) In [177]: rng = pd . date_range ( \"2000-01-01\" , periods = 6 ) In [178]: df1 = pd . DataFrame ( np . random . randn ( 6 , 3 ), index = rng , columns = [ \"A\" , \"B\" , \"C\" ]) In [179]: df2 = df1 . copy () Depending on df construction, ignore_index may be needed In [180]: df = pd . concat ([ df1 , df2 ], ignore_index = True ) In [181]: df Out[181]: A B C 0 -0.870117 -0.479265 -0.790855 1 0.144817 1.726395 -0.464535 2 -0.821906 1.597605 0.187307 3 -0.128342 -1.511638 -0.289858 4 0.399194 -1.430030 -0.639760 5 1.115116 -2.012600 1.810662 6 -0.870117 -0.479265 -0.790855 7 0.144817 1.726395 -0.464535 8 -0.821906 1.597605 0.187307 9 -0.128342 -1.511638 -0.289858 10 0.399194 -1.430030 -0.639760 11 1.115116 -2.012600 1.810662 Self Join of a DataFrame GH 2996 In [182]: df = pd . DataFrame ( .....: data = { .....: \"Area\" : [ \"A\" ] * 5 + [ \"C\" ] * 2 , .....: \"Bins\" : [ 110 ] * 2 + [ 160 ] * 3 + [ 40 ] * 2 , .....: \"Test_0\" : [ 0 , 1 , 0 , 1 , 2 , 0 , 1 ], .....: \"Data\" : np . random . randn ( 7 ), .....: } .....: ) .....: In [183]: df Out[183]: Area Bins Test_0 Data 0 A 110 0 -0.433937 1 A 110 1 -0.160552 2 A 160 0 0.744434 3 A 160 1 1.754213 4 A 160 2 0.000850 5 C 40 0 0.342243 6 C 40 1 1.070599 In [184]: df [ \"Test_1\" ] = df [ \"Test_0\" ] - 1 In [185]: pd . merge ( .....: df , .....: df , .....: left_on = [ \"Bins\" , \"Area\" , \"Test_0\" ], .....: right_on = [ \"Bins\" , \"Area\" , \"Test_1\" ], .....: suffixes = ( \"_L\" , \"_R\" ), .....: ) .....: Out[185]: Area Bins Test_0_L Data_L Test_1_L Test_0_R Data_R Test_1_R 0 A 110 0 -0.433937 -1 1 -0.160552 0 1 A 160 0 0.744434 -1 1 1.754213 0 2 A 160 1 1.754213 0 2 0.000850 1 3 C 40 0 0.342243 -1 1 1.070599 0 How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range Plotting # The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable In [186]: df = pd . DataFrame ( .....: { .....: \"stratifying_var\" : np . random . uniform ( 0 , 100 , 20 ), .....: \"price\" : np . random . normal ( 100 , 5 , 20 ), .....: } .....: ) .....: In [187]: df [ \"quartiles\" ] = pd . qcut ( .....: df [ \"stratifying_var\" ], 4 , labels = [ \"0-25%\" , \"25-50%\" , \"50-75%\" , \"75-100%\" ] .....: ) .....: In [188]: df . boxplot ( column = \"price\" , by = \"quartiles\" ) Out[188]: <Axes: title={'center': 'price'}, xlabel='quartiles'> Data in/out # Performance comparison of SQL vs HDF5 CSV # The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH 2886 Write a multi-row index CSV without writing duplicates Reading multiple files to create a single DataFrame # The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat() : In [189]: for i in range ( 3 ): .....: data = pd . DataFrame ( np . random . randn ( 10 , 4 )) .....: data . to_csv ( \"file_ {} .csv\" . format ( i )) .....: In [190]: files = [ \"file_0.csv\" , \"file_1.csv\" , \"file_2.csv\" ] In [191]: result = pd . concat ([ pd . read_csv ( f ) for f in files ], ignore_index = True ) You can use the same approach to read all files matching a pattern. Here is an example using glob : In [192]: import glob In [193]: import os In [194]: files = glob . glob ( \"file_*.csv\" ) In [195]: result = pd . concat ([ pd . read_csv ( f ) for f in files ], ignore_index = True ) Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs . Parsing date components in multi-columns # Parsing date components in multi-columns is faster with a format In [196]: i = pd . date_range ( \"20000101\" , periods = 10000 ) In [197]: df = pd . DataFrame ({ \"year\" : i . year , \"month\" : i . month , \"day\" : i . day }) In [198]: df . head () Out[198]: year month day 0 2000 1 1 1 2000 1 2 2 2000 1 3 3 2000 1 4 4 2000 1 5 In [199]: % timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d') .....: ds = df . apply ( lambda x : \" %04d%02d%02d \" % ( x [ \"year\" ], x [ \"month\" ], x [ \"day\" ]), axis = 1 ) .....: ds . head () .....: % timeit pd.to_datetime(ds) .....: 2.94 ms +- 300 us per loop (mean +- std. dev. of 7 runs, 100 loops each) 1.23 ms +- 4.74 us per loop (mean +- std. dev. of 7 runs, 1,000 loops each) Skip row between header and data # In [200]: data = \"\"\";;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: ;;;; .....: date;Param1;Param2;Param4;Param5 .....: ;mÂ²;Â°C;mÂ²;m .....: ;;;; .....: 01.01.1990 00:00;1;1;2;3 .....: 01.01.1990 01:00;5;3;4;5 .....: 01.01.1990 02:00;9;5;6;7 .....: 01.01.1990 03:00;13;7;8;9 .....: 01.01.1990 04:00;17;9;10;11 .....: 01.01.1990 05:00;21;11;12;13 .....: \"\"\" .....: Option 1: pass rows explicitly to skip rows # In [201]: from io import StringIO In [202]: pd . read_csv ( .....: StringIO ( data ), .....: sep = \";\" , .....: skiprows = [ 11 , 12 ], .....: index_col = 0 , .....: parse_dates = True , .....: header = 10 , .....: ) .....: Out[202]: Param1 Param2 Param4 Param5 date 1990-01-01 00:00:00 1 1 2 3 1990-01-01 01:00:00 5 3 4 5 1990-01-01 02:00:00 9 5 6 7 1990-01-01 03:00:00 13 7 8 9 1990-01-01 04:00:00 17 9 10 11 1990-01-01 05:00:00 21 11 12 13 Option 2: read column names and then data # In [203]: pd . read_csv ( StringIO ( data ), sep = \";\" , header = 10 , nrows = 10 ) . columns Out[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object') In [204]: columns = pd . read_csv ( StringIO ( data ), sep = \";\" , header = 10 , nrows = 10 ) . columns In [205]: pd . read_csv ( .....: StringIO ( data ), sep = \";\" , index_col = 0 , header = 12 , parse_dates = True , names = columns .....: ) .....: Out[205]: Param1 Param2 Param4 Param5 date 1990-01-01 00:00:00 1 1 2 3 1990-01-01 01:00:00 5 3 4 5 1990-01-01 02:00:00 9 5 6 7 1990-01-01 03:00:00 13 7 8 9 1990-01-01 04:00:00 17 9 10 11 1990-01-01 05:00:00 21 11 12 13 SQL # The SQL docs Reading from databases with SQL Excel # The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH 19842#issuecomment-892150745 HTML # Reading HTML tables from a server that cannot handle the default request header HDFStore # The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH 3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node In [206]: df = pd . DataFrame ( np . random . randn ( 8 , 3 )) In [207]: store = pd . HDFStore ( \"test.h5\" ) In [208]: store . put ( \"df\" , df ) # you can store an arbitrary Python object via pickle In [209]: store . get_storer ( \"df\" ) . attrs . my_attribute = { \"A\" : 10 } In [210]: store . get_storer ( \"df\" ) . attrs . my_attribute Out[210]: {'A': 10} You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. In [211]: store = pd . HDFStore ( \"test.h5\" , \"w\" , driver = \"H5FD_CORE\" ) In [212]: df = pd . DataFrame ( np . random . randn ( 8 , 3 )) In [213]: store [ \"test\" ] = df # only after closing the store, data is written to disk: In [214]: store . close () Binary files # pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, #include <stdio.h> #include <stdint.h> typedef struct _Data { int32_t count ; double avg ; float scale ; } Data ; int main ( int argc , const char * argv []) { size_t n = 10 ; Data d [ n ]; for ( int i = 0 ; i < n ; ++ i ) { d [ i ]. count = i ; d [ i ]. avg = i + 1.0 ; d [ i ]. scale = ( float ) i + 2.0f ; } FILE * file = fopen ( \"binary.dat\" , \"wb\" ); fwrite ( & d , sizeof ( Data ), n , file ); fclose ( file ); return 0 ; } the following Python code will read the binary file 'binary.dat' into a pandas DataFrame , where each element of the struct corresponds to a column in the frame: names = \"count\" , \"avg\" , \"scale\" # note that the offsets are larger than the size of the type because of # struct padding offsets = 0 , 8 , 16 formats = \"i4\" , \"f8\" , \"f4\" dt = np . dtype ({ \"names\" : names , \"offsets\" : offsets , \"formats\" : formats }, align = True ) df = pd . DataFrame ( np . fromfile ( \"binary.dat\" , dt )) Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandasâ IO facilities. Computation # Numerical integration (sample-based) of a time series Correlation # Often itâs useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr() . This can be achieved by passing a boolean mask to where as follows: In [215]: df = pd . DataFrame ( np . random . random ( size = ( 100 , 5 ))) In [216]: corr_mat = df . corr () In [217]: mask = np . tril ( np . ones_like ( corr_mat , dtype = np . bool_ ), k =- 1 ) In [218]: corr_mat . where ( mask ) Out[218]: 0 1 2 3 4 0 NaN NaN NaN NaN NaN 1 -0.079861 NaN NaN NaN NaN 2 -0.236573 0.183801 NaN NaN NaN 3 -0.013795 -0.051975 0.037235 NaN NaN 4 -0.031974 0.118342 -0.073499 -0.02063 NaN The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. In [219]: def distcorr ( x , y ): .....: n = len ( x ) .....: a = np . zeros ( shape = ( n , n )) .....: b = np . zeros ( shape = ( n , n )) .....: for i in range ( n ): .....: for j in range ( i + 1 , n ): .....: a [ i , j ] = abs ( x [ i ] - x [ j ]) .....: b [ i , j ] = abs ( y [ i ] - y [ j ]) .....: a += a . T .....: b += b . T .....: a_bar = np . vstack ([ np . nanmean ( a , axis = 0 )] * n ) .....: b_bar = np . vstack ([ np . nanmean ( b , axis = 0 )] * n ) .....: A = a - a_bar - a_bar . T + np . full ( shape = ( n , n ), fill_value = a_bar . mean ()) .....: B = b - b_bar - b_bar . T + np . full ( shape = ( n , n ), fill_value = b_bar . mean ()) .....: cov_ab = np . sqrt ( np . nansum ( A * B )) / n .....: std_a = np . sqrt ( np . sqrt ( np . nansum ( A ** 2 )) / n ) .....: std_b = np . sqrt ( np . sqrt ( np . nansum ( B ** 2 )) / n ) .....: return cov_ab / std_a / std_b .....: In [220]: df = pd . DataFrame ( np . random . normal ( size = ( 100 , 3 ))) In [221]: df . corr ( method = distcorr ) Out[221]: 0 1 2 0 1.000000 0.197613 0.216328 1 0.197613 1.000000 0.208749 2 0.216328 0.208749 1.000000 Timedeltas # The Timedeltas docs. Using timedeltas In [222]: import datetime In [223]: s = pd . Series ( pd . date_range ( \"2012-1-1\" , periods = 3 , freq = \"D\" )) In [224]: s - s . max () Out[224]: 0 -2 days 1 -1 days 2 0 days dtype: timedelta64[ns] In [225]: s . max () - s Out[225]: 0 2 days 1 1 days 2 0 days dtype: timedelta64[ns] In [226]: s - datetime . datetime ( 2011 , 1 , 1 , 3 , 5 ) Out[226]: 0 364 days 20:55:00 1 365 days 20:55:00 2 366 days 20:55:00 dtype: timedelta64[ns] In [227]: s + datetime . timedelta ( minutes = 5 ) Out[227]: 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] In [228]: datetime . datetime ( 2011 , 1 , 1 , 3 , 5 ) - s Out[228]: 0 -365 days +03:05:00 1 -366 days +03:05:00 2 -367 days +03:05:00 dtype: timedelta64[ns] In [229]: datetime . timedelta ( minutes = 5 ) + s Out[229]: 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] Adding and subtracting deltas and dates In [230]: deltas = pd . Series ([ datetime . timedelta ( days = i ) for i in range ( 3 )]) In [231]: df = pd . DataFrame ({ \"A\" : s , \"B\" : deltas }) In [232]: df Out[232]: A B 0 2012-01-01 0 days 1 2012-01-02 1 days 2 2012-01-03 2 days In [233]: df [ \"New Dates\" ] = df [ \"A\" ] + df [ \"B\" ] In [234]: df [ \"Delta\" ] = df [ \"A\" ] - df [ \"New Dates\" ] In [235]: df Out[235]: A B New Dates Delta 0 2012-01-01 0 days 2012-01-01 0 days 1 2012-01-02 1 days 2012-01-03 -1 days 2 2012-01-03 2 days 2012-01-05 -2 days In [236]: df . dtypes Out[236]: A datetime64[ns] B timedelta64[ns] New Dates datetime64[ns] Delta timedelta64[ns] dtype: object Another example Values can be set to NaT using np.nan, similar to datetime In [237]: y = s - s . shift () In [238]: y Out[238]: 0 NaT 1 1 days 2 1 days dtype: timedelta64[ns] In [239]: y [ 1 ] = np . nan In [240]: y Out[240]: 0 NaT 1 NaT 2 1 days dtype: timedelta64[ns] Creating example data # To create a dataframe from every combination of some given values, like Râs expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: In [241]: def expand_grid ( data_dict ): .....: rows = itertools . product ( * data_dict . values ()) .....: return pd . DataFrame . from_records ( rows , columns = data_dict . keys ()) .....: In [242]: df = expand_grid ( .....: { \"height\" : [ 60 , 70 ], \"weight\" : [ 100 , 140 , 180 ], \"sex\" : [ \"Male\" , \"Female\" ]} .....: ) .....: In [243]: df Out[243]: height weight sex 0 60 100 Male 1 60 100 Female 2 60 140 Male 3 60 140 Female 4 60 180 Male 5 60 180 Female 6 70 100 Male 7 70 100 Female 8 70 140 Male 9 70 140 Female 10 70 180 Male 11 70 180 Female Constant series # To assess if a series has a constant value, we can check if series.nunique() <= 1 . However, a more performant approach, that does not count all unique values first, is: In [244]: v = s . to_numpy () In [245]: is_constant = v . shape [ 0 ] == 0 or ( s [ 0 ] == s ) . all () This approach assumes that the series does not contain missing values. For the case that we would drop NA values, we can simply remove those values first: In [246]: v = s . dropna () . to_numpy () In [247]: is_constant = v . shape [ 0 ] == 0 or ( s [ 0 ] == s ) . all () If missing values are considered distinct from any other value, then one could use: In [248]: v = s . to_numpy () In [249]: is_constant = v . shape [ 0 ] == 0 or ( s [ 0 ] == s ) . all () or not pd . notna ( v ) . any () (Note that this example does not disambiguate between np.nan , pd.NA and None ) previous Frequently Asked Questions (FAQ) next API reference On this page Idioms if-thenâ¦ Splitting Building criteria Selection Dataframes New columns Multiindexing Arithmetic Slicing Sorting Levels Missing data Replace Grouping Expanding data Splitting Pivot Apply Timeseries Resampling Merge Plotting Data in/out CSV Reading multiple files to create a single DataFrame Parsing date components in multi-columns Skip row between header and data Option 1: pass rows explicitly to skip rows Option 2: read column names and then data SQL Excel HTML HDFStore Binary files Computation Correlation Timedeltas Creating example data Constant series Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/cookbook.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference DataFrame DataFrame # Constructor # DataFrame ([data,Â index,Â columns,Â dtype,Â copy]) Two-dimensional, size-mutable, potentially heterogeneous tabular data. Attributes and underlying data # Axes DataFrame.index The index (row labels) of the DataFrame. DataFrame.columns The column labels of the DataFrame. DataFrame.dtypes Return the dtypes in the DataFrame. DataFrame.info ([verbose,Â buf,Â max_cols,Â ...]) Print a concise summary of a DataFrame. DataFrame.select_dtypes ([include,Â exclude]) Return a subset of the DataFrame's columns based on the column dtypes. DataFrame.values Return a Numpy representation of the DataFrame. DataFrame.axes Return a list representing the axes of the DataFrame. DataFrame.ndim Return an int representing the number of axes / array dimensions. DataFrame.size Return an int representing the number of elements in this object. DataFrame.shape Return a tuple representing the dimensionality of the DataFrame. DataFrame.memory_usage ([index,Â deep]) Return the memory usage of each column in bytes. DataFrame.empty Indicator whether Series/DataFrame is empty. DataFrame.set_flags (*[,Â copy,Â ...]) Return a new object with updated flags. Conversion # DataFrame.astype (dtype[,Â copy,Â errors]) Cast a pandas object to a specified dtype dtype . DataFrame.convert_dtypes ([infer_objects,Â ...]) Convert columns to the best possible dtypes using dtypes supporting pd.NA . DataFrame.infer_objects ([copy]) Attempt to infer better dtypes for object columns. DataFrame.copy ([deep]) Make a copy of this object's indices and data. DataFrame.bool () (DEPRECATED) Return the bool of a single element Series or DataFrame. DataFrame.to_numpy ([dtype,Â copy,Â na_value]) Convert the DataFrame to a NumPy array. Indexing, iteration # DataFrame.head ([n]) Return the first n rows. DataFrame.at Access a single value for a row/column label pair. DataFrame.iat Access a single value for a row/column pair by integer position. DataFrame.loc Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc (DEPRECATED) Purely integer-location based indexing for selection by position. DataFrame.insert (loc,Â column,Â value[,Â ...]) Insert column into DataFrame at specified location. DataFrame.__iter__ () Iterate over info axis. DataFrame.items () Iterate over (column name, Series) pairs. DataFrame.keys () Get the 'info axis' (see Indexing for more). DataFrame.iterrows () Iterate over DataFrame rows as (index, Series) pairs. DataFrame.itertuples ([index,Â name]) Iterate over DataFrame rows as namedtuples. DataFrame.pop (item) Return item and drop from frame. DataFrame.tail ([n]) Return the last n rows. DataFrame.xs (key[,Â axis,Â level,Â drop_level]) Return cross-section from the Series/DataFrame. DataFrame.get (key[,Â default]) Get item from object for given key (ex: DataFrame column). DataFrame.isin (values) Whether each element in the DataFrame is contained in values. DataFrame.where (cond[,Â other,Â inplace,Â ...]) Replace values where the condition is False. DataFrame.mask (cond[,Â other,Â inplace,Â axis,Â ...]) Replace values where the condition is True. DataFrame.query (expr,Â *[,Â inplace]) Query the columns of a DataFrame with a boolean expression. For more information on .at , .iat , .loc , and .iloc , see the indexing documentation . Binary operator functions # DataFrame.__add__ (other) Get Addition of DataFrame and other, column-wise. DataFrame.add (other[,Â axis,Â level,Â fill_value]) Get Addition of dataframe and other, element-wise (binary operator add ). DataFrame.sub (other[,Â axis,Â level,Â fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator sub ). DataFrame.mul (other[,Â axis,Â level,Â fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator mul ). DataFrame.div (other[,Â axis,Â level,Â fill_value]) Get Floating division of dataframe and other, element-wise (binary operator truediv ). DataFrame.truediv (other[,Â axis,Â level,Â ...]) Get Floating division of dataframe and other, element-wise (binary operator truediv ). DataFrame.floordiv (other[,Â axis,Â level,Â ...]) Get Integer division of dataframe and other, element-wise (binary operator floordiv ). DataFrame.mod (other[,Â axis,Â level,Â fill_value]) Get Modulo of dataframe and other, element-wise (binary operator mod ). DataFrame.pow (other[,Â axis,Â level,Â fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator pow ). DataFrame.dot (other) Compute the matrix multiplication between the DataFrame and other. DataFrame.radd (other[,Â axis,Â level,Â fill_value]) Get Addition of dataframe and other, element-wise (binary operator radd ). DataFrame.rsub (other[,Â axis,Â level,Â fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator rsub ). DataFrame.rmul (other[,Â axis,Â level,Â fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator rmul ). DataFrame.rdiv (other[,Â axis,Â level,Â fill_value]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). DataFrame.rtruediv (other[,Â axis,Â level,Â ...]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). DataFrame.rfloordiv (other[,Â axis,Â level,Â ...]) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv ). DataFrame.rmod (other[,Â axis,Â level,Â fill_value]) Get Modulo of dataframe and other, element-wise (binary operator rmod ). DataFrame.rpow (other[,Â axis,Â level,Â fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator rpow ). DataFrame.lt (other[,Â axis,Â level]) Get Less than of dataframe and other, element-wise (binary operator lt ). DataFrame.gt (other[,Â axis,Â level]) Get Greater than of dataframe and other, element-wise (binary operator gt ). DataFrame.le (other[,Â axis,Â level]) Get Less than or equal to of dataframe and other, element-wise (binary operator le ). DataFrame.ge (other[,Â axis,Â level]) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge ). DataFrame.ne (other[,Â axis,Â level]) Get Not equal to of dataframe and other, element-wise (binary operator ne ). DataFrame.eq (other[,Â axis,Â level]) Get Equal to of dataframe and other, element-wise (binary operator eq ). DataFrame.combine (other,Â func[,Â fill_value,Â ...]) Perform column-wise combine with another DataFrame. DataFrame.combine_first (other) Update null elements with value in the same location in other . Function application, GroupBy & window # DataFrame.apply (func[,Â axis,Â raw,Â ...]) Apply a function along an axis of the DataFrame. DataFrame.map (func[,Â na_action]) Apply a function to a Dataframe elementwise. DataFrame.applymap (func[,Â na_action]) (DEPRECATED) Apply a function to a Dataframe elementwise. DataFrame.pipe (func,Â *args,Â **kwargs) Apply chainable functions that expect Series or DataFrames. DataFrame.agg ([func,Â axis]) Aggregate using one or more operations over the specified axis. DataFrame.aggregate ([func,Â axis]) Aggregate using one or more operations over the specified axis. DataFrame.transform (func[,Â axis]) Call func on self producing a DataFrame with the same axis shape as self. DataFrame.groupby ([by,Â axis,Â level,Â ...]) Group DataFrame using a mapper or by a Series of columns. DataFrame.rolling (window[,Â min_periods,Â ...]) Provide rolling window calculations. DataFrame.expanding ([min_periods,Â axis,Â method]) Provide expanding window calculations. DataFrame.ewm ([com,Â span,Â halflife,Â alpha,Â ...]) Provide exponentially weighted (EW) calculations. Computations / descriptive stats # DataFrame.abs () Return a Series/DataFrame with absolute numeric value of each element. DataFrame.all ([axis,Â bool_only,Â skipna]) Return whether all elements are True, potentially over an axis. DataFrame.any (*[,Â axis,Â bool_only,Â skipna]) Return whether any element is True, potentially over an axis. DataFrame.clip ([lower,Â upper,Â axis,Â inplace]) Trim values at input threshold(s). DataFrame.corr ([method,Â min_periods,Â ...]) Compute pairwise correlation of columns, excluding NA/null values. DataFrame.corrwith (other[,Â axis,Â drop,Â ...]) Compute pairwise correlation. DataFrame.count ([axis,Â numeric_only]) Count non-NA cells for each column or row. DataFrame.cov ([min_periods,Â ddof,Â numeric_only]) Compute pairwise covariance of columns, excluding NA/null values. DataFrame.cummax ([axis,Â skipna]) Return cumulative maximum over a DataFrame or Series axis. DataFrame.cummin ([axis,Â skipna]) Return cumulative minimum over a DataFrame or Series axis. DataFrame.cumprod ([axis,Â skipna]) Return cumulative product over a DataFrame or Series axis. DataFrame.cumsum ([axis,Â skipna]) Return cumulative sum over a DataFrame or Series axis. DataFrame.describe ([percentiles,Â include,Â ...]) Generate descriptive statistics. DataFrame.diff ([periods,Â axis]) First discrete difference of element. DataFrame.eval (expr,Â *[,Â inplace]) Evaluate a string describing operations on DataFrame columns. DataFrame.kurt ([axis,Â skipna,Â numeric_only]) Return unbiased kurtosis over requested axis. DataFrame.kurtosis ([axis,Â skipna,Â numeric_only]) Return unbiased kurtosis over requested axis. DataFrame.max ([axis,Â skipna,Â numeric_only]) Return the maximum of the values over the requested axis. DataFrame.mean ([axis,Â skipna,Â numeric_only]) Return the mean of the values over the requested axis. DataFrame.median ([axis,Â skipna,Â numeric_only]) Return the median of the values over the requested axis. DataFrame.min ([axis,Â skipna,Â numeric_only]) Return the minimum of the values over the requested axis. DataFrame.mode ([axis,Â numeric_only,Â dropna]) Get the mode(s) of each element along the selected axis. DataFrame.pct_change ([periods,Â fill_method,Â ...]) Fractional change between the current and a prior element. DataFrame.prod ([axis,Â skipna,Â numeric_only,Â ...]) Return the product of the values over the requested axis. DataFrame.product ([axis,Â skipna,Â ...]) Return the product of the values over the requested axis. DataFrame.quantile ([q,Â axis,Â numeric_only,Â ...]) Return values at the given quantile over requested axis. DataFrame.rank ([axis,Â method,Â numeric_only,Â ...]) Compute numerical data ranks (1 through n) along axis. DataFrame.round ([decimals]) Round a DataFrame to a variable number of decimal places. DataFrame.sem ([axis,Â skipna,Â ddof,Â numeric_only]) Return unbiased standard error of the mean over requested axis. DataFrame.skew ([axis,Â skipna,Â numeric_only]) Return unbiased skew over requested axis. DataFrame.sum ([axis,Â skipna,Â numeric_only,Â ...]) Return the sum of the values over the requested axis. DataFrame.std ([axis,Â skipna,Â ddof,Â numeric_only]) Return sample standard deviation over requested axis. DataFrame.var ([axis,Â skipna,Â ddof,Â numeric_only]) Return unbiased variance over requested axis. DataFrame.nunique ([axis,Â dropna]) Count number of distinct elements in specified axis. DataFrame.value_counts ([subset,Â normalize,Â ...]) Return a Series containing the frequency of each distinct row in the Dataframe. Reindexing / selection / label manipulation # DataFrame.add_prefix (prefix[,Â axis]) Prefix labels with string prefix . DataFrame.add_suffix (suffix[,Â axis]) Suffix labels with string suffix . DataFrame.align (other[,Â join,Â axis,Â level,Â ...]) Align two objects on their axes with the specified join method. DataFrame.at_time (time[,Â asof,Â axis]) Select values at particular time of day (e.g., 9:30AM). DataFrame.between_time (start_time,Â end_time) Select values between particular times of the day (e.g., 9:00-9:30 AM). DataFrame.drop ([labels,Â axis,Â index,Â ...]) Drop specified labels from rows or columns. DataFrame.drop_duplicates ([subset,Â keep,Â ...]) Return DataFrame with duplicate rows removed. DataFrame.duplicated ([subset,Â keep]) Return boolean Series denoting duplicate rows. DataFrame.equals (other) Test whether two objects contain the same elements. DataFrame.filter ([items,Â like,Â regex,Â axis]) Subset the dataframe rows or columns according to the specified index labels. DataFrame.first (offset) (DEPRECATED) Select initial periods of time series data based on a date offset. DataFrame.head ([n]) Return the first n rows. DataFrame.idxmax ([axis,Â skipna,Â numeric_only]) Return index of first occurrence of maximum over requested axis. DataFrame.idxmin ([axis,Â skipna,Â numeric_only]) Return index of first occurrence of minimum over requested axis. DataFrame.last (offset) (DEPRECATED) Select final periods of time series data based on a date offset. DataFrame.reindex ([labels,Â index,Â columns,Â ...]) Conform DataFrame to new index with optional filling logic. DataFrame.reindex_like (other[,Â method,Â ...]) Return an object with matching indices as other object. DataFrame.rename ([mapper,Â index,Â columns,Â ...]) Rename columns or index labels. DataFrame.rename_axis ([mapper,Â index,Â ...]) Set the name of the axis for the index or columns. DataFrame.reset_index ([level,Â drop,Â ...]) Reset the index, or a level of it. DataFrame.sample ([n,Â frac,Â replace,Â ...]) Return a random sample of items from an axis of object. DataFrame.set_axis (labels,Â *[,Â axis,Â copy]) Assign desired index to given axis. DataFrame.set_index (keys,Â *[,Â drop,Â append,Â ...]) Set the DataFrame index using existing columns. DataFrame.tail ([n]) Return the last n rows. DataFrame.take (indices[,Â axis]) Return the elements in the given positional indices along an axis. DataFrame.truncate ([before,Â after,Â axis,Â copy]) Truncate a Series or DataFrame before and after some index value. Missing data handling # DataFrame.backfill (*[,Â axis,Â inplace,Â ...]) (DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap. DataFrame.bfill (*[,Â axis,Â inplace,Â limit,Â ...]) Fill NA/NaN values by using the next valid observation to fill the gap. DataFrame.dropna (*[,Â axis,Â how,Â thresh,Â ...]) Remove missing values. DataFrame.ffill (*[,Â axis,Â inplace,Â limit,Â ...]) Fill NA/NaN values by propagating the last valid observation to next valid. DataFrame.fillna ([value,Â method,Â axis,Â ...]) Fill NA/NaN values using the specified method. DataFrame.interpolate ([method,Â axis,Â limit,Â ...]) Fill NaN values using an interpolation method. DataFrame.isna () Detect missing values. DataFrame.isnull () DataFrame.isnull is an alias for DataFrame.isna. DataFrame.notna () Detect existing (non-missing) values. DataFrame.notnull () DataFrame.notnull is an alias for DataFrame.notna. DataFrame.pad (*[,Â axis,Â inplace,Â limit,Â ...]) (DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid. DataFrame.replace ([to_replace,Â value,Â ...]) Replace values given in to_replace with value . Reshaping, sorting, transposing # DataFrame.droplevel (level[,Â axis]) Return Series/DataFrame with requested index / column level(s) removed. DataFrame.pivot (*,Â columns[,Â index,Â values]) Return reshaped DataFrame organized by given index / column values. DataFrame.pivot_table ([values,Â index,Â ...]) Create a spreadsheet-style pivot table as a DataFrame. DataFrame.reorder_levels (order[,Â axis]) Rearrange index levels using input order. DataFrame.sort_values (by,Â *[,Â axis,Â ...]) Sort by the values along either axis. DataFrame.sort_index (*[,Â axis,Â level,Â ...]) Sort object by labels (along an axis). DataFrame.nlargest (n,Â columns[,Â keep]) Return the first n rows ordered by columns in descending order. DataFrame.nsmallest (n,Â columns[,Â keep]) Return the first n rows ordered by columns in ascending order. DataFrame.swaplevel ([i,Â j,Â axis]) Swap levels i and j in a MultiIndex . DataFrame.stack ([level,Â dropna,Â sort,Â ...]) Stack the prescribed level(s) from columns to index. DataFrame.unstack ([level,Â fill_value,Â sort]) Pivot a level of the (necessarily hierarchical) index labels. DataFrame.swapaxes (axis1,Â axis2[,Â copy]) (DEPRECATED) Interchange axes and swap values axes appropriately. DataFrame.melt ([id_vars,Â value_vars,Â ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. DataFrame.explode (column[,Â ignore_index]) Transform each element of a list-like to a row, replicating index values. DataFrame.squeeze ([axis]) Squeeze 1 dimensional axis objects into scalars. DataFrame.to_xarray () Return an xarray object from the pandas object. DataFrame.T The transpose of the DataFrame. DataFrame.transpose (*args[,Â copy]) Transpose index and columns. Combining / comparing / joining / merging # DataFrame.assign (**kwargs) Assign new columns to a DataFrame. DataFrame.compare (other[,Â align_axis,Â ...]) Compare to another DataFrame and show the differences. DataFrame.join (other[,Â on,Â how,Â lsuffix,Â ...]) Join columns of another DataFrame. DataFrame.merge (right[,Â how,Â on,Â left_on,Â ...]) Merge DataFrame or named Series objects with a database-style join. DataFrame.update (other[,Â join,Â overwrite,Â ...]) Modify in place using non-NA values from another DataFrame. Time Series-related # DataFrame.asfreq (freq[,Â method,Â how,Â ...]) Convert time series to specified frequency. DataFrame.asof (where[,Â subset]) Return the last row(s) without any NaNs before where . DataFrame.shift ([periods,Â freq,Â axis,Â ...]) Shift index by desired number of periods with an optional time freq . DataFrame.first_valid_index () Return index for first non-NA value or None, if no non-NA value is found. DataFrame.last_valid_index () Return index for last non-NA value or None, if no non-NA value is found. DataFrame.resample (rule[,Â axis,Â closed,Â ...]) Resample time-series data. DataFrame.to_period ([freq,Â axis,Â copy]) Convert DataFrame from DatetimeIndex to PeriodIndex. DataFrame.to_timestamp ([freq,Â how,Â axis,Â copy]) Cast to DatetimeIndex of timestamps, at beginning of period. DataFrame.tz_convert (tz[,Â axis,Â level,Â copy]) Convert tz-aware axis to target time zone. DataFrame.tz_localize (tz[,Â axis,Â level,Â ...]) Localize tz-naive index of a Series or DataFrame to target time zone. Flags # Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in DataFrame.attrs . Flags (obj,Â *,Â allows_duplicate_labels) Flags that apply to pandas objects. Metadata # DataFrame.attrs is a dictionary for storing global metadata for this DataFrame. Warning DataFrame.attrs is considered experimental and may change without warning. DataFrame.attrs Dictionary of global attributes of this dataset. Plotting # DataFrame.plot is both a callable method and a namespace attribute for specific plotting methods of the form DataFrame.plot.<kind> . DataFrame.plot ([x,Â y,Â kind,Â ax,Â ....]) DataFrame plotting accessor and method DataFrame.plot.area ([x,Â y,Â stacked]) Draw a stacked area plot. DataFrame.plot.bar ([x,Â y]) Vertical bar plot. DataFrame.plot.barh ([x,Â y]) Make a horizontal bar plot. DataFrame.plot.box ([by]) Make a box plot of the DataFrame columns. DataFrame.plot.density ([bw_method,Â ind]) Generate Kernel Density Estimate plot using Gaussian kernels. DataFrame.plot.hexbin (x,Â y[,Â C,Â ...]) Generate a hexagonal binning plot. DataFrame.plot.hist ([by,Â bins]) Draw one histogram of the DataFrame's columns. DataFrame.plot.kde ([bw_method,Â ind]) Generate Kernel Density Estimate plot using Gaussian kernels. DataFrame.plot.line ([x,Â y]) Plot Series or DataFrame as lines. DataFrame.plot.pie (**kwargs) Generate a pie plot. DataFrame.plot.scatter (x,Â y[,Â s,Â c]) Create a scatter plot with varying marker point size and color. DataFrame.boxplot ([column,Â by,Â ax,Â ...]) Make a box plot from DataFrame columns. DataFrame.hist ([column,Â by,Â grid,Â ...]) Make a histogram of the DataFrame's columns. Sparse accessor # Sparse-dtype specific methods and attributes are provided under the DataFrame.sparse accessor. DataFrame.sparse.density Ratio of non-sparse points to total (dense) data points. DataFrame.sparse.from_spmatrix (data[,Â ...]) Create a new DataFrame from a scipy sparse matrix. DataFrame.sparse.to_coo () Return the contents of the frame as a sparse SciPy COO matrix. DataFrame.sparse.to_dense () Convert a DataFrame with sparse values to dense. Serialization / IO / conversion # DataFrame.from_dict (data[,Â orient,Â dtype,Â ...]) Construct DataFrame from dict of array-like or dicts. DataFrame.from_records (data[,Â index,Â ...]) Convert structured or record ndarray to DataFrame. DataFrame.to_orc ([path,Â engine,Â index,Â ...]) Write a DataFrame to the ORC format. DataFrame.to_parquet ([path,Â engine,Â ...]) Write a DataFrame to the binary parquet format. DataFrame.to_pickle (path,Â *[,Â compression,Â ...]) Pickle (serialize) object to file. DataFrame.to_csv ([path_or_buf,Â sep,Â na_rep,Â ...]) Write object to a comma-separated values (csv) file. DataFrame.to_hdf (path_or_buf,Â *,Â key[,Â ...]) Write the contained data to an HDF5 file using HDFStore. DataFrame.to_sql (name,Â con,Â *[,Â schema,Â ...]) Write records stored in a DataFrame to a SQL database. DataFrame.to_dict ([orient,Â into,Â index]) Convert the DataFrame to a dictionary. DataFrame.to_excel (excel_writer,Â *[,Â ...]) Write object to an Excel sheet. DataFrame.to_json ([path_or_buf,Â orient,Â ...]) Convert the object to a JSON string. DataFrame.to_html ([buf,Â columns,Â col_space,Â ...]) Render a DataFrame as an HTML table. DataFrame.to_feather (path,Â **kwargs) Write a DataFrame to the binary Feather format. DataFrame.to_latex ([buf,Â columns,Â header,Â ...]) Render object to a LaTeX tabular, longtable, or nested table. DataFrame.to_stata (path,Â *[,Â convert_dates,Â ...]) Export DataFrame object to Stata dta format. DataFrame.to_gbq (destination_table,Â *[,Â ...]) (DEPRECATED) Write a DataFrame to a Google BigQuery table. DataFrame.to_records ([index,Â column_dtypes,Â ...]) Convert DataFrame to a NumPy record array. DataFrame.to_string ([buf,Â columns,Â ...]) Render a DataFrame to a console-friendly tabular output. DataFrame.to_clipboard (*[,Â excel,Â sep]) Copy object to the system clipboard. DataFrame.to_markdown ([buf,Â mode,Â index,Â ...]) Print DataFrame in Markdown-friendly format. DataFrame.style Returns a Styler object. DataFrame.__dataframe__ ([nan_as_null,Â ...]) Return the dataframe interchange object implementing the interchange protocol. previous pandas.Series.to_markdown next pandas.DataFrame On this page Constructor Attributes and underlying data Conversion Indexing, iteration Binary operator functions Function application, GroupBy & window Computations / descriptive stats Reindexing / selection / label manipulation Missing data handling Reshaping, sorting, transposing Combining / comparing / joining / merging Time Series-related Flags Metadata Plotting Sparse accessor Serialization / IO / conversion Show Source",
    "url": "https://pandas.pydata.org/docs/reference/frame.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Missing values Missing values # NA is the way to represent missing values for nullable dtypes (see below): NA NA (\"not available\") missing value indicator. NaT is the missing value for timedelta and datetime data (see below): NaT (N)ot-(A)-(T)ime, the time equivalent of NaN. previous pandas.test next pandas.NA Show Source",
    "url": "https://pandas.pydata.org/docs/reference/missing_value.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Working... Working with missing data # Values considered âmissingâ # pandas uses different sentinel values to represent a missing (also referred to as NA) depending on the data type. numpy.nan for NumPy data types. The disadvantage of using NumPy data types is that the original data type will be coerced to np.float64 or object . In [1]: pd . Series ([ 1 , 2 ], dtype = np . int64 ) . reindex ([ 0 , 1 , 2 ]) Out[1]: 0 1.0 1 2.0 2 NaN dtype: float64 In [2]: pd . Series ([ True , False ], dtype = np . bool_ ) . reindex ([ 0 , 1 , 2 ]) Out[2]: 0 True 1 False 2 NaN dtype: object NaT for NumPy np.datetime64 , np.timedelta64 , and PeriodDtype . For typing applications, use api.types.NaTType . In [3]: pd . Series ([ 1 , 2 ], dtype = np . dtype ( \"timedelta64[ns]\" )) . reindex ([ 0 , 1 , 2 ]) Out[3]: 0 0 days 00:00:00.000000001 1 0 days 00:00:00.000000002 2 NaT dtype: timedelta64[ns] In [4]: pd . Series ([ 1 , 2 ], dtype = np . dtype ( \"datetime64[ns]\" )) . reindex ([ 0 , 1 , 2 ]) Out[4]: 0 1970-01-01 00:00:00.000000001 1 1970-01-01 00:00:00.000000002 2 NaT dtype: datetime64[ns] In [5]: pd . Series ([ \"2020\" , \"2020\" ], dtype = pd . PeriodDtype ( \"D\" )) . reindex ([ 0 , 1 , 2 ]) Out[5]: 0 2020-01-01 1 2020-01-01 2 NaT dtype: period[D] NA for StringDtype , Int64Dtype (and other bit widths), Float64Dtype`(and other bit widths), :class:`BooleanDtype and ArrowDtype . These types will maintain the original data type of the data. For typing applications, use api.types.NAType . In [6]: pd . Series ([ 1 , 2 ], dtype = \"Int64\" ) . reindex ([ 0 , 1 , 2 ]) Out[6]: 0 1 1 2 2 <NA> dtype: Int64 In [7]: pd . Series ([ True , False ], dtype = \"boolean[pyarrow]\" ) . reindex ([ 0 , 1 , 2 ]) Out[7]: 0 True 1 False 2 <NA> dtype: bool[pyarrow] To detect these missing value, use the isna() or notna() methods. In [8]: ser = pd . Series ([ pd . Timestamp ( \"2020-01-01\" ), pd . NaT ]) In [9]: ser Out[9]: 0 2020-01-01 1 NaT dtype: datetime64[ns] In [10]: pd . isna ( ser ) Out[10]: 0 False 1 True dtype: bool Note isna() or notna() will also consider None a missing value. In [11]: ser = pd . Series ([ 1 , None ], dtype = object ) In [12]: ser Out[12]: 0 1 1 None dtype: object In [13]: pd . isna ( ser ) Out[13]: 0 False 1 True dtype: bool Warning Equality compaisons between np.nan , NaT , and NA do not act like None In [14]: None == None # noqa: E711 Out[14]: True In [15]: np . nan == np . nan Out[15]: False In [16]: pd . NaT == pd . NaT Out[16]: False In [17]: pd . NA == pd . NA Out[17]: <NA> Therefore, an equality comparison between a DataFrame or Series with one of these missing values does not provide the same information as isna() or notna() . In [18]: ser = pd . Series ([ True , None ], dtype = \"boolean[pyarrow]\" ) In [19]: ser == pd . NA Out[19]: 0 <NA> 1 <NA> dtype: bool[pyarrow] In [20]: pd . isna ( ser ) Out[20]: 0 False 1 True dtype: bool NA semantics # Warning Experimental: the behaviour of NA` can still change without warning. Starting from pandas 1.0, an experimental NA value (singleton) is available to represent scalar missing values. The goal of NA is provide a âmissingâ indicator that can be used consistently across data types (instead of np.nan , None or pd.NaT depending on the data type). For example, when having missing values in a Series with the nullable integer dtype, it will use NA : In [21]: s = pd . Series ([ 1 , 2 , None ], dtype = \"Int64\" ) In [22]: s Out[22]: 0 1 1 2 2 <NA> dtype: Int64 In [23]: s [ 2 ] Out[23]: <NA> In [24]: s [ 2 ] is pd . NA Out[24]: True Currently, pandas does not yet use those data types using NA by default a DataFrame or Series , so you need to specify the dtype explicitly. An easy way to convert to those dtypes is explained in the conversion section . Propagation in arithmetic and comparison operations # In general, missing values propagate in operations involving NA . When one of the operands is unknown, the outcome of the operation is also unknown. For example, NA propagates in arithmetic operations, similarly to np.nan : In [25]: pd . NA + 1 Out[25]: <NA> In [26]: \"a\" * pd . NA Out[26]: <NA> There are a few special cases when the result is known, even when one of the operands is NA . In [27]: pd . NA ** 0 Out[27]: 1 In [28]: 1 ** pd . NA Out[28]: 1 In equality and comparison operations, NA also propagates. This deviates from the behaviour of np.nan , where comparisons with np.nan always return False . In [29]: pd . NA == 1 Out[29]: <NA> In [30]: pd . NA == pd . NA Out[30]: <NA> In [31]: pd . NA < 2.5 Out[31]: <NA> To check if a value is equal to NA , use isna() In [32]: pd . isna ( pd . NA ) Out[32]: True Note An exception on this basic propagation rule are reductions (such as the mean or the minimum), where pandas defaults to skipping missing values. See the calculation section for more. Logical operations # For logical operations, NA follows the rules of the three-valued logic (or Kleene logic , similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required. For example, for the logical âorâ operation ( | ), if one of the operands is True , we already know the result will be True , regardless of the other value (so regardless the missing value would be True or False ). In this case, NA does not propagate: In [33]: True | False Out[33]: True In [34]: True | pd . NA Out[34]: True In [35]: pd . NA | True Out[35]: True On the other hand, if one of the operands is False , the result depends on the value of the other operand. Therefore, in this case NA propagates: In [36]: False | True Out[36]: True In [37]: False | False Out[37]: False In [38]: False | pd . NA Out[38]: <NA> The behaviour of the logical âandâ operation ( & ) can be derived using similar logic (where now NA will not propagate if one of the operands is already False ): In [39]: False & True Out[39]: False In [40]: False & False Out[40]: False In [41]: False & pd . NA Out[41]: False In [42]: True & True Out[42]: True In [43]: True & False Out[43]: False In [44]: True & pd . NA Out[44]: <NA> NA in a boolean context # Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value. In [45]: bool ( pd . NA ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In [ 45 ], line 1 ----> 1 bool ( pd . NA ) File ~/work/pandas/pandas/pandas/_libs/missing.pyx:392, in pandas._libs.missing.NAType.__bool__ () TypeError : boolean value of NA is ambiguous This also means that NA cannot be used in a context where it is evaluated to a boolean, such as if condition: ... where condition can potentially be NA . In such cases, isna() can be used to check for NA or condition being NA can be avoided, for example by filling missing values beforehand. A similar situation occurs when using Series or DataFrame objects in if statements, see Using if/truth statements with pandas . NumPy ufuncs # pandas.NA implements NumPyâs __array_ufunc__ protocol. Most ufuncs work with NA , and generally return NA : In [46]: np . log ( pd . NA ) Out[46]: <NA> In [47]: np . add ( pd . NA , 1 ) Out[47]: <NA> Warning Currently, ufuncs involving an ndarray and NA will return an object-dtype filled with NA values. In [48]: a = np . array ([ 1 , 2 , 3 ]) In [49]: np . greater ( a , pd . NA ) Out[49]: array([<NA>, <NA>, <NA>], dtype=object) The return type here may change to return a different array type in the future. See DataFrame interoperability with NumPy functions for more on ufuncs. Conversion # If you have a DataFrame or Series using np.nan , Series.convert_dtypes() and DataFrame.convert_dtypes() in DataFrame that can convert data to use the data types that use NA such as Int64Dtype or ArrowDtype . This is especially helpful after reading in data sets from IO methods where data types were inferred. In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns. In [50]: import io In [51]: data = io . StringIO ( \"a,b \\n ,True \\n 2,\" ) In [52]: df = pd . read_csv ( data ) In [53]: df . dtypes Out[53]: a float64 b object dtype: object In [54]: df_conv = df . convert_dtypes () In [55]: df_conv Out[55]: a b 0 <NA> True 1 2 <NA> In [56]: df_conv . dtypes Out[56]: a Int64 b boolean dtype: object Inserting missing data # You can insert missing values by simply assigning to a Series or DataFrame . The missing value sentinel used will be chosen based on the dtype. In [57]: ser = pd . Series ([ 1. , 2. , 3. ]) In [58]: ser . loc [ 0 ] = None In [59]: ser Out[59]: 0 NaN 1 2.0 2 3.0 dtype: float64 In [60]: ser = pd . Series ([ pd . Timestamp ( \"2021\" ), pd . Timestamp ( \"2021\" )]) In [61]: ser . iloc [ 0 ] = np . nan In [62]: ser Out[62]: 0 NaT 1 2021-01-01 dtype: datetime64[ns] In [63]: ser = pd . Series ([ True , False ], dtype = \"boolean[pyarrow]\" ) In [64]: ser . iloc [ 0 ] = None In [65]: ser Out[65]: 0 <NA> 1 False dtype: bool[pyarrow] For object types, pandas will use the value given: In [66]: s = pd . Series ([ \"a\" , \"b\" , \"c\" ], dtype = object ) In [67]: s . loc [ 0 ] = None In [68]: s . loc [ 1 ] = np . nan In [69]: s Out[69]: 0 None 1 NaN 2 c dtype: object Calculations with missing data # Missing values propagate through arithmetic operations between pandas objects. In [70]: ser1 = pd . Series ([ np . nan , np . nan , 2 , 3 ]) In [71]: ser2 = pd . Series ([ np . nan , 1 , np . nan , 4 ]) In [72]: ser1 Out[72]: 0 NaN 1 NaN 2 2.0 3 3.0 dtype: float64 In [73]: ser2 Out[73]: 0 NaN 1 1.0 2 NaN 3 4.0 dtype: float64 In [74]: ser1 + ser2 Out[74]: 0 NaN 1 NaN 2 NaN 3 7.0 dtype: float64 The descriptive statistics and computational methods discussed in the data structure overview (and listed here and here ) are all account for missing data. When summing data, NA values or empty data will be treated as zero. In [75]: pd . Series ([ np . nan ]) . sum () Out[75]: 0.0 In [76]: pd . Series ([], dtype = \"float64\" ) . sum () Out[76]: 0.0 When taking the product, NA values or empty data will be treated as 1. In [77]: pd . Series ([ np . nan ]) . prod () Out[77]: 1.0 In [78]: pd . Series ([], dtype = \"float64\" ) . prod () Out[78]: 1.0 Cumulative methods like cumsum() and cumprod() ignore NA values by default preserve them in the result. This behavior can be changed with skipna Cumulative methods like cumsum() and cumprod() ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna=False . In [79]: ser = pd . Series ([ 1 , np . nan , 3 , np . nan ]) In [80]: ser Out[80]: 0 1.0 1 NaN 2 3.0 3 NaN dtype: float64 In [81]: ser . cumsum () Out[81]: 0 1.0 1 NaN 2 4.0 3 NaN dtype: float64 In [82]: ser . cumsum ( skipna = False ) Out[82]: 0 1.0 1 NaN 2 NaN 3 NaN dtype: float64 Dropping missing data # dropna() dropa rows or columns with missing data. In [83]: df = pd . DataFrame ([[ np . nan , 1 , 2 ], [ 1 , 2 , np . nan ], [ 1 , 2 , 3 ]]) In [84]: df Out[84]: 0 1 2 0 NaN 1 2.0 1 1.0 2 NaN 2 1.0 2 3.0 In [85]: df . dropna () Out[85]: 0 1 2 2 1.0 2 3.0 In [86]: df . dropna ( axis = 1 ) Out[86]: 1 0 1 1 2 2 2 In [87]: ser = pd . Series ([ 1 , pd . NA ], dtype = \"int64[pyarrow]\" ) In [88]: ser . dropna () Out[88]: 0 1 dtype: int64[pyarrow] Filling missing data # Filling by value # fillna() replaces NA values with non-NA data. Replace NA with a scalar value In [89]: data = { \"np\" : [ 1.0 , np . nan , np . nan , 2 ], \"arrow\" : pd . array ([ 1.0 , pd . NA , pd . NA , 2 ], dtype = \"float64[pyarrow]\" )} In [90]: df = pd . DataFrame ( data ) In [91]: df Out[91]: np arrow 0 1.0 1.0 1 NaN <NA> 2 NaN <NA> 3 2.0 2.0 In [92]: df . fillna ( 0 ) Out[92]: np arrow 0 1.0 1.0 1 0.0 0.0 2 0.0 0.0 3 2.0 2.0 Fill gaps forward or backward In [93]: df . ffill () Out[93]: np arrow 0 1.0 1.0 1 1.0 1.0 2 1.0 1.0 3 2.0 2.0 In [94]: df . bfill () Out[94]: np arrow 0 1.0 1.0 1 2.0 2.0 2 2.0 2.0 3 2.0 2.0 Limit the number of NA values filled In [95]: df . ffill ( limit = 1 ) Out[95]: np arrow 0 1.0 1.0 1 1.0 1.0 2 NaN <NA> 3 2.0 2.0 NA values can be replaced with corresponding value from a Series or DataFrame where the index and column aligns between the original object and the filled object. In [96]: dff = pd . DataFrame ( np . arange ( 30 , dtype = np . float64 ) . reshape ( 10 , 3 ), columns = list ( \"ABC\" )) In [97]: dff . iloc [ 3 : 5 , 0 ] = np . nan In [98]: dff . iloc [ 4 : 6 , 1 ] = np . nan In [99]: dff . iloc [ 5 : 8 , 2 ] = np . nan In [100]: dff Out[100]: A B C 0 0.0 1.0 2.0 1 3.0 4.0 5.0 2 6.0 7.0 8.0 3 NaN 10.0 11.0 4 NaN NaN 14.0 5 15.0 NaN NaN 6 18.0 19.0 NaN 7 21.0 22.0 NaN 8 24.0 25.0 26.0 9 27.0 28.0 29.0 In [101]: dff . fillna ( dff . mean ()) Out[101]: A B C 0 0.00 1.0 2.000000 1 3.00 4.0 5.000000 2 6.00 7.0 8.000000 3 14.25 10.0 11.000000 4 14.25 14.5 14.000000 5 15.00 14.5 13.571429 6 18.00 19.0 13.571429 7 21.00 22.0 13.571429 8 24.00 25.0 26.000000 9 27.00 28.0 29.000000 Note DataFrame.where() can also be used to fill NA values.Same result as above. In [102]: dff . where ( pd . notna ( dff ), dff . mean (), axis = \"columns\" ) Out[102]: A B C 0 0.00 1.0 2.000000 1 3.00 4.0 5.000000 2 6.00 7.0 8.000000 3 14.25 10.0 11.000000 4 14.25 14.5 14.000000 5 15.00 14.5 13.571429 6 18.00 19.0 13.571429 7 21.00 22.0 13.571429 8 24.00 25.0 26.000000 9 27.00 28.0 29.000000 Interpolation # DataFrame.interpolate() and Series.interpolate() fills NA values using various interpolation methods. In [103]: df = pd . DataFrame ( .....: { .....: \"A\" : [ 1 , 2.1 , np . nan , 4.7 , 5.6 , 6.8 ], .....: \"B\" : [ 0.25 , np . nan , np . nan , 4 , 12.2 , 14.4 ], .....: } .....: ) .....: In [104]: df Out[104]: A B 0 1.0 0.25 1 2.1 NaN 2 NaN NaN 3 4.7 4.00 4 5.6 12.20 5 6.8 14.40 In [105]: df . interpolate () Out[105]: A B 0 1.0 0.25 1 2.1 1.50 2 3.4 2.75 3 4.7 4.00 4 5.6 12.20 5 6.8 14.40 In [106]: idx = pd . date_range ( \"2020-01-01\" , periods = 10 , freq = \"D\" ) In [107]: data = np . random . default_rng ( 2 ) . integers ( 0 , 10 , 10 ) . astype ( np . float64 ) In [108]: ts = pd . Series ( data , index = idx ) In [109]: ts . iloc [[ 1 , 2 , 5 , 6 , 9 ]] = np . nan In [110]: ts Out[110]: 2020-01-01 8.0 2020-01-02 NaN 2020-01-03 NaN 2020-01-04 2.0 2020-01-05 4.0 2020-01-06 NaN 2020-01-07 NaN 2020-01-08 0.0 2020-01-09 3.0 2020-01-10 NaN Freq: D, dtype: float64 In [111]: ts . plot () Out[111]: <Axes: > In [112]: ts . interpolate () Out[112]: 2020-01-01 8.000000 2020-01-02 6.000000 2020-01-03 4.000000 2020-01-04 2.000000 2020-01-05 4.000000 2020-01-06 2.666667 2020-01-07 1.333333 2020-01-08 0.000000 2020-01-09 3.000000 2020-01-10 3.000000 Freq: D, dtype: float64 In [113]: ts . interpolate () . plot () Out[113]: <Axes: > Interpolation relative to a Timestamp in the DatetimeIndex is available by setting method=\"time\" In [114]: ts2 = ts . iloc [[ 0 , 1 , 3 , 7 , 9 ]] In [115]: ts2 Out[115]: 2020-01-01 8.0 2020-01-02 NaN 2020-01-04 2.0 2020-01-08 0.0 2020-01-10 NaN dtype: float64 In [116]: ts2 . interpolate () Out[116]: 2020-01-01 8.0 2020-01-02 5.0 2020-01-04 2.0 2020-01-08 0.0 2020-01-10 0.0 dtype: float64 In [117]: ts2 . interpolate ( method = \"time\" ) Out[117]: 2020-01-01 8.0 2020-01-02 6.0 2020-01-04 2.0 2020-01-08 0.0 2020-01-10 0.0 dtype: float64 For a floating-point index, use method='values' : In [118]: idx = [ 0.0 , 1.0 , 10.0 ] In [119]: ser = pd . Series ([ 0.0 , np . nan , 10.0 ], idx ) In [120]: ser Out[120]: 0.0 0.0 1.0 NaN 10.0 10.0 dtype: float64 In [121]: ser . interpolate () Out[121]: 0.0 0.0 1.0 5.0 10.0 10.0 dtype: float64 In [122]: ser . interpolate ( method = \"values\" ) Out[122]: 0.0 0.0 1.0 1.0 10.0 10.0 dtype: float64 If you have scipy installed, you can pass the name of a 1-d interpolation routine to method . as specified in the scipy interpolation documentation and reference guide . The appropriate interpolation method will depend on the data type. Tip If you are dealing with a time series that is growing at an increasing rate, use method='barycentric' . If you have values approximating a cumulative distribution function, use method='pchip' . To fill missing values with goal of smooth plotting use method='akima' . In [123]: df = pd . DataFrame ( .....: { .....: \"A\" : [ 1 , 2.1 , np . nan , 4.7 , 5.6 , 6.8 ], .....: \"B\" : [ 0.25 , np . nan , np . nan , 4 , 12.2 , 14.4 ], .....: } .....: ) .....: In [124]: df Out[124]: A B 0 1.0 0.25 1 2.1 NaN 2 NaN NaN 3 4.7 4.00 4 5.6 12.20 5 6.8 14.40 In [125]: df . interpolate ( method = \"barycentric\" ) Out[125]: A B 0 1.00 0.250 1 2.10 -7.660 2 3.53 -4.515 3 4.70 4.000 4 5.60 12.200 5 6.80 14.400 In [126]: df . interpolate ( method = \"pchip\" ) Out[126]: A B 0 1.00000 0.250000 1 2.10000 0.672808 2 3.43454 1.928950 3 4.70000 4.000000 4 5.60000 12.200000 5 6.80000 14.400000 In [127]: df . interpolate ( method = \"akima\" ) Out[127]: A B 0 1.000000 0.250000 1 2.100000 -0.873316 2 3.406667 0.320034 3 4.700000 4.000000 4 5.600000 12.200000 5 6.800000 14.400000 When interpolating via a polynomial or spline approximation, you must also specify the degree or order of the approximation: In [128]: df . interpolate ( method = \"spline\" , order = 2 ) Out[128]: A B 0 1.000000 0.250000 1 2.100000 -0.428598 2 3.404545 1.206900 3 4.700000 4.000000 4 5.600000 12.200000 5 6.800000 14.400000 In [129]: df . interpolate ( method = \"polynomial\" , order = 2 ) Out[129]: A B 0 1.000000 0.250000 1 2.100000 -2.703846 2 3.451351 -1.453846 3 4.700000 4.000000 4 5.600000 12.200000 5 6.800000 14.400000 Comparing several methods. In [130]: np . random . seed ( 2 ) In [131]: ser = pd . Series ( np . arange ( 1 , 10.1 , 0.25 ) ** 2 + np . random . randn ( 37 )) In [132]: missing = np . array ([ 4 , 13 , 14 , 15 , 16 , 17 , 18 , 20 , 29 ]) In [133]: ser . iloc [ missing ] = np . nan In [134]: methods = [ \"linear\" , \"quadratic\" , \"cubic\" ] In [135]: df = pd . DataFrame ({ m : ser . interpolate ( method = m ) for m in methods }) In [136]: df . plot () Out[136]: <Axes: > Interpolating new observations from expanding data with Series.reindex() . In [137]: ser = pd . Series ( np . sort ( np . random . uniform ( size = 100 ))) # interpolate at new_index In [138]: new_index = ser . index . union ( pd . Index ([ 49.25 , 49.5 , 49.75 , 50.25 , 50.5 , 50.75 ])) In [139]: interp_s = ser . reindex ( new_index ) . interpolate ( method = \"pchip\" ) In [140]: interp_s . loc [ 49 : 51 ] Out[140]: 49.00 0.471410 49.25 0.476841 49.50 0.481780 49.75 0.485998 50.00 0.489266 50.25 0.491814 50.50 0.493995 50.75 0.495763 51.00 0.497074 dtype: float64 Interpolation limits # interpolate() accepts a limit keyword argument to limit the number of consecutive NaN values filled since the last valid observation In [141]: ser = pd . Series ([ np . nan , np . nan , 5 , np . nan , np . nan , np . nan , 13 , np . nan , np . nan ]) In [142]: ser Out[142]: 0 NaN 1 NaN 2 5.0 3 NaN 4 NaN 5 NaN 6 13.0 7 NaN 8 NaN dtype: float64 In [143]: ser . interpolate () Out[143]: 0 NaN 1 NaN 2 5.0 3 7.0 4 9.0 5 11.0 6 13.0 7 13.0 8 13.0 dtype: float64 In [144]: ser . interpolate ( limit = 1 ) Out[144]: 0 NaN 1 NaN 2 5.0 3 7.0 4 NaN 5 NaN 6 13.0 7 13.0 8 NaN dtype: float64 By default, NaN values are filled in a forward direction. Use limit_direction parameter to fill backward or from both directions. In [145]: ser . interpolate ( limit = 1 , limit_direction = \"backward\" ) Out[145]: 0 NaN 1 5.0 2 5.0 3 NaN 4 NaN 5 11.0 6 13.0 7 NaN 8 NaN dtype: float64 In [146]: ser . interpolate ( limit = 1 , limit_direction = \"both\" ) Out[146]: 0 NaN 1 5.0 2 5.0 3 7.0 4 NaN 5 11.0 6 13.0 7 13.0 8 NaN dtype: float64 In [147]: ser . interpolate ( limit_direction = \"both\" ) Out[147]: 0 5.0 1 5.0 2 5.0 3 7.0 4 9.0 5 11.0 6 13.0 7 13.0 8 13.0 dtype: float64 By default, NaN values are filled whether they are surrounded by existing valid values or outside existing valid values. The limit_area parameter restricts filling to either inside or outside values. # fill one consecutive inside value in both directions In [148]: ser . interpolate ( limit_direction = \"both\" , limit_area = \"inside\" , limit = 1 ) Out[148]: 0 NaN 1 NaN 2 5.0 3 7.0 4 NaN 5 11.0 6 13.0 7 NaN 8 NaN dtype: float64 # fill all consecutive outside values backward In [149]: ser . interpolate ( limit_direction = \"backward\" , limit_area = \"outside\" ) Out[149]: 0 5.0 1 5.0 2 5.0 3 NaN 4 NaN 5 NaN 6 13.0 7 NaN 8 NaN dtype: float64 # fill all consecutive outside values in both directions In [150]: ser . interpolate ( limit_direction = \"both\" , limit_area = \"outside\" ) Out[150]: 0 5.0 1 5.0 2 5.0 3 NaN 4 NaN 5 NaN 6 13.0 7 13.0 8 13.0 dtype: float64 Replacing values # Series.replace() and DataFrame.replace() can be used similar to Series.fillna() and DataFrame.fillna() to replace or insert missing values. In [151]: df = pd . DataFrame ( np . eye ( 3 )) In [152]: df Out[152]: 0 1 2 0 1.0 0.0 0.0 1 0.0 1.0 0.0 2 0.0 0.0 1.0 In [153]: df_missing = df . replace ( 0 , np . nan ) In [154]: df_missing Out[154]: 0 1 2 0 1.0 NaN NaN 1 NaN 1.0 NaN 2 NaN NaN 1.0 In [155]: df_filled = df_missing . replace ( np . nan , 2 ) In [156]: df_filled Out[156]: 0 1 2 0 1.0 2.0 2.0 1 2.0 1.0 2.0 2 2.0 2.0 1.0 Replacing more than one value is possible by passing a list. In [157]: df_filled . replace ([ 1 , 44 ], [ 2 , 28 ]) Out[157]: 0 1 2 0 2.0 2.0 2.0 1 2.0 2.0 2.0 2 2.0 2.0 2.0 Replacing using a mapping dict. In [158]: df_filled . replace ({ 1 : 44 , 2 : 28 }) Out[158]: 0 1 2 0 44.0 28.0 28.0 1 28.0 44.0 28.0 2 28.0 28.0 44.0 Regular expression replacement # Note Python strings prefixed with the r character such as r'hello world' are ârawâ strings . They have different semantics regarding backslashes than strings without this prefix. Backslashes in raw strings will be interpreted as an escaped backslash, e.g., r'\\' == '\\\\' . Replace the â.â with NaN In [159]: d = { \"a\" : list ( range ( 4 )), \"b\" : list ( \"ab..\" ), \"c\" : [ \"a\" , \"b\" , np . nan , \"d\" ]} In [160]: df = pd . DataFrame ( d ) In [161]: df . replace ( \".\" , np . nan ) Out[161]: a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d Replace the â.â with NaN with regular expression that removes surrounding whitespace In [162]: df . replace ( r \"\\s*\\.\\s*\" , np . nan , regex = True ) Out[162]: a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d Replace with a list of regexes. In [163]: df . replace ([ r \"\\.\" , r \"(a)\" ], [ \"dot\" , r \"\\1stuff\" ], regex = True ) Out[163]: a b c 0 0 astuff astuff 1 1 b b 2 2 dot NaN 3 3 dot d Replace with a regex in a mapping dict. In [164]: df . replace ({ \"b\" : r \"\\s*\\.\\s*\" }, { \"b\" : np . nan }, regex = True ) Out[164]: a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d Pass nested dictionaries of regular expressions that use the regex keyword. In [165]: df . replace ({ \"b\" : { \"b\" : r \"\" }}, regex = True ) Out[165]: a b c 0 0 a a 1 1 b 2 2 . NaN 3 3 . d In [166]: df . replace ( regex = { \"b\" : { r \"\\s*\\.\\s*\" : np . nan }}) Out[166]: a b c 0 0 a a 1 1 b b 2 2 NaN NaN 3 3 NaN d In [167]: df . replace ({ \"b\" : r \"\\s*(\\.)\\s*\" }, { \"b\" : r \"\\1ty\" }, regex = True ) Out[167]: a b c 0 0 a a 1 1 b b 2 2 .ty NaN 3 3 .ty d Pass a list of regular expressions that will replace matches with a scalar. In [168]: df . replace ([ r \"\\s*\\.\\s*\" , r \"a|b\" ], \"placeholder\" , regex = True ) Out[168]: a b c 0 0 placeholder placeholder 1 1 placeholder placeholder 2 2 placeholder NaN 3 3 placeholder d All of the regular expression examples can also be passed with the to_replace argument as the regex argument. In this case the value argument must be passed explicitly by name or regex must be a nested dictionary. In [169]: df . replace ( regex = [ r \"\\s*\\.\\s*\" , r \"a|b\" ], value = \"placeholder\" ) Out[169]: a b c 0 0 placeholder placeholder 1 1 placeholder placeholder 2 2 placeholder NaN 3 3 placeholder d Note A regular expression object from re.compile is a valid input as well. previous Working with text data next Duplicate Labels On this page Values considered âmissingâ NA semantics Propagation in arithmetic and comparison operations Logical operations NA in a boolean context NumPy ufuncs Conversion Inserting missing data Calculations with missing data Dropping missing data Filling missing data Filling by value Interpolation Interpolation limits Replacing values Regular expression replacement Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/missing_data.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Indexing... Indexing and selecting data # The axis labeling information in pandas objects serves many purposes: Identifies data (i.e. provides metadata ) using known indicators, important for analysis, visualization, and interactive console display. Enables automatic and explicit data alignment. Allows intuitive getting and setting of subsets of the data set. In this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area. Note The Python and NumPy indexing operators [] and attribute operator . provide quick and easy access to pandas data structures across a wide range of use cases. This makes interactive work intuitive, as thereâs little new to learn if you already know how to deal with Python dictionaries and NumPy arrays. However, since the type of the data to be accessed isnât known in advance, directly using standard operators has some optimization limits. For production code, we recommended that you take advantage of the optimized pandas data access methods exposed in this chapter. Warning Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy . See the MultiIndex / Advanced Indexing for MultiIndex and more advanced indexing documentation. See the cookbook for some advanced strategies. Different choices for indexing # Object selection has had a number of user-requested additions in order to support more explicit location based indexing. pandas now supports three types of multi-axis indexing. .loc is primarily label based, but may also be used with a boolean array. .loc will raise KeyError when the items are not found. Allowed inputs are: A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.). A list or array of labels ['a', 'b', 'c'] . A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels and Endpoints are inclusive .) A boolean array (any NA values will be treated as False ). A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). A tuple of row (and column) indices whose elements are one of the above inputs. See more at Selection by Label . .iloc is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with Python/NumPy slice semantics). Allowed inputs are: An integer e.g. 5 . A list or array of integers [4, 3, 0] . A slice object with ints 1:7 . A boolean array (any NA values will be treated as False ). A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). A tuple of row (and column) indices whose elements are one of the above inputs. See more at Selection by Position , Advanced Indexing and Advanced Hierarchical . .loc , .iloc , and also [] indexing can accept a callable as indexer. See more at Selection By Callable . Note Destructuring tuple keys into row (and column) indexes occurs before callables are applied, so you cannot return a tuple from a callable to index both rows and columns. Getting values from an object with multi-axes selection uses the following notation (using .loc as an example, but the following applies to .iloc as well). Any of the axes accessors may be the null slice : . Axes left out of the specification are assumed to be : , e.g. p.loc['a'] is equivalent to p.loc['a', :] . In [1]: ser = pd . Series ( range ( 5 ), index = list ( \"abcde\" )) In [2]: ser . loc [[ \"a\" , \"c\" , \"e\" ]] Out[2]: a 0 c 2 e 4 dtype: int64 In [3]: df = pd . DataFrame ( np . arange ( 25 ) . reshape ( 5 , 5 ), index = list ( \"abcde\" ), columns = list ( \"abcde\" )) In [4]: df . loc [[ \"a\" , \"c\" , \"e\" ], [ \"b\" , \"d\" ]] Out[4]: b d a 1 3 c 11 13 e 21 23 Basics # As mentioned when introducing the data structures in the last section , the primary function of indexing with [] (a.k.a. __getitem__ for those familiar with implementing class behavior in Python) is selecting out lower-dimensional slices. The following table shows return type values when indexing pandas objects with [] : Object Type Selection Return Value Type Series series[label] scalar value DataFrame frame[colname] Series corresponding to colname Here we construct a simple time series data set to use for illustrating the indexing functionality: In [5]: dates = pd . date_range ( '1/1/2000' , periods = 8 ) In [6]: df = pd . DataFrame ( np . random . randn ( 8 , 4 ), ...: index = dates , columns = [ 'A' , 'B' , 'C' , 'D' ]) ...: In [7]: df Out[7]: A B C D 2000-01-01 0.469112 -0.282863 -1.509059 -1.135632 2000-01-02 1.212112 -0.173215 0.119209 -1.044236 2000-01-03 -0.861849 -2.104569 -0.494929 1.071804 2000-01-04 0.721555 -0.706771 -1.039575 0.271860 2000-01-05 -0.424972 0.567020 0.276232 -1.087401 2000-01-06 -0.673690 0.113648 -1.478427 0.524988 2000-01-07 0.404705 0.577046 -1.715002 -1.039268 2000-01-08 -0.370647 -1.157892 -1.344312 0.844885 Note None of the indexing functionality is time series specific unless specifically stated. Thus, as per above, we have the most basic indexing using [] : In [8]: s = df [ 'A' ] In [9]: s [ dates [ 5 ]] Out[9]: -0.6736897080883706 You can pass a list of columns to [] to select columns in that order. If a column is not contained in the DataFrame, an exception will be raised. Multiple columns can also be set in this manner: In [10]: df Out[10]: A B C D 2000-01-01 0.469112 -0.282863 -1.509059 -1.135632 2000-01-02 1.212112 -0.173215 0.119209 -1.044236 2000-01-03 -0.861849 -2.104569 -0.494929 1.071804 2000-01-04 0.721555 -0.706771 -1.039575 0.271860 2000-01-05 -0.424972 0.567020 0.276232 -1.087401 2000-01-06 -0.673690 0.113648 -1.478427 0.524988 2000-01-07 0.404705 0.577046 -1.715002 -1.039268 2000-01-08 -0.370647 -1.157892 -1.344312 0.844885 In [11]: df [[ 'B' , 'A' ]] = df [[ 'A' , 'B' ]] In [12]: df Out[12]: A B C D 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 2000-01-04 -0.706771 0.721555 -1.039575 0.271860 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 2000-01-08 -1.157892 -0.370647 -1.344312 0.844885 You may find this useful for applying a transform (in-place) to a subset of the columns. Warning pandas aligns all AXES when setting Series and DataFrame from .loc . This will not modify df because the column alignment is before value assignment. In [13]: df [[ 'A' , 'B' ]] Out[13]: A B 2000-01-01 -0.282863 0.469112 2000-01-02 -0.173215 1.212112 2000-01-03 -2.104569 -0.861849 2000-01-04 -0.706771 0.721555 2000-01-05 0.567020 -0.424972 2000-01-06 0.113648 -0.673690 2000-01-07 0.577046 0.404705 2000-01-08 -1.157892 -0.370647 In [14]: df . loc [:, [ 'B' , 'A' ]] = df [[ 'A' , 'B' ]] In [15]: df [[ 'A' , 'B' ]] Out[15]: A B 2000-01-01 -0.282863 0.469112 2000-01-02 -0.173215 1.212112 2000-01-03 -2.104569 -0.861849 2000-01-04 -0.706771 0.721555 2000-01-05 0.567020 -0.424972 2000-01-06 0.113648 -0.673690 2000-01-07 0.577046 0.404705 2000-01-08 -1.157892 -0.370647 The correct way to swap column values is by using raw values: In [16]: df . loc [:, [ 'B' , 'A' ]] = df [[ 'A' , 'B' ]] . to_numpy () In [17]: df [[ 'A' , 'B' ]] Out[17]: A B 2000-01-01 0.469112 -0.282863 2000-01-02 1.212112 -0.173215 2000-01-03 -0.861849 -2.104569 2000-01-04 0.721555 -0.706771 2000-01-05 -0.424972 0.567020 2000-01-06 -0.673690 0.113648 2000-01-07 0.404705 0.577046 2000-01-08 -0.370647 -1.157892 However, pandas does not align AXES when setting Series and DataFrame from .iloc because .iloc operates by position. This will modify df because the column alignment is not done before value assignment. In [18]: df [[ 'A' , 'B' ]] Out[18]: A B 2000-01-01 0.469112 -0.282863 2000-01-02 1.212112 -0.173215 2000-01-03 -0.861849 -2.104569 2000-01-04 0.721555 -0.706771 2000-01-05 -0.424972 0.567020 2000-01-06 -0.673690 0.113648 2000-01-07 0.404705 0.577046 2000-01-08 -0.370647 -1.157892 In [19]: df . iloc [:, [ 1 , 0 ]] = df [[ 'A' , 'B' ]] In [20]: df [[ 'A' , 'B' ]] Out[20]: A B 2000-01-01 -0.282863 0.469112 2000-01-02 -0.173215 1.212112 2000-01-03 -2.104569 -0.861849 2000-01-04 -0.706771 0.721555 2000-01-05 0.567020 -0.424972 2000-01-06 0.113648 -0.673690 2000-01-07 0.577046 0.404705 2000-01-08 -1.157892 -0.370647 Attribute access # You may access an index on a Series or column on a DataFrame directly as an attribute: In [21]: sa = pd . Series ([ 1 , 2 , 3 ], index = list ( 'abc' )) In [22]: dfa = df . copy () In [23]: sa . b Out[23]: 2 In [24]: dfa . A Out[24]: 2000-01-01 -0.282863 2000-01-02 -0.173215 2000-01-03 -2.104569 2000-01-04 -0.706771 2000-01-05 0.567020 2000-01-06 0.113648 2000-01-07 0.577046 2000-01-08 -1.157892 Freq: D, Name: A, dtype: float64 In [25]: sa . a = 5 In [26]: sa Out[26]: a 5 b 2 c 3 dtype: int64 In [27]: dfa . A = list ( range ( len ( dfa . index ))) # ok if A already exists In [28]: dfa Out[28]: A B C D 2000-01-01 0 0.469112 -1.509059 -1.135632 2000-01-02 1 1.212112 0.119209 -1.044236 2000-01-03 2 -0.861849 -0.494929 1.071804 2000-01-04 3 0.721555 -1.039575 0.271860 2000-01-05 4 -0.424972 0.276232 -1.087401 2000-01-06 5 -0.673690 -1.478427 0.524988 2000-01-07 6 0.404705 -1.715002 -1.039268 2000-01-08 7 -0.370647 -1.344312 0.844885 In [29]: dfa [ 'A' ] = list ( range ( len ( dfa . index ))) # use this form to create a new column In [30]: dfa Out[30]: A B C D 2000-01-01 0 0.469112 -1.509059 -1.135632 2000-01-02 1 1.212112 0.119209 -1.044236 2000-01-03 2 -0.861849 -0.494929 1.071804 2000-01-04 3 0.721555 -1.039575 0.271860 2000-01-05 4 -0.424972 0.276232 -1.087401 2000-01-06 5 -0.673690 -1.478427 0.524988 2000-01-07 6 0.404705 -1.715002 -1.039268 2000-01-08 7 -0.370647 -1.344312 0.844885 Warning You can use this access only if the index element is a valid Python identifier, e.g. s.1 is not allowed. See here for an explanation of valid identifiers . The attribute will not be available if it conflicts with an existing method name, e.g. s.min is not allowed, but s['min'] is possible. Similarly, the attribute will not be available if it conflicts with any of the following list: index , major_axis , minor_axis , items . In any of these cases, standard indexing will still work, e.g. s['1'] , s['min'] , and s['index'] will access the corresponding element or column. If you are using the IPython environment, you may also use tab-completion to see these accessible attributes. You can also assign a dict to a row of a DataFrame : In [31]: x = pd . DataFrame ({ 'x' : [ 1 , 2 , 3 ], 'y' : [ 3 , 4 , 5 ]}) In [32]: x . iloc [ 1 ] = { 'x' : 9 , 'y' : 99 } In [33]: x Out[33]: x y 0 1 3 1 9 99 2 3 5 You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful; if you try to use attribute access to create a new column, it creates a new attribute rather than a new column and will this raise a UserWarning : In [34]: df_new = pd . DataFrame ({ 'one' : [ 1. , 2. , 3. ]}) In [35]: df_new . two = [ 4 , 5 , 6 ] In [36]: df_new Out[36]: one 0 1.0 1 2.0 2 3.0 Slicing ranges # The most robust and consistent way of slicing ranges along arbitrary axes is described in the Selection by Position section detailing the .iloc method. For now, we explain the semantics of slicing using the [] operator. With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels: In [37]: s [: 5 ] Out[37]: 2000-01-01 0.469112 2000-01-02 1.212112 2000-01-03 -0.861849 2000-01-04 0.721555 2000-01-05 -0.424972 Freq: D, Name: A, dtype: float64 In [38]: s [:: 2 ] Out[38]: 2000-01-01 0.469112 2000-01-03 -0.861849 2000-01-05 -0.424972 2000-01-07 0.404705 Freq: 2D, Name: A, dtype: float64 In [39]: s [:: - 1 ] Out[39]: 2000-01-08 -0.370647 2000-01-07 0.404705 2000-01-06 -0.673690 2000-01-05 -0.424972 2000-01-04 0.721555 2000-01-03 -0.861849 2000-01-02 1.212112 2000-01-01 0.469112 Freq: -1D, Name: A, dtype: float64 Note that setting works as well: In [40]: s2 = s . copy () In [41]: s2 [: 5 ] = 0 In [42]: s2 Out[42]: 2000-01-01 0.000000 2000-01-02 0.000000 2000-01-03 0.000000 2000-01-04 0.000000 2000-01-05 0.000000 2000-01-06 -0.673690 2000-01-07 0.404705 2000-01-08 -0.370647 Freq: D, Name: A, dtype: float64 With DataFrame, slicing inside of [] slices the rows . This is provided largely as a convenience since it is such a common operation. In [43]: df [: 3 ] Out[43]: A B C D 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 In [44]: df [:: - 1 ] Out[44]: A B C D 2000-01-08 -1.157892 -0.370647 -1.344312 0.844885 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 2000-01-04 -0.706771 0.721555 -1.039575 0.271860 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632 Selection by label # Warning Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy . Warning .loc is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a DatetimeIndex . These will raise a TypeError . In [45]: dfl = pd . DataFrame ( np . random . randn ( 5 , 4 ), ....: columns = list ( 'ABCD' ), ....: index = pd . date_range ( '20130101' , periods = 5 )) ....: In [46]: dfl Out[46]: A B C D 2013-01-01 1.075770 -0.109050 1.643563 -1.469388 2013-01-02 0.357021 -0.674600 -1.776904 -0.968914 2013-01-03 -1.294524 0.413738 0.276662 -0.472035 2013-01-04 -0.013960 -0.362543 -0.006154 -0.923061 2013-01-05 0.895717 0.805244 -1.206412 2.565646 In [47]: dfl . loc [ 2 : 3 ] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In [ 47 ], line 1 ----> 1 dfl . loc [ 2 : 3 ] File ~/work/pandas/pandas/pandas/core/indexing.py:1192, in _LocationIndexer.__getitem__ (self, key) 1190 maybe_callable = com . apply_if_callable ( key , self . obj ) 1191 maybe_callable = self . _check_deprecated_callable_usage ( key , maybe_callable ) -> 1192 return self . _getitem_axis ( maybe_callable , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1412, in _LocIndexer._getitem_axis (self, key, axis) 1410 if isinstance ( key , slice ): 1411 self . _validate_key ( key , axis ) -> 1412 return self . _get_slice_axis ( key , axis = axis ) 1413 elif com . is_bool_indexer ( key ): 1414 return self . _getbool_axis ( key , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1444, in _LocIndexer._get_slice_axis (self, slice_obj, axis) 1441 return obj . copy ( deep = False ) 1443 labels = obj . _get_axis ( axis ) -> 1444 indexer = labels . slice_indexer ( slice_obj . start , slice_obj . stop , slice_obj . step ) 1446 if isinstance ( indexer , slice ): 1447 return self . obj . _slice ( indexer , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:682, in DatetimeIndex.slice_indexer (self, start, end, step) 674 # GH#33146 if start and end are combinations of str and None and Index is not 675 # monotonic, we can not use Index.slice_indexer because it does not honor the 676 # actual elements, is only searching for start and end 677 if ( 678 check_str_or_none ( start ) 679 or check_str_or_none ( end ) 680 or self . is_monotonic_increasing 681 ): --> 682 return Index . slice_indexer ( self , start , end , step ) 684 mask = np . array ( True ) 685 in_index = True File ~/work/pandas/pandas/pandas/core/indexes/base.py:6708, in Index.slice_indexer (self, start, end, step) 6664 def slice_indexer ( 6665 self , 6666 start : Hashable | None = None , 6667 end : Hashable | None = None , 6668 step : int | None = None , 6669 ) -> slice : 6670 \"\"\" 6671 Compute the slice indexer for input labels and step. 6672 (...) 6706 slice(1, 3, None) 6707 \"\"\" -> 6708 start_slice , end_slice = self . slice_locs ( start , end , step = step ) 6710 # return a slice 6711 if not is_scalar ( start_slice ): File ~/work/pandas/pandas/pandas/core/indexes/base.py:6934, in Index.slice_locs (self, start, end, step) 6932 start_slice = None 6933 if start is not None : -> 6934 start_slice = self . get_slice_bound ( start , \"left\" ) 6935 if start_slice is None : 6936 start_slice = 0 File ~/work/pandas/pandas/pandas/core/indexes/base.py:6849, in Index.get_slice_bound (self, label, side) 6845 original_label = label 6847 # For datetime indices label may be a string that has to be converted 6848 # to datetime boundary according to its resolution. -> 6849 label = self . _maybe_cast_slice_bound ( label , side ) 6851 # we need to look up the label 6852 try : File ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:642, in DatetimeIndex._maybe_cast_slice_bound (self, label, side) 637 if isinstance ( label , dt . date ) and not isinstance ( label , dt . datetime ): 638 # Pandas supports slicing with dates, treated as datetimes at midnight. 639 # https://github.com/pandas-dev/pandas/issues/31501 640 label = Timestamp ( label ) . to_pydatetime () --> 642 label = super () . _maybe_cast_slice_bound ( label , side ) 643 self . _data . _assert_tzawareness_compat ( label ) 644 return Timestamp ( label ) File ~/work/pandas/pandas/pandas/core/indexes/datetimelike.py:378, in DatetimeIndexOpsMixin._maybe_cast_slice_bound (self, label, side) 376 return lower if side == \"left\" else upper 377 elif not isinstance ( label , self . _data . _recognized_scalars ): --> 378 self . _raise_invalid_indexer ( \"slice\" , label ) 380 return label File ~/work/pandas/pandas/pandas/core/indexes/base.py:4308, in Index._raise_invalid_indexer (self, form, key, reraise) 4306 if reraise is not lib . no_default : 4307 raise TypeError ( msg ) from reraise -> 4308 raise TypeError ( msg ) TypeError : cannot do slice indexing on DatetimeIndex with these indexers [2] of type int String likes in slicing can be convertible to the type of the index and lead to natural slicing. In [48]: dfl . loc [ '20130102' : '20130104' ] Out[48]: A B C D 2013-01-02 0.357021 -0.674600 -1.776904 -0.968914 2013-01-03 -1.294524 0.413738 0.276662 -0.472035 2013-01-04 -0.013960 -0.362543 -0.006154 -0.923061 pandas provides a suite of methods in order to have purely label based indexing . This is a strict inclusion based protocol. Every label asked for must be in the index, or a KeyError will be raised. When slicing, both the start bound AND the stop bound are included , if present in the index. Integers are valid labels, but they refer to the label and not the position . The .loc attribute is the primary access method. The following are valid inputs: A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.). A list or array of labels ['a', 'b', 'c'] . A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels . A boolean array. A callable , see Selection By Callable . In [49]: s1 = pd . Series ( np . random . randn ( 6 ), index = list ( 'abcdef' )) In [50]: s1 Out[50]: a 1.431256 b 1.340309 c -1.170299 d -0.226169 e 0.410835 f 0.813850 dtype: float64 In [51]: s1 . loc [ 'c' :] Out[51]: c -1.170299 d -0.226169 e 0.410835 f 0.813850 dtype: float64 In [52]: s1 . loc [ 'b' ] Out[52]: 1.3403088497993827 Note that setting works as well: In [53]: s1 . loc [ 'c' :] = 0 In [54]: s1 Out[54]: a 1.431256 b 1.340309 c 0.000000 d 0.000000 e 0.000000 f 0.000000 dtype: float64 With a DataFrame: In [55]: df1 = pd . DataFrame ( np . random . randn ( 6 , 4 ), ....: index = list ( 'abcdef' ), ....: columns = list ( 'ABCD' )) ....: In [56]: df1 Out[56]: A B C D a 0.132003 -0.827317 -0.076467 -1.187678 b 1.130127 -1.436737 -1.413681 1.607920 c 1.024180 0.569605 0.875906 -2.211372 d 0.974466 -2.006747 -0.410001 -0.078638 e 0.545952 -1.219217 -1.226825 0.769804 f -1.281247 -0.727707 -0.121306 -0.097883 In [57]: df1 . loc [[ 'a' , 'b' , 'd' ], :] Out[57]: A B C D a 0.132003 -0.827317 -0.076467 -1.187678 b 1.130127 -1.436737 -1.413681 1.607920 d 0.974466 -2.006747 -0.410001 -0.078638 Accessing via label slices: In [58]: df1 . loc [ 'd' :, 'A' : 'C' ] Out[58]: A B C d 0.974466 -2.006747 -0.410001 e 0.545952 -1.219217 -1.226825 f -1.281247 -0.727707 -0.121306 For getting a cross section using a label (equivalent to df.xs('a') ): In [59]: df1 . loc [ 'a' ] Out[59]: A 0.132003 B -0.827317 C -0.076467 D -1.187678 Name: a, dtype: float64 For getting values with a boolean array: In [60]: df1 . loc [ 'a' ] > 0 Out[60]: A True B False C False D False Name: a, dtype: bool In [61]: df1 . loc [:, df1 . loc [ 'a' ] > 0 ] Out[61]: A a 0.132003 b 1.130127 c 1.024180 d 0.974466 e 0.545952 f -1.281247 NA values in a boolean array propagate as False : In [62]: mask = pd . array ([ True , False , True , False , pd . NA , False ], dtype = \"boolean\" ) In [63]: mask Out[63]: <BooleanArray> [True, False, True, False, <NA>, False] Length: 6, dtype: boolean In [64]: df1 [ mask ] Out[64]: A B C D a 0.132003 -0.827317 -0.076467 -1.187678 c 1.024180 0.569605 0.875906 -2.211372 For getting a value explicitly: # this is also equivalent to ``df1.at['a','A']`` In [65]: df1 . loc [ 'a' , 'A' ] Out[65]: 0.13200317033032932 Slicing with labels # When using .loc with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned: In [66]: s = pd . Series ( list ( 'abcde' ), index = [ 0 , 3 , 2 , 5 , 4 ]) In [67]: s . loc [ 3 : 5 ] Out[67]: 3 b 2 c 5 d dtype: object If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two: In [68]: s . sort_index () Out[68]: 0 a 2 c 3 b 4 e 5 d dtype: object In [69]: s . sort_index () . loc [ 1 : 6 ] Out[69]: 2 c 3 b 4 e 5 d dtype: object However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, s.loc[1:6] would raise KeyError . For the rationale behind this behavior, see Endpoints are inclusive . In [70]: s = pd . Series ( list ( 'abcdef' ), index = [ 0 , 3 , 2 , 5 , 4 , 2 ]) In [71]: s . loc [ 3 : 5 ] Out[71]: 3 b 2 c 5 d dtype: object Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, s.loc[2:5] would raise a KeyError . For more information about duplicate labels, see Duplicate Labels . Selection by position # Warning Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy . pandas provides a suite of methods in order to get purely integer based indexing . The semantics follow closely Python and NumPy slicing. These are 0-based indexing. When slicing, the start bound is included , while the upper bound is excluded . Trying to use a non-integer, even a valid label will raise an IndexError . The .iloc attribute is the primary access method. The following are valid inputs: An integer e.g. 5 . A list or array of integers [4, 3, 0] . A slice object with ints 1:7 . A boolean array. A callable , see Selection By Callable . A tuple of row (and column) indexes, whose elements are one of the above types. In [72]: s1 = pd . Series ( np . random . randn ( 5 ), index = list ( range ( 0 , 10 , 2 ))) In [73]: s1 Out[73]: 0 0.695775 2 0.341734 4 0.959726 6 -1.110336 8 -0.619976 dtype: float64 In [74]: s1 . iloc [: 3 ] Out[74]: 0 0.695775 2 0.341734 4 0.959726 dtype: float64 In [75]: s1 . iloc [ 3 ] Out[75]: -1.110336102891167 Note that setting works as well: In [76]: s1 . iloc [: 3 ] = 0 In [77]: s1 Out[77]: 0 0.000000 2 0.000000 4 0.000000 6 -1.110336 8 -0.619976 dtype: float64 With a DataFrame: In [78]: df1 = pd . DataFrame ( np . random . randn ( 6 , 4 ), ....: index = list ( range ( 0 , 12 , 2 )), ....: columns = list ( range ( 0 , 8 , 2 ))) ....: In [79]: df1 Out[79]: 0 2 4 6 0 0.149748 -0.732339 0.687738 0.176444 2 0.403310 -0.154951 0.301624 -2.179861 4 -1.369849 -0.954208 1.462696 -1.743161 6 -0.826591 -0.345352 1.314232 0.690579 8 0.995761 2.396780 0.014871 3.357427 10 -0.317441 -1.236269 0.896171 -0.487602 Select via integer slicing: In [80]: df1 . iloc [: 3 ] Out[80]: 0 2 4 6 0 0.149748 -0.732339 0.687738 0.176444 2 0.403310 -0.154951 0.301624 -2.179861 4 -1.369849 -0.954208 1.462696 -1.743161 In [81]: df1 . iloc [ 1 : 5 , 2 : 4 ] Out[81]: 4 6 2 0.301624 -2.179861 4 1.462696 -1.743161 6 1.314232 0.690579 8 0.014871 3.357427 Select via integer list: In [82]: df1 . iloc [[ 1 , 3 , 5 ], [ 1 , 3 ]] Out[82]: 2 6 2 -0.154951 -2.179861 6 -0.345352 0.690579 10 -1.236269 -0.487602 In [83]: df1 . iloc [ 1 : 3 , :] Out[83]: 0 2 4 6 2 0.403310 -0.154951 0.301624 -2.179861 4 -1.369849 -0.954208 1.462696 -1.743161 In [84]: df1 . iloc [:, 1 : 3 ] Out[84]: 2 4 0 -0.732339 0.687738 2 -0.154951 0.301624 4 -0.954208 1.462696 6 -0.345352 1.314232 8 2.396780 0.014871 10 -1.236269 0.896171 # this is also equivalent to ``df1.iat[1,1]`` In [85]: df1 . iloc [ 1 , 1 ] Out[85]: -0.1549507744249032 For getting a cross section using an integer position (equiv to df.xs(1) ): In [86]: df1 . iloc [ 1 ] Out[86]: 0 0.403310 2 -0.154951 4 0.301624 6 -2.179861 Name: 2, dtype: float64 Out of range slice indexes are handled gracefully just as in Python/NumPy. # these are allowed in Python/NumPy. In [87]: x = list ( 'abcdef' ) In [88]: x Out[88]: ['a', 'b', 'c', 'd', 'e', 'f'] In [89]: x [ 4 : 10 ] Out[89]: ['e', 'f'] In [90]: x [ 8 : 10 ] Out[90]: [] In [91]: s = pd . Series ( x ) In [92]: s Out[92]: 0 a 1 b 2 c 3 d 4 e 5 f dtype: object In [93]: s . iloc [ 4 : 10 ] Out[93]: 4 e 5 f dtype: object In [94]: s . iloc [ 8 : 10 ] Out[94]: Series([], dtype: object) Note that using slices that go out of bounds can result in an empty axis (e.g. an empty DataFrame being returned). In [95]: dfl = pd . DataFrame ( np . random . randn ( 5 , 2 ), columns = list ( 'AB' )) In [96]: dfl Out[96]: A B 0 -0.082240 -2.182937 1 0.380396 0.084844 2 0.432390 1.519970 3 -0.493662 0.600178 4 0.274230 0.132885 In [97]: dfl . iloc [:, 2 : 3 ] Out[97]: Empty DataFrame Columns: [] Index: [0, 1, 2, 3, 4] In [98]: dfl . iloc [:, 1 : 3 ] Out[98]: B 0 -2.182937 1 0.084844 2 1.519970 3 0.600178 4 0.132885 In [99]: dfl . iloc [ 4 : 6 ] Out[99]: A B 4 0.27423 0.132885 A single indexer that is out of bounds will raise an IndexError . A list of indexers where any element is out of bounds will raise an IndexError . In [100]: dfl . iloc [[ 4 , 5 , 6 ]] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexing.py:1715, in _iLocIndexer._get_list_axis (self, key, axis) 1714 try : -> 1715 return self . obj . _take_with_is_copy ( key , axis = axis ) 1716 except IndexError as err : 1717 # re-raise with different error message, e.g. test_getitem_ndarray_3d File ~/work/pandas/pandas/pandas/core/generic.py:4175, in NDFrame._take_with_is_copy (self, indices, axis) 4166 \"\"\" 4167 Internal version of the `take` method that sets the `_is_copy` 4168 attribute to keep track of the parent dataframe (using in indexing (...) 4173 See the docstring of `take` for full explanation of the parameters. 4174 \"\"\" -> 4175 result = self . take ( indices = indices , axis = axis ) 4176 # Maybe set copy if we didn't actually change the index. File ~/work/pandas/pandas/pandas/core/generic.py:4155, in NDFrame.take (self, indices, axis, **kwargs) 4151 indices = np . arange ( 4152 indices . start , indices . stop , indices . step , dtype = np . intp 4153 ) -> 4155 new_data = self . _mgr . take ( 4156 indices , 4157 axis = self . _get_block_manager_axis ( axis ), 4158 verify = True , 4159 ) 4160 return self . _constructor_from_mgr ( new_data , axes = new_data . axes ) . __finalize__ ( 4161 self , method = \"take\" 4162 ) File ~/work/pandas/pandas/pandas/core/internals/managers.py:910, in BaseBlockManager.take (self, indexer, axis, verify) 909 n = self . shape [ axis ] --> 910 indexer = maybe_convert_indices ( indexer , n , verify = verify ) 912 new_labels = self . axes [ axis ] . take ( indexer ) File ~/work/pandas/pandas/pandas/core/indexers/utils.py:282, in maybe_convert_indices (indices, n, verify) 281 if mask . any (): --> 282 raise IndexError ( \"indices are out-of-bounds\" ) 283 return indices IndexError : indices are out-of-bounds The above exception was the direct cause of the following exception : IndexError Traceback (most recent call last) Cell In [ 100 ], line 1 ----> 1 dfl . iloc [[ 4 , 5 , 6 ]] File ~/work/pandas/pandas/pandas/core/indexing.py:1192, in _LocationIndexer.__getitem__ (self, key) 1190 maybe_callable = com . apply_if_callable ( key , self . obj ) 1191 maybe_callable = self . _check_deprecated_callable_usage ( key , maybe_callable ) -> 1192 return self . _getitem_axis ( maybe_callable , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1744, in _iLocIndexer._getitem_axis (self, key, axis) 1742 # a list of integers 1743 elif is_list_like_indexer ( key ): -> 1744 return self . _get_list_axis ( key , axis = axis ) 1746 # a single integer 1747 else : 1748 key = item_from_zerodim ( key ) File ~/work/pandas/pandas/pandas/core/indexing.py:1718, in _iLocIndexer._get_list_axis (self, key, axis) 1715 return self . obj . _take_with_is_copy ( key , axis = axis ) 1716 except IndexError as err : 1717 # re-raise with different error message, e.g. test_getitem_ndarray_3d -> 1718 raise IndexError ( \"positional indexers are out-of-bounds\" ) from err IndexError : positional indexers are out-of-bounds In [101]: dfl . iloc [:, 4 ] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) Cell In [ 101 ], line 1 ----> 1 dfl . iloc [:, 4 ] File ~/work/pandas/pandas/pandas/core/indexing.py:1185, in _LocationIndexer.__getitem__ (self, key) 1183 if self . _is_scalar_access ( key ): 1184 return self . obj . _get_value ( * key , takeable = self . _takeable ) -> 1185 return self . _getitem_tuple ( key ) 1186 else : 1187 # we by definition only have the 0th axis 1188 axis = self . axis or 0 File ~/work/pandas/pandas/pandas/core/indexing.py:1691, in _iLocIndexer._getitem_tuple (self, tup) 1690 def _getitem_tuple ( self , tup : tuple ): -> 1691 tup = self . _validate_tuple_indexer ( tup ) 1692 with suppress ( IndexingError ): 1693 return self . _getitem_lowerdim ( tup ) File ~/work/pandas/pandas/pandas/core/indexing.py:967, in _LocationIndexer._validate_tuple_indexer (self, key) 965 for i , k in enumerate ( key ): 966 try : --> 967 self . _validate_key ( k , i ) 968 except ValueError as err : 969 raise ValueError ( 970 \"Location based indexing can only have \" 971 f \"[ { self . _valid_types } ] types\" 972 ) from err File ~/work/pandas/pandas/pandas/core/indexing.py:1593, in _iLocIndexer._validate_key (self, key, axis) 1591 return 1592 elif is_integer ( key ): -> 1593 self . _validate_integer ( key , axis ) 1594 elif isinstance ( key , tuple ): 1595 # a tuple should already have been caught by this point 1596 # so don't treat a tuple as a valid indexer 1597 raise IndexingError ( \"Too many indexers\" ) File ~/work/pandas/pandas/pandas/core/indexing.py:1686, in _iLocIndexer._validate_integer (self, key, axis) 1684 len_axis = len ( self . obj . _get_axis ( axis )) 1685 if key >= len_axis or key < - len_axis : -> 1686 raise IndexError ( \"single positional indexer is out-of-bounds\" ) IndexError : single positional indexer is out-of-bounds Selection by callable # .loc , .iloc , and also [] indexing can accept a callable as indexer. The callable must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing. Note For .iloc indexing, returning a tuple from the callable is not supported, since tuple destructuring for row and column indexes occurs before applying callables. In [102]: df1 = pd . DataFrame ( np . random . randn ( 6 , 4 ), .....: index = list ( 'abcdef' ), .....: columns = list ( 'ABCD' )) .....: In [103]: df1 Out[103]: A B C D a -0.023688 2.410179 1.450520 0.206053 b -0.251905 -2.213588 1.063327 1.266143 c 0.299368 -0.863838 0.408204 -1.048089 d -0.025747 -0.988387 0.094055 1.262731 e 1.289997 0.082423 -0.055758 0.536580 f -0.489682 0.369374 -0.034571 -2.484478 In [104]: df1 . loc [ lambda df : df [ 'A' ] > 0 , :] Out[104]: A B C D c 0.299368 -0.863838 0.408204 -1.048089 e 1.289997 0.082423 -0.055758 0.536580 In [105]: df1 . loc [:, lambda df : [ 'A' , 'B' ]] Out[105]: A B a -0.023688 2.410179 b -0.251905 -2.213588 c 0.299368 -0.863838 d -0.025747 -0.988387 e 1.289997 0.082423 f -0.489682 0.369374 In [106]: df1 . iloc [:, lambda df : [ 0 , 1 ]] Out[106]: A B a -0.023688 2.410179 b -0.251905 -2.213588 c 0.299368 -0.863838 d -0.025747 -0.988387 e 1.289997 0.082423 f -0.489682 0.369374 In [107]: df1 [ lambda df : df . columns [ 0 ]] Out[107]: a -0.023688 b -0.251905 c 0.299368 d -0.025747 e 1.289997 f -0.489682 Name: A, dtype: float64 You can use callable indexing in Series . In [108]: df1 [ 'A' ] . loc [ lambda s : s > 0 ] Out[108]: c 0.299368 e 1.289997 Name: A, dtype: float64 Using these methods / indexers, you can chain data selection operations without using a temporary variable. In [109]: bb = pd . read_csv ( 'data/baseball.csv' , index_col = 'id' ) In [110]: ( bb . groupby ([ 'year' , 'team' ]) . sum ( numeric_only = True ) .....: . loc [ lambda df : df [ 'r' ] > 100 ]) .....: Out[110]: stint g ab r h X2b ... so ibb hbp sh sf gidp year team ... 2007 CIN 6 379 745 101 203 35 ... 127.0 14.0 1.0 1.0 15.0 18.0 DET 5 301 1062 162 283 54 ... 176.0 3.0 10.0 4.0 8.0 28.0 HOU 4 311 926 109 218 47 ... 212.0 3.0 9.0 16.0 6.0 17.0 LAN 11 413 1021 153 293 61 ... 141.0 8.0 9.0 3.0 8.0 29.0 NYN 13 622 1854 240 509 101 ... 310.0 24.0 23.0 18.0 15.0 48.0 SFN 5 482 1305 198 337 67 ... 188.0 51.0 8.0 16.0 6.0 41.0 TEX 2 198 729 115 200 40 ... 140.0 4.0 5.0 2.0 8.0 16.0 TOR 4 459 1408 187 378 96 ... 265.0 16.0 12.0 4.0 16.0 38.0 [8 rows x 18 columns] Combining positional and label-based indexing # If you wish to get the 0th and the 2nd elements from the index in the âAâ column, you can do: In [111]: dfd = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], .....: 'B' : [ 4 , 5 , 6 ]}, .....: index = list ( 'abc' )) .....: In [112]: dfd Out[112]: A B a 1 4 b 2 5 c 3 6 In [113]: dfd . loc [ dfd . index [[ 0 , 2 ]], 'A' ] Out[113]: a 1 c 3 Name: A, dtype: int64 This can also be expressed using .iloc , by explicitly getting locations on the indexers, and using positional indexing to select things. In [114]: dfd . iloc [[ 0 , 2 ], dfd . columns . get_loc ( 'A' )] Out[114]: a 1 c 3 Name: A, dtype: int64 For getting multiple indexers, using .get_indexer : In [115]: dfd . iloc [[ 0 , 2 ], dfd . columns . get_indexer ([ 'A' , 'B' ])] Out[115]: A B a 1 4 c 3 6 Reindexing # The idiomatic way to achieve selecting potentially not-found elements is via .reindex() . See also the section on reindexing . In [116]: s = pd . Series ([ 1 , 2 , 3 ]) In [117]: s . reindex ([ 1 , 2 , 3 ]) Out[117]: 1 2.0 2 3.0 3 NaN dtype: float64 Alternatively, if you want to select only valid keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection. In [118]: labels = [ 1 , 2 , 3 ] In [119]: s . loc [ s . index . intersection ( labels )] Out[119]: 1 2 2 3 dtype: int64 Having a duplicated index will raise for a .reindex() : In [120]: s = pd . Series ( np . arange ( 4 ), index = [ 'a' , 'a' , 'b' , 'c' ]) In [121]: labels = [ 'c' , 'd' ] In [122]: s . reindex ( labels ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [ 122 ], line 1 ----> 1 s . reindex ( labels ) File ~/work/pandas/pandas/pandas/core/series.py:5172, in Series.reindex (self, index, axis, method, copy, level, fill_value, limit, tolerance) 5155 @doc ( 5156 NDFrame . reindex , # type: ignore[has-type] 5157 klass = _shared_doc_kwargs [ \"klass\" ], ( ... ) 5170 tolerance = None , 5171 ) -> Series : -> 5172 return super () . reindex ( 5173 index = index , 5174 method = method , 5175 copy = copy , 5176 level = level , 5177 fill_value = fill_value , 5178 limit = limit , 5179 tolerance = tolerance , 5180 ) File ~/work/pandas/pandas/pandas/core/generic.py:5632, in NDFrame.reindex (self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance) 5629 return self . _reindex_multi ( axes , copy , fill_value ) 5631 # perform the reindex on the axes -> 5632 return self . _reindex_axes ( 5633 axes , level , limit , tolerance , method , fill_value , copy 5634 ) . __finalize__ ( self , method = \"reindex\" ) File ~/work/pandas/pandas/pandas/core/generic.py:5655, in NDFrame._reindex_axes (self, axes, level, limit, tolerance, method, fill_value, copy) 5652 continue 5654 ax = self . _get_axis ( a ) -> 5655 new_index , indexer = ax . reindex ( 5656 labels , level = level , limit = limit , tolerance = tolerance , method = method 5657 ) 5659 axis = self . _get_axis_number ( a ) 5660 obj = obj . _reindex_with_indexers ( 5661 { axis : [ new_index , indexer ]}, 5662 fill_value = fill_value , 5663 copy = copy , 5664 allow_dups = False , 5665 ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:4436, in Index.reindex (self, target, method, level, limit, tolerance) 4433 raise ValueError ( \"cannot handle a non-unique multi-index!\" ) 4434 elif not self . is_unique : 4435 # GH#42568 -> 4436 raise ValueError ( \"cannot reindex on an axis with duplicate labels\" ) 4437 else : 4438 indexer , _ = self . get_indexer_non_unique ( target ) ValueError : cannot reindex on an axis with duplicate labels Generally, you can intersect the desired labels with the current axis, and then reindex. In [123]: s . loc [ s . index . intersection ( labels )] . reindex ( labels ) Out[123]: c 3.0 d NaN dtype: float64 However, this would still raise if your resulting index is duplicated. In [124]: labels = [ 'a' , 'd' ] In [125]: s . loc [ s . index . intersection ( labels )] . reindex ( labels ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [ 125 ], line 1 ----> 1 s . loc [ s . index . intersection ( labels )] . reindex ( labels ) File ~/work/pandas/pandas/pandas/core/series.py:5172, in Series.reindex (self, index, axis, method, copy, level, fill_value, limit, tolerance) 5155 @doc ( 5156 NDFrame . reindex , # type: ignore[has-type] 5157 klass = _shared_doc_kwargs [ \"klass\" ], ( ... ) 5170 tolerance = None , 5171 ) -> Series : -> 5172 return super () . reindex ( 5173 index = index , 5174 method = method , 5175 copy = copy , 5176 level = level , 5177 fill_value = fill_value , 5178 limit = limit , 5179 tolerance = tolerance , 5180 ) File ~/work/pandas/pandas/pandas/core/generic.py:5632, in NDFrame.reindex (self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance) 5629 return self . _reindex_multi ( axes , copy , fill_value ) 5631 # perform the reindex on the axes -> 5632 return self . _reindex_axes ( 5633 axes , level , limit , tolerance , method , fill_value , copy 5634 ) . __finalize__ ( self , method = \"reindex\" ) File ~/work/pandas/pandas/pandas/core/generic.py:5655, in NDFrame._reindex_axes (self, axes, level, limit, tolerance, method, fill_value, copy) 5652 continue 5654 ax = self . _get_axis ( a ) -> 5655 new_index , indexer = ax . reindex ( 5656 labels , level = level , limit = limit , tolerance = tolerance , method = method 5657 ) 5659 axis = self . _get_axis_number ( a ) 5660 obj = obj . _reindex_with_indexers ( 5661 { axis : [ new_index , indexer ]}, 5662 fill_value = fill_value , 5663 copy = copy , 5664 allow_dups = False , 5665 ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:4436, in Index.reindex (self, target, method, level, limit, tolerance) 4433 raise ValueError ( \"cannot handle a non-unique multi-index!\" ) 4434 elif not self . is_unique : 4435 # GH#42568 -> 4436 raise ValueError ( \"cannot reindex on an axis with duplicate labels\" ) 4437 else : 4438 indexer , _ = self . get_indexer_non_unique ( target ) ValueError : cannot reindex on an axis with duplicate labels Selecting random samples # A random selection of rows or columns from a Series or DataFrame with the sample() method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows. In [126]: s = pd . Series ([ 0 , 1 , 2 , 3 , 4 , 5 ]) # When no arguments are passed, returns 1 row. In [127]: s . sample () Out[127]: 4 4 dtype: int64 # One may specify either a number of rows: In [128]: s . sample ( n = 3 ) Out[128]: 0 0 4 4 1 1 dtype: int64 # Or a fraction of the rows: In [129]: s . sample ( frac = 0.5 ) Out[129]: 5 5 3 3 1 1 dtype: int64 By default, sample will return each row at most once, but one can also sample with replacement using the replace option: In [130]: s = pd . Series ([ 0 , 1 , 2 , 3 , 4 , 5 ]) # Without replacement (default): In [131]: s . sample ( n = 6 , replace = False ) Out[131]: 0 0 1 1 5 5 3 3 2 2 4 4 dtype: int64 # With replacement: In [132]: s . sample ( n = 6 , replace = True ) Out[132]: 0 0 4 4 3 3 2 2 4 4 4 4 dtype: int64 By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the sample function sampling weights as weights . These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example: In [133]: s = pd . Series ([ 0 , 1 , 2 , 3 , 4 , 5 ]) In [134]: example_weights = [ 0 , 0 , 0.2 , 0.2 , 0.2 , 0.4 ] In [135]: s . sample ( n = 3 , weights = example_weights ) Out[135]: 5 5 4 4 3 3 dtype: int64 # Weights will be re-normalized automatically In [136]: example_weights2 = [ 0.5 , 0 , 0 , 0 , 0 , 0 ] In [137]: s . sample ( n = 1 , weights = example_weights2 ) Out[137]: 0 0 dtype: int64 When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string. In [138]: df2 = pd . DataFrame ({ 'col1' : [ 9 , 8 , 7 , 6 ], .....: 'weight_column' : [ 0.5 , 0.4 , 0.1 , 0 ]}) .....: In [139]: df2 . sample ( n = 3 , weights = 'weight_column' ) Out[139]: col1 weight_column 1 8 0.4 0 9 0.5 2 7 0.1 sample also allows users to sample columns instead of rows using the axis argument. In [140]: df3 = pd . DataFrame ({ 'col1' : [ 1 , 2 , 3 ], 'col2' : [ 2 , 3 , 4 ]}) In [141]: df3 . sample ( n = 1 , axis = 1 ) Out[141]: col1 0 1 1 2 2 3 Finally, one can also set a seed for sample âs random number generator using the random_state argument, which will accept either an integer (as a seed) or a NumPy RandomState object. In [142]: df4 = pd . DataFrame ({ 'col1' : [ 1 , 2 , 3 ], 'col2' : [ 2 , 3 , 4 ]}) # With a given seed, the sample will always draw the same rows. In [143]: df4 . sample ( n = 2 , random_state = 2 ) Out[143]: col1 col2 2 3 4 1 2 3 In [144]: df4 . sample ( n = 2 , random_state = 2 ) Out[144]: col1 col2 2 3 4 1 2 3 Setting with enlargement # The .loc/[] operations can perform enlargement when setting a non-existent key for that axis. In the Series case this is effectively an appending operation. In [145]: se = pd . Series ([ 1 , 2 , 3 ]) In [146]: se Out[146]: 0 1 1 2 2 3 dtype: int64 In [147]: se [ 5 ] = 5. In [148]: se Out[148]: 0 1.0 1 2.0 2 3.0 5 5.0 dtype: float64 A DataFrame can be enlarged on either axis via .loc . In [149]: dfi = pd . DataFrame ( np . arange ( 6 ) . reshape ( 3 , 2 ), .....: columns = [ 'A' , 'B' ]) .....: In [150]: dfi Out[150]: A B 0 0 1 1 2 3 2 4 5 In [151]: dfi . loc [:, 'C' ] = dfi . loc [:, 'A' ] In [152]: dfi Out[152]: A B C 0 0 1 0 1 2 3 2 2 4 5 4 This is like an append operation on the DataFrame . In [153]: dfi . loc [ 3 ] = 5 In [154]: dfi Out[154]: A B C 0 0 1 0 1 2 3 2 2 4 5 4 3 5 5 5 Fast scalar value getting and setting # Since indexing with [] must handle a lot of cases (single-label access, slicing, boolean indexing, etc.), it has a bit of overhead in order to figure out what youâre asking for. If you only want to access a scalar value, the fastest way is to use the at and iat methods, which are implemented on all of the data structures. Similarly to loc , at provides label based scalar lookups, while, iat provides integer based lookups analogously to iloc In [155]: s . iat [ 5 ] Out[155]: 5 In [156]: df . at [ dates [ 5 ], 'A' ] Out[156]: 0.1136484096888855 In [157]: df . iat [ 3 , 0 ] Out[157]: -0.7067711336300845 You can also set using these same indexers. In [158]: df . at [ dates [ 5 ], 'E' ] = 7 In [159]: df . iat [ 3 , 0 ] = 7 at may enlarge the object in-place as above if the indexer is missing. In [160]: df . at [ dates [ - 1 ] + pd . Timedelta ( '1 day' ), 0 ] = 7 In [161]: df Out[161]: A B C D E 0 2000-01-01 -0.282863 0.469112 -1.509059 -1.135632 NaN NaN 2000-01-02 -0.173215 1.212112 0.119209 -1.044236 NaN NaN 2000-01-03 -2.104569 -0.861849 -0.494929 1.071804 NaN NaN 2000-01-04 7.000000 0.721555 -1.039575 0.271860 NaN NaN 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 NaN NaN 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 7.0 NaN 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 NaN NaN 2000-01-08 -1.157892 -0.370647 -1.344312 0.844885 NaN NaN 2000-01-09 NaN NaN NaN NaN NaN 7.0 Boolean indexing # Another common operation is the use of boolean vectors to filter the data. The operators are: | for or , & for and , and ~ for not . These must be grouped by using parentheses, since by default Python will evaluate an expression such as df['A'] > 2 & df['B'] < 3 as df['A'] > (2 & df['B']) < 3 , while the desired evaluation order is (df['A'] > 2) & (df['B'] < 3) . Using a boolean vector to index a Series works exactly as in a NumPy ndarray: In [162]: s = pd . Series ( range ( - 3 , 4 )) In [163]: s Out[163]: 0 -3 1 -2 2 -1 3 0 4 1 5 2 6 3 dtype: int64 In [164]: s [ s > 0 ] Out[164]: 4 1 5 2 6 3 dtype: int64 In [165]: s [( s < - 1 ) | ( s > 0.5 )] Out[165]: 0 -3 1 -2 4 1 5 2 6 3 dtype: int64 In [166]: s [ ~ ( s < 0 )] Out[166]: 3 0 4 1 5 2 6 3 dtype: int64 You may select rows from a DataFrame using a boolean vector the same length as the DataFrameâs index (for example, something derived from one of the columns of the DataFrame): In [167]: df [ df [ 'A' ] > 0 ] Out[167]: A B C D E 0 2000-01-04 7.000000 0.721555 -1.039575 0.271860 NaN NaN 2000-01-05 0.567020 -0.424972 0.276232 -1.087401 NaN NaN 2000-01-06 0.113648 -0.673690 -1.478427 0.524988 7.0 NaN 2000-01-07 0.577046 0.404705 -1.715002 -1.039268 NaN NaN List comprehensions and the map method of Series can also be used to produce more complex criteria: In [168]: df2 = pd . DataFrame ({ 'a' : [ 'one' , 'one' , 'two' , 'three' , 'two' , 'one' , 'six' ], .....: 'b' : [ 'x' , 'y' , 'y' , 'x' , 'y' , 'x' , 'x' ], .....: 'c' : np . random . randn ( 7 )}) .....: # only want 'two' or 'three' In [169]: criterion = df2 [ 'a' ] . map ( lambda x : x . startswith ( 't' )) In [170]: df2 [ criterion ] Out[170]: a b c 2 two y 0.041290 3 three x 0.361719 4 two y -0.238075 # equivalent but slower In [171]: df2 [[ x . startswith ( 't' ) for x in df2 [ 'a' ]]] Out[171]: a b c 2 two y 0.041290 3 three x 0.361719 4 two y -0.238075 # Multiple criteria In [172]: df2 [ criterion & ( df2 [ 'b' ] == 'x' )] Out[172]: a b c 3 three x 0.361719 With the choice methods Selection by Label , Selection by Position , and Advanced Indexing you may select along more than one axis using boolean vectors combined with other indexing expressions. In [173]: df2 . loc [ criterion & ( df2 [ 'b' ] == 'x' ), 'b' : 'c' ] Out[173]: b c 3 x 0.361719 Warning iloc supports two kinds of boolean indexing. If the indexer is a boolean Series , an error will be raised. For instance, in the following example, df.iloc[s.values, 1] is ok. The boolean indexer is an array. But df.iloc[s, 1] would raise ValueError . In [174]: df = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], .....: index = list ( 'abc' ), .....: columns = [ 'A' , 'B' ]) .....: In [175]: s = ( df [ 'A' ] > 2 ) In [176]: s Out[176]: a False b True c True Name: A, dtype: bool In [177]: df . loc [ s , 'B' ] Out[177]: b 4 c 6 Name: B, dtype: int64 In [178]: df . iloc [ s . values , 1 ] Out[178]: b 4 c 6 Name: B, dtype: int64 Indexing with isin # Consider the isin() method of Series , which returns a boolean vector that is true wherever the Series elements exist in the passed list. This allows you to select rows where one or more columns have values you want: In [179]: s = pd . Series ( np . arange ( 5 ), index = np . arange ( 5 )[:: - 1 ], dtype = 'int64' ) In [180]: s Out[180]: 4 0 3 1 2 2 1 3 0 4 dtype: int64 In [181]: s . isin ([ 2 , 4 , 6 ]) Out[181]: 4 False 3 False 2 True 1 False 0 True dtype: bool In [182]: s [ s . isin ([ 2 , 4 , 6 ])] Out[182]: 2 2 0 4 dtype: int64 The same method is available for Index objects and is useful for the cases when you donât know which of the sought labels are in fact present: In [183]: s [ s . index . isin ([ 2 , 4 , 6 ])] Out[183]: 4 0 2 2 dtype: int64 # compare it to the following In [184]: s . reindex ([ 2 , 4 , 6 ]) Out[184]: 2 2.0 4 0.0 6 NaN dtype: float64 In addition to that, MultiIndex allows selecting a separate level to use in the membership check: In [185]: s_mi = pd . Series ( np . arange ( 6 ), .....: index = pd . MultiIndex . from_product ([[ 0 , 1 ], [ 'a' , 'b' , 'c' ]])) .....: In [186]: s_mi Out[186]: 0 a 0 b 1 c 2 1 a 3 b 4 c 5 dtype: int64 In [187]: s_mi . iloc [ s_mi . index . isin ([( 1 , 'a' ), ( 2 , 'b' ), ( 0 , 'c' )])] Out[187]: 0 c 2 1 a 3 dtype: int64 In [188]: s_mi . iloc [ s_mi . index . isin ([ 'a' , 'c' , 'e' ], level = 1 )] Out[188]: 0 a 0 c 2 1 a 3 c 5 dtype: int64 DataFrame also has an isin() method. When calling isin , pass a set of values as either an array or dict. If values is an array, isin returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values. In [189]: df = pd . DataFrame ({ 'vals' : [ 1 , 2 , 3 , 4 ], 'ids' : [ 'a' , 'b' , 'f' , 'n' ], .....: 'ids2' : [ 'a' , 'n' , 'c' , 'n' ]}) .....: In [190]: values = [ 'a' , 'b' , 1 , 3 ] In [191]: df . isin ( values ) Out[191]: vals ids ids2 0 True True True 1 False True False 2 True False False 3 False False False Oftentimes youâll want to match certain values with certain columns. Just make values a dict where the key is the column, and the value is a list of items you want to check for. In [192]: values = { 'ids' : [ 'a' , 'b' ], 'vals' : [ 1 , 3 ]} In [193]: df . isin ( values ) Out[193]: vals ids ids2 0 True True False 1 False True False 2 True False False 3 False False False To return the DataFrame of booleans where the values are not in the original DataFrame, use the ~ operator: In [194]: values = { 'ids' : [ 'a' , 'b' ], 'vals' : [ 1 , 3 ]} In [195]: ~ df . isin ( values ) Out[195]: vals ids ids2 0 False False True 1 True False True 2 False True True 3 True True True Combine DataFrameâs isin with the any() and all() methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion: In [196]: values = { 'ids' : [ 'a' , 'b' ], 'ids2' : [ 'a' , 'c' ], 'vals' : [ 1 , 3 ]} In [197]: row_mask = df . isin ( values ) . all ( 1 ) In [198]: df [ row_mask ] Out[198]: vals ids ids2 0 1 a a The where() Method and Masking # Selecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the where method in Series and DataFrame . To return only the selected rows: In [199]: s [ s > 0 ] Out[199]: 3 1 2 2 1 3 0 4 dtype: int64 To return a Series of the same shape as the original: In [200]: s . where ( s > 0 ) Out[200]: 4 NaN 3 1.0 2 2.0 1 3.0 0 4.0 dtype: float64 Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. where is used under the hood as the implementation. The code below is equivalent to df.where(df < 0) . In [201]: dates = pd . date_range ( '1/1/2000' , periods = 8 ) In [202]: df = pd . DataFrame ( np . random . randn ( 8 , 4 ), .....: index = dates , columns = [ 'A' , 'B' , 'C' , 'D' ]) .....: In [203]: df [ df < 0 ] Out[203]: A B C D 2000-01-01 -2.104139 -1.309525 NaN NaN 2000-01-02 -0.352480 NaN -1.192319 NaN 2000-01-03 -0.864883 NaN -0.227870 NaN 2000-01-04 NaN -1.222082 NaN -1.233203 2000-01-05 NaN -0.605656 -1.169184 NaN 2000-01-06 NaN -0.948458 NaN -0.684718 2000-01-07 -2.670153 -0.114722 NaN -0.048048 2000-01-08 NaN NaN -0.048788 -0.808838 In addition, where takes an optional other argument for replacement of values where the condition is False, in the returned copy. In [204]: df . where ( df < 0 , - df ) Out[204]: A B C D 2000-01-01 -2.104139 -1.309525 -0.485855 -0.245166 2000-01-02 -0.352480 -0.390389 -1.192319 -1.655824 2000-01-03 -0.864883 -0.299674 -0.227870 -0.281059 2000-01-04 -0.846958 -1.222082 -0.600705 -1.233203 2000-01-05 -0.669692 -0.605656 -1.169184 -0.342416 2000-01-06 -0.868584 -0.948458 -2.297780 -0.684718 2000-01-07 -2.670153 -0.114722 -0.168904 -0.048048 2000-01-08 -0.801196 -1.392071 -0.048788 -0.808838 You may wish to set values based on some boolean criteria. This can be done intuitively like so: In [205]: s2 = s . copy () In [206]: s2 [ s2 < 0 ] = 0 In [207]: s2 Out[207]: 4 0 3 1 2 2 1 3 0 4 dtype: int64 In [208]: df2 = df . copy () In [209]: df2 [ df2 < 0 ] = 0 In [210]: df2 Out[210]: A B C D 2000-01-01 0.000000 0.000000 0.485855 0.245166 2000-01-02 0.000000 0.390389 0.000000 1.655824 2000-01-03 0.000000 0.299674 0.000000 0.281059 2000-01-04 0.846958 0.000000 0.600705 0.000000 2000-01-05 0.669692 0.000000 0.000000 0.342416 2000-01-06 0.868584 0.000000 2.297780 0.000000 2000-01-07 0.000000 0.000000 0.168904 0.000000 2000-01-08 0.801196 1.392071 0.000000 0.000000 where returns a modified copy of the data. Note The signature for DataFrame.where() differs from numpy.where() . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . In [211]: df . where ( df < 0 , - df ) == np . where ( df < 0 , df , - df ) Out[211]: A B C D 2000-01-01 True True True True 2000-01-02 True True True True 2000-01-03 True True True True 2000-01-04 True True True True 2000-01-05 True True True True 2000-01-06 True True True True 2000-01-07 True True True True 2000-01-08 True True True True Alignment Furthermore, where aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via .loc (but on the contents rather than the axis labels). In [212]: df2 = df . copy () In [213]: df2 [ df2 [ 1 : 4 ] > 0 ] = 3 In [214]: df2 Out[214]: A B C D 2000-01-01 -2.104139 -1.309525 0.485855 0.245166 2000-01-02 -0.352480 3.000000 -1.192319 3.000000 2000-01-03 -0.864883 3.000000 -0.227870 3.000000 2000-01-04 3.000000 -1.222082 3.000000 -1.233203 2000-01-05 0.669692 -0.605656 -1.169184 0.342416 2000-01-06 0.868584 -0.948458 2.297780 -0.684718 2000-01-07 -2.670153 -0.114722 0.168904 -0.048048 2000-01-08 0.801196 1.392071 -0.048788 -0.808838 Where can also accept axis and level parameters to align the input when performing the where . In [215]: df2 = df . copy () In [216]: df2 . where ( df2 > 0 , df2 [ 'A' ], axis = 'index' ) Out[216]: A B C D 2000-01-01 -2.104139 -2.104139 0.485855 0.245166 2000-01-02 -0.352480 0.390389 -0.352480 1.655824 2000-01-03 -0.864883 0.299674 -0.864883 0.281059 2000-01-04 0.846958 0.846958 0.600705 0.846958 2000-01-05 0.669692 0.669692 0.669692 0.342416 2000-01-06 0.868584 0.868584 2.297780 0.868584 2000-01-07 -2.670153 -2.670153 0.168904 -2.670153 2000-01-08 0.801196 1.392071 0.801196 0.801196 This is equivalent to (but faster than) the following. In [217]: df2 = df . copy () In [218]: df . apply ( lambda x , y : x . where ( x > 0 , y ), y = df [ 'A' ]) Out[218]: A B C D 2000-01-01 -2.104139 -2.104139 0.485855 0.245166 2000-01-02 -0.352480 0.390389 -0.352480 1.655824 2000-01-03 -0.864883 0.299674 -0.864883 0.281059 2000-01-04 0.846958 0.846958 0.600705 0.846958 2000-01-05 0.669692 0.669692 0.669692 0.342416 2000-01-06 0.868584 0.868584 2.297780 0.868584 2000-01-07 -2.670153 -2.670153 0.168904 -2.670153 2000-01-08 0.801196 1.392071 0.801196 0.801196 where can accept a callable as condition and other arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and other argument. In [219]: df3 = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], .....: 'B' : [ 4 , 5 , 6 ], .....: 'C' : [ 7 , 8 , 9 ]}) .....: In [220]: df3 . where ( lambda x : x > 4 , lambda x : x + 10 ) Out[220]: A B C 0 11 14 7 1 12 5 8 2 13 6 9 Mask # mask() is the inverse boolean operation of where . In [221]: s . mask ( s >= 0 ) Out[221]: 4 NaN 3 NaN 2 NaN 1 NaN 0 NaN dtype: float64 In [222]: df . mask ( df >= 0 ) Out[222]: A B C D 2000-01-01 -2.104139 -1.309525 NaN NaN 2000-01-02 -0.352480 NaN -1.192319 NaN 2000-01-03 -0.864883 NaN -0.227870 NaN 2000-01-04 NaN -1.222082 NaN -1.233203 2000-01-05 NaN -0.605656 -1.169184 NaN 2000-01-06 NaN -0.948458 NaN -0.684718 2000-01-07 -2.670153 -0.114722 NaN -0.048048 2000-01-08 NaN NaN -0.048788 -0.808838 Setting with enlargement conditionally using numpy() # An alternative to where() is to use numpy.where() . Combined with setting a new column, you can use it to enlarge a DataFrame where the values are determined conditionally. Consider you have two choices to choose from in the following DataFrame. And you want to set a new column color to âgreenâ when the second column has âZâ. You can do the following: In [223]: df = pd . DataFrame ({ 'col1' : list ( 'ABBC' ), 'col2' : list ( 'ZZXY' )}) In [224]: df [ 'color' ] = np . where ( df [ 'col2' ] == 'Z' , 'green' , 'red' ) In [225]: df Out[225]: col1 col2 color 0 A Z green 1 B Z green 2 B X red 3 C Y red If you have multiple conditions, you can use numpy.select() to achieve that. Say corresponding to three conditions there are three choice of colors, with a fourth color as a fallback, you can do the following. In [226]: conditions = [ .....: ( df [ 'col2' ] == 'Z' ) & ( df [ 'col1' ] == 'A' ), .....: ( df [ 'col2' ] == 'Z' ) & ( df [ 'col1' ] == 'B' ), .....: ( df [ 'col1' ] == 'B' ) .....: ] .....: In [227]: choices = [ 'yellow' , 'blue' , 'purple' ] In [228]: df [ 'color' ] = np . select ( conditions , choices , default = 'black' ) In [229]: df Out[229]: col1 col2 color 0 A Z yellow 1 B Z blue 2 B X purple 3 C Y black The query() Method # DataFrame objects have a query() method that allows selection using an expression. You can get the value of the frame where column b has values between the values of columns a and c . For example: In [230]: n = 10 In [231]: df = pd . DataFrame ( np . random . rand ( n , 3 ), columns = list ( 'abc' )) In [232]: df Out[232]: a b c 0 0.438921 0.118680 0.863670 1 0.138138 0.577363 0.686602 2 0.595307 0.564592 0.520630 3 0.913052 0.926075 0.616184 4 0.078718 0.854477 0.898725 5 0.076404 0.523211 0.591538 6 0.792342 0.216974 0.564056 7 0.397890 0.454131 0.915716 8 0.074315 0.437913 0.019794 9 0.559209 0.502065 0.026437 # pure python In [233]: df [( df [ 'a' ] < df [ 'b' ]) & ( df [ 'b' ] < df [ 'c' ])] Out[233]: a b c 1 0.138138 0.577363 0.686602 4 0.078718 0.854477 0.898725 5 0.076404 0.523211 0.591538 7 0.397890 0.454131 0.915716 # query In [234]: df . query ( '(a < b) & (b < c)' ) Out[234]: a b c 1 0.138138 0.577363 0.686602 4 0.078718 0.854477 0.898725 5 0.076404 0.523211 0.591538 7 0.397890 0.454131 0.915716 Do the same thing but fall back on a named index if there is no column with the name a . In [235]: df = pd . DataFrame ( np . random . randint ( n / 2 , size = ( n , 2 )), columns = list ( 'bc' )) In [236]: df . index . name = 'a' In [237]: df Out[237]: b c a 0 0 4 1 0 1 2 3 4 3 4 3 4 1 4 5 0 3 6 0 1 7 3 4 8 2 3 9 1 1 In [238]: df . query ( 'a < b and b < c' ) Out[238]: b c a 2 3 4 If instead you donât want to or cannot name your index, you can use the name index in your query expression: In [239]: df = pd . DataFrame ( np . random . randint ( n , size = ( n , 2 )), columns = list ( 'bc' )) In [240]: df Out[240]: b c 0 3 1 1 3 0 2 5 6 3 5 2 4 7 4 5 0 1 6 2 5 7 0 1 8 6 0 9 7 9 In [241]: df . query ( 'index < b < c' ) Out[241]: b c 2 5 6 Note If the name of your index overlaps with a column name, the column name is given precedence. For example, In [242]: df = pd . DataFrame ({ 'a' : np . random . randint ( 5 , size = 5 )}) In [243]: df . index . name = 'a' In [244]: df . query ( 'a > 2' ) # uses the column 'a', not the index Out[244]: a a 1 3 3 3 You can still use the index in a query expression by using the special identifier âindexâ: In [245]: df . query ( 'index > 2' ) Out[245]: a a 3 3 4 2 If for some reason you have a column named index , then you can refer to the index as ilevel_0 as well, but at this point you should consider renaming your columns to something less ambiguous. MultiIndex query() Syntax # You can also use the levels of a DataFrame with a MultiIndex as if they were columns in the frame: In [246]: n = 10 In [247]: colors = np . random . choice ([ 'red' , 'green' ], size = n ) In [248]: foods = np . random . choice ([ 'eggs' , 'ham' ], size = n ) In [249]: colors Out[249]: array(['red', 'red', 'red', 'green', 'green', 'green', 'green', 'green', 'green', 'green'], dtype='<U5') In [250]: foods Out[250]: array(['ham', 'ham', 'eggs', 'eggs', 'eggs', 'ham', 'ham', 'eggs', 'eggs', 'eggs'], dtype='<U4') In [251]: index = pd . MultiIndex . from_arrays ([ colors , foods ], names = [ 'color' , 'food' ]) In [252]: df = pd . DataFrame ( np . random . randn ( n , 2 ), index = index ) In [253]: df Out[253]: 0 1 color food red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 green eggs -0.748199 1.318931 eggs -2.029766 0.792652 ham 0.461007 -0.542749 ham -0.305384 -0.479195 eggs 0.095031 -0.270099 eggs -0.707140 -0.773882 eggs 0.229453 0.304418 In [254]: df . query ( 'color == \"red\"' ) Out[254]: 0 1 color food red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 If the levels of the MultiIndex are unnamed, you can refer to them using special names: In [255]: df . index . names = [ None , None ] In [256]: df Out[256]: 0 1 red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 green eggs -0.748199 1.318931 eggs -2.029766 0.792652 ham 0.461007 -0.542749 ham -0.305384 -0.479195 eggs 0.095031 -0.270099 eggs -0.707140 -0.773882 eggs 0.229453 0.304418 In [257]: df . query ( 'ilevel_0 == \"red\"' ) Out[257]: 0 1 red ham 0.194889 -0.381994 ham 0.318587 2.089075 eggs -0.728293 -0.090255 The convention is ilevel_0 , which means âindex level 0â for the 0th level of the index . query() Use Cases # A use case for query() is when you have a collection of DataFrame objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame youâre interested in querying In [258]: df = pd . DataFrame ( np . random . rand ( n , 3 ), columns = list ( 'abc' )) In [259]: df Out[259]: a b c 0 0.224283 0.736107 0.139168 1 0.302827 0.657803 0.713897 2 0.611185 0.136624 0.984960 3 0.195246 0.123436 0.627712 4 0.618673 0.371660 0.047902 5 0.480088 0.062993 0.185760 6 0.568018 0.483467 0.445289 7 0.309040 0.274580 0.587101 8 0.258993 0.477769 0.370255 9 0.550459 0.840870 0.304611 In [260]: df2 = pd . DataFrame ( np . random . rand ( n + 2 , 3 ), columns = df . columns ) In [261]: df2 Out[261]: a b c 0 0.357579 0.229800 0.596001 1 0.309059 0.957923 0.965663 2 0.123102 0.336914 0.318616 3 0.526506 0.323321 0.860813 4 0.518736 0.486514 0.384724 5 0.190804 0.505723 0.614533 6 0.891939 0.623977 0.676639 7 0.480559 0.378528 0.460858 8 0.420223 0.136404 0.141295 9 0.732206 0.419540 0.604675 10 0.604466 0.848974 0.896165 11 0.589168 0.920046 0.732716 In [262]: expr = '0.0 <= a <= c <= 0.5' In [263]: map ( lambda frame : frame . query ( expr ), [ df , df2 ]) Out[263]: <map at 0x7f841f1de740> query() Python versus pandas Syntax Comparison # Full numpy-like syntax: In [264]: df = pd . DataFrame ( np . random . randint ( n , size = ( n , 3 )), columns = list ( 'abc' )) In [265]: df Out[265]: a b c 0 7 8 9 1 1 0 7 2 2 7 2 3 6 2 2 4 2 6 3 5 3 8 2 6 1 7 2 7 5 1 5 8 9 8 0 9 1 5 0 In [266]: df . query ( '(a < b) & (b < c)' ) Out[266]: a b c 0 7 8 9 In [267]: df [( df [ 'a' ] < df [ 'b' ]) & ( df [ 'b' ] < df [ 'c' ])] Out[267]: a b c 0 7 8 9 Slightly nicer by removing the parentheses (comparison operators bind tighter than & and | ): In [268]: df . query ( 'a < b & b < c' ) Out[268]: a b c 0 7 8 9 Use English instead of symbols: In [269]: df . query ( 'a < b and b < c' ) Out[269]: a b c 0 7 8 9 Pretty close to how you might write it on paper: In [270]: df . query ( 'a < b < c' ) Out[270]: a b c 0 7 8 9 The in and not in operators # query() also supports special use of Pythonâs in and not in comparison operators, providing a succinct syntax for calling the isin method of a Series or DataFrame . # get all rows where columns \"a\" and \"b\" have overlapping values In [271]: df = pd . DataFrame ({ 'a' : list ( 'aabbccddeeff' ), 'b' : list ( 'aaaabbbbcccc' ), .....: 'c' : np . random . randint ( 5 , size = 12 ), .....: 'd' : np . random . randint ( 9 , size = 12 )}) .....: In [272]: df Out[272]: a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 In [273]: df . query ( 'a in b' ) Out[273]: a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 # How you'd do it in pure Python In [274]: df [ df [ 'a' ] . isin ( df [ 'b' ])] Out[274]: a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 In [275]: df . query ( 'a not in b' ) Out[275]: a b c d 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 # pure Python In [276]: df [ ~ df [ 'a' ] . isin ( df [ 'b' ])] Out[276]: a b c d 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 You can combine this with other expressions for very succinct queries: # rows where cols a and b have overlapping values # and col c's values are less than col d's In [277]: df . query ( 'a in b and c < d' ) Out[277]: a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 4 c b 3 6 5 c b 0 2 # pure Python In [278]: df [ df [ 'b' ] . isin ( df [ 'a' ]) & ( df [ 'c' ] < df [ 'd' ])] Out[278]: a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 4 c b 3 6 5 c b 0 2 10 f c 0 6 11 f c 1 2 Note Note that in and not in are evaluated in Python, since numexpr has no equivalent of this operation. However, only the in / not in expression itself is evaluated in vanilla Python. For example, in the expression df . query ( 'a in b + c + d' ) (b + c + d) is evaluated by numexpr and then the in operation is evaluated in plain Python. In general, any operations that can be evaluated using numexpr will be. Special use of the == operator with list objects # Comparing a list of values to a column using == / != works similarly to in / not in . In [279]: df . query ( 'b == [\"a\", \"b\", \"c\"]' ) Out[279]: a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 # pure Python In [280]: df [ df [ 'b' ] . isin ([ \"a\" , \"b\" , \"c\" ])] Out[280]: a b c d 0 a a 2 6 1 a a 4 7 2 b a 1 6 3 b a 2 1 4 c b 3 6 5 c b 0 2 6 d b 3 3 7 d b 2 1 8 e c 4 3 9 e c 2 0 10 f c 0 6 11 f c 1 2 In [281]: df . query ( 'c == [1, 2]' ) Out[281]: a b c d 0 a a 2 6 2 b a 1 6 3 b a 2 1 7 d b 2 1 9 e c 2 0 11 f c 1 2 In [282]: df . query ( 'c != [1, 2]' ) Out[282]: a b c d 1 a a 4 7 4 c b 3 6 5 c b 0 2 6 d b 3 3 8 e c 4 3 10 f c 0 6 # using in/not in In [283]: df . query ( '[1, 2] in c' ) Out[283]: a b c d 0 a a 2 6 2 b a 1 6 3 b a 2 1 7 d b 2 1 9 e c 2 0 11 f c 1 2 In [284]: df . query ( '[1, 2] not in c' ) Out[284]: a b c d 1 a a 4 7 4 c b 3 6 5 c b 0 2 6 d b 3 3 8 e c 4 3 10 f c 0 6 # pure Python In [285]: df [ df [ 'c' ] . isin ([ 1 , 2 ])] Out[285]: a b c d 0 a a 2 6 2 b a 1 6 3 b a 2 1 7 d b 2 1 9 e c 2 0 11 f c 1 2 Boolean operators # You can negate boolean expressions with the word not or the ~ operator. In [286]: df = pd . DataFrame ( np . random . rand ( n , 3 ), columns = list ( 'abc' )) In [287]: df [ 'bools' ] = np . random . rand ( len ( df )) > 0.5 In [288]: df . query ( '~bools' ) Out[288]: a b c bools 2 0.697753 0.212799 0.329209 False 7 0.275396 0.691034 0.826619 False 8 0.190649 0.558748 0.262467 False In [289]: df . query ( 'not bools' ) Out[289]: a b c bools 2 0.697753 0.212799 0.329209 False 7 0.275396 0.691034 0.826619 False 8 0.190649 0.558748 0.262467 False In [290]: df . query ( 'not bools' ) == df [ ~ df [ 'bools' ]] Out[290]: a b c bools 2 True True True True 7 True True True True 8 True True True True Of course, expressions can be arbitrarily complex too: # short query syntax In [291]: shorter = df . query ( 'a < b < c and (not bools) or bools > 2' ) # equivalent in pure Python In [292]: longer = df [( df [ 'a' ] < df [ 'b' ]) .....: & ( df [ 'b' ] < df [ 'c' ]) .....: & ( ~ df [ 'bools' ]) .....: | ( df [ 'bools' ] > 2 )] .....: In [293]: shorter Out[293]: a b c bools 7 0.275396 0.691034 0.826619 False In [294]: longer Out[294]: a b c bools 7 0.275396 0.691034 0.826619 False In [295]: shorter == longer Out[295]: a b c bools 7 True True True True Performance of query() # DataFrame.query() using numexpr is slightly faster than Python for large frames. You will only see the performance benefits of using the numexpr engine with DataFrame.query() if your frame has more than approximately 100,000 rows. This plot was created using a DataFrame with 3 columns each containing floating point values generated using numpy.random.randn() . In [296]: df = pd . DataFrame ( np . random . randn ( 8 , 4 ), .....: index = dates , columns = [ 'A' , 'B' , 'C' , 'D' ]) .....: In [297]: df2 = df . copy () Duplicate data # If you want to identify and remove duplicate rows in a DataFrame, there are two methods that will help: duplicated and drop_duplicates . Each takes as an argument the columns to use to identify duplicated rows. duplicated returns a boolean vector whose length is the number of rows, and which indicates whether a row is duplicated. drop_duplicates removes duplicate rows. By default, the first observed row of a duplicate set is considered unique, but each method has a keep parameter to specify targets to be kept. keep='first' (default): mark / drop duplicates except for the first occurrence. keep='last' : mark / drop duplicates except for the last occurrence. keep=False : mark / drop all duplicates. In [298]: df2 = pd . DataFrame ({ 'a' : [ 'one' , 'one' , 'two' , 'two' , 'two' , 'three' , 'four' ], .....: 'b' : [ 'x' , 'y' , 'x' , 'y' , 'x' , 'x' , 'x' ], .....: 'c' : np . random . randn ( 7 )}) .....: In [299]: df2 Out[299]: a b c 0 one x -1.067137 1 one y 0.309500 2 two x -0.211056 3 two y -1.842023 4 two x -0.390820 5 three x -1.964475 6 four x 1.298329 In [300]: df2 . duplicated ( 'a' ) Out[300]: 0 False 1 True 2 False 3 True 4 True 5 False 6 False dtype: bool In [301]: df2 . duplicated ( 'a' , keep = 'last' ) Out[301]: 0 True 1 False 2 True 3 True 4 False 5 False 6 False dtype: bool In [302]: df2 . duplicated ( 'a' , keep = False ) Out[302]: 0 True 1 True 2 True 3 True 4 True 5 False 6 False dtype: bool In [303]: df2 . drop_duplicates ( 'a' ) Out[303]: a b c 0 one x -1.067137 2 two x -0.211056 5 three x -1.964475 6 four x 1.298329 In [304]: df2 . drop_duplicates ( 'a' , keep = 'last' ) Out[304]: a b c 1 one y 0.309500 4 two x -0.390820 5 three x -1.964475 6 four x 1.298329 In [305]: df2 . drop_duplicates ( 'a' , keep = False ) Out[305]: a b c 5 three x -1.964475 6 four x 1.298329 Also, you can pass a list of columns to identify duplications. In [306]: df2 . duplicated ([ 'a' , 'b' ]) Out[306]: 0 False 1 False 2 False 3 False 4 True 5 False 6 False dtype: bool In [307]: df2 . drop_duplicates ([ 'a' , 'b' ]) Out[307]: a b c 0 one x -1.067137 1 one y 0.309500 2 two x -0.211056 3 two y -1.842023 5 three x -1.964475 6 four x 1.298329 To drop duplicates by index value, use Index.duplicated then perform slicing. The same set of options are available for the keep parameter. In [308]: df3 = pd . DataFrame ({ 'a' : np . arange ( 6 ), .....: 'b' : np . random . randn ( 6 )}, .....: index = [ 'a' , 'a' , 'b' , 'c' , 'b' , 'a' ]) .....: In [309]: df3 Out[309]: a b a 0 1.440455 a 1 2.456086 b 2 1.038402 c 3 -0.894409 b 4 0.683536 a 5 3.082764 In [310]: df3 . index . duplicated () Out[310]: array([False, True, False, False, True, True]) In [311]: df3 [ ~ df3 . index . duplicated ()] Out[311]: a b a 0 1.440455 b 2 1.038402 c 3 -0.894409 In [312]: df3 [ ~ df3 . index . duplicated ( keep = 'last' )] Out[312]: a b c 3 -0.894409 b 4 0.683536 a 5 3.082764 In [313]: df3 [ ~ df3 . index . duplicated ( keep = False )] Out[313]: a b c 3 -0.894409 Dictionary-like get() method # Each of Series or DataFrame have a get method which can return a default value. In [314]: s = pd . Series ([ 1 , 2 , 3 ], index = [ 'a' , 'b' , 'c' ]) In [315]: s . get ( 'a' ) # equivalent to s['a'] Out[315]: 1 In [316]: s . get ( 'x' , default =- 1 ) Out[316]: -1 Looking up values by index/column labels # Sometimes you want to extract a set of values given a sequence of row labels and column labels, this can be achieved by pandas.factorize and NumPy indexing. For instance: In [317]: df = pd . DataFrame ({ 'col' : [ \"A\" , \"A\" , \"B\" , \"B\" ], .....: 'A' : [ 80 , 23 , np . nan , 22 ], .....: 'B' : [ 80 , 55 , 76 , 67 ]}) .....: In [318]: df Out[318]: col A B 0 A 80.0 80 1 A 23.0 55 2 B NaN 76 3 B 22.0 67 In [319]: idx , cols = pd . factorize ( df [ 'col' ]) In [320]: df . reindex ( cols , axis = 1 ) . to_numpy ()[ np . arange ( len ( df )), idx ] Out[320]: array([80., 23., 76., 67.]) Formerly this could be achieved with the dedicated DataFrame.lookup method which was deprecated in version 1.2.0 and removed in version 2.0.0. Index objects # The pandas Index class and its subclasses can be viewed as implementing an ordered multiset . Duplicates are allowed. Index also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an Index directly is to pass a list or other sequence to Index : In [321]: index = pd . Index ([ 'e' , 'd' , 'a' , 'b' ]) In [322]: index Out[322]: Index(['e', 'd', 'a', 'b'], dtype='object') In [323]: 'd' in index Out[323]: True or using numbers: In [324]: index = pd . Index ([ 1 , 5 , 12 ]) In [325]: index Out[325]: Index([1, 5, 12], dtype='int64') In [326]: 5 in index Out[326]: True If no dtype is given, Index tries to infer the dtype from the data. It is also possible to give an explicit dtype when instantiating an Index : In [327]: index = pd . Index ([ 'e' , 'd' , 'a' , 'b' ], dtype = \"string\" ) In [328]: index Out[328]: Index(['e', 'd', 'a', 'b'], dtype='string') In [329]: index = pd . Index ([ 1 , 5 , 12 ], dtype = \"int8\" ) In [330]: index Out[330]: Index([1, 5, 12], dtype='int8') In [331]: index = pd . Index ([ 1 , 5 , 12 ], dtype = \"float32\" ) In [332]: index Out[332]: Index([1.0, 5.0, 12.0], dtype='float32') You can also pass a name to be stored in the index: In [333]: index = pd . Index ([ 'e' , 'd' , 'a' , 'b' ], name = 'something' ) In [334]: index . name Out[334]: 'something' The name, if set, will be shown in the console display: In [335]: index = pd . Index ( list ( range ( 5 )), name = 'rows' ) In [336]: columns = pd . Index ([ 'A' , 'B' , 'C' ], name = 'cols' ) In [337]: df = pd . DataFrame ( np . random . randn ( 5 , 3 ), index = index , columns = columns ) In [338]: df Out[338]: cols A B C rows 0 1.295989 -1.051694 1.340429 1 -2.366110 0.428241 0.387275 2 0.433306 0.929548 0.278094 3 2.154730 -0.315628 0.264223 4 1.126818 1.132290 -0.353310 In [339]: df [ 'A' ] Out[339]: rows 0 1.295989 1 -2.366110 2 0.433306 3 2.154730 4 1.126818 Name: A, dtype: float64 Setting metadata # Indexes are âmostly immutableâ, but it is possible to set and change their name attribute. You can use the rename , set_names to set these attributes directly, and they default to returning a copy. See Advanced Indexing for usage of MultiIndexes. In [340]: ind = pd . Index ([ 1 , 2 , 3 ]) In [341]: ind . rename ( \"apple\" ) Out[341]: Index([1, 2, 3], dtype='int64', name='apple') In [342]: ind Out[342]: Index([1, 2, 3], dtype='int64') In [343]: ind = ind . set_names ([ \"apple\" ]) In [344]: ind . name = \"bob\" In [345]: ind Out[345]: Index([1, 2, 3], dtype='int64', name='bob') set_names , set_levels , and set_codes also take an optional level argument In [346]: index = pd . MultiIndex . from_product ([ range ( 3 ), [ 'one' , 'two' ]], names = [ 'first' , 'second' ]) In [347]: index Out[347]: MultiIndex([(0, 'one'), (0, 'two'), (1, 'one'), (1, 'two'), (2, 'one'), (2, 'two')], names=['first', 'second']) In [348]: index . levels [ 1 ] Out[348]: Index(['one', 'two'], dtype='object', name='second') In [349]: index . set_levels ([ \"a\" , \"b\" ], level = 1 ) Out[349]: MultiIndex([(0, 'a'), (0, 'b'), (1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')], names=['first', 'second']) Set operations on Index objects # The two main operations are union and intersection . Difference is provided via the .difference() method. In [350]: a = pd . Index ([ 'c' , 'b' , 'a' ]) In [351]: b = pd . Index ([ 'c' , 'e' , 'd' ]) In [352]: a . difference ( b ) Out[352]: Index(['a', 'b'], dtype='object') Also available is the symmetric_difference operation, which returns elements that appear in either idx1 or idx2 , but not in both. This is equivalent to the Index created by idx1.difference(idx2).union(idx2.difference(idx1)) , with duplicates dropped. In [353]: idx1 = pd . Index ([ 1 , 2 , 3 , 4 ]) In [354]: idx2 = pd . Index ([ 2 , 3 , 4 , 5 ]) In [355]: idx1 . symmetric_difference ( idx2 ) Out[355]: Index([1, 5], dtype='int64') Note The resulting index from a set operation will be sorted in ascending order. When performing Index.union() between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float In [356]: idx1 = pd . Index ([ 0 , 1 , 2 ]) In [357]: idx2 = pd . Index ([ 0.5 , 1.5 ]) In [358]: idx1 . union ( idx2 ) Out[358]: Index([0.0, 0.5, 1.0, 1.5, 2.0], dtype='float64') Missing values # Important Even though Index can hold missing values ( NaN ), it should be avoided if you do not want any unexpected results. For example, some operations exclude missing values implicitly. Index.fillna fills missing values with specified scalar value. In [359]: idx1 = pd . Index ([ 1 , np . nan , 3 , 4 ]) In [360]: idx1 Out[360]: Index([1.0, nan, 3.0, 4.0], dtype='float64') In [361]: idx1 . fillna ( 2 ) Out[361]: Index([1.0, 2.0, 3.0, 4.0], dtype='float64') In [362]: idx2 = pd . DatetimeIndex ([ pd . Timestamp ( '2011-01-01' ), .....: pd . NaT , .....: pd . Timestamp ( '2011-01-03' )]) .....: In [363]: idx2 Out[363]: DatetimeIndex(['2011-01-01', 'NaT', '2011-01-03'], dtype='datetime64[ns]', freq=None) In [364]: idx2 . fillna ( pd . Timestamp ( '2011-01-02' )) Out[364]: DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq=None) Set / reset index # Occasionally you will load or create a data set into a DataFrame and want to add an index after youâve already done so. There are a couple of different ways. Set an index # DataFrame has a set_index() method which takes a column name (for a regular Index ) or a list of column names (for a MultiIndex ). To create a new, re-indexed DataFrame: In [365]: data = pd . DataFrame ({ 'a' : [ 'bar' , 'bar' , 'foo' , 'foo' ], .....: 'b' : [ 'one' , 'two' , 'one' , 'two' ], .....: 'c' : [ 'z' , 'y' , 'x' , 'w' ], .....: 'd' : [ 1. , 2. , 3 , 4 ]}) .....: In [366]: data Out[366]: a b c d 0 bar one z 1.0 1 bar two y 2.0 2 foo one x 3.0 3 foo two w 4.0 In [367]: indexed1 = data . set_index ( 'c' ) In [368]: indexed1 Out[368]: a b d c z bar one 1.0 y bar two 2.0 x foo one 3.0 w foo two 4.0 In [369]: indexed2 = data . set_index ([ 'a' , 'b' ]) In [370]: indexed2 Out[370]: c d a b bar one z 1.0 two y 2.0 foo one x 3.0 two w 4.0 The append keyword option allow you to keep the existing index and append the given columns to a MultiIndex: In [371]: frame = data . set_index ( 'c' , drop = False ) In [372]: frame = frame . set_index ([ 'a' , 'b' ], append = True ) In [373]: frame Out[373]: c d c a b z bar one z 1.0 y bar two y 2.0 x foo one x 3.0 w foo two w 4.0 Other options in set_index allow you not drop the index columns. In [374]: data . set_index ( 'c' , drop = False ) Out[374]: a b c d c z bar one z 1.0 y bar two y 2.0 x foo one x 3.0 w foo two w 4.0 Reset the index # As a convenience, there is a new function on DataFrame called reset_index() which transfers the index values into the DataFrameâs columns and sets a simple integer index. This is the inverse operation of set_index() . In [375]: data Out[375]: a b c d 0 bar one z 1.0 1 bar two y 2.0 2 foo one x 3.0 3 foo two w 4.0 In [376]: data . reset_index () Out[376]: index a b c d 0 0 bar one z 1.0 1 1 bar two y 2.0 2 2 foo one x 3.0 3 3 foo two w 4.0 The output is more similar to a SQL table or a record array. The names for the columns derived from the index are the ones stored in the names attribute. You can use the level keyword to remove only a portion of the index: In [377]: frame Out[377]: c d c a b z bar one z 1.0 y bar two y 2.0 x foo one x 3.0 w foo two w 4.0 In [378]: frame . reset_index ( level = 1 ) Out[378]: a c d c b z one bar z 1.0 y two bar y 2.0 x one foo x 3.0 w two foo w 4.0 reset_index takes an optional parameter drop which if true simply discards the index, instead of putting index values in the DataFrameâs columns. Adding an ad hoc index # You can assign a custom index to the index attribute: In [379]: df_idx = pd . DataFrame ( range ( 4 )) In [380]: df_idx . index = pd . Index ([ 10 , 20 , 30 , 40 ], name = \"a\" ) In [381]: df_idx Out[381]: 0 a 10 0 20 1 30 2 40 3 Returning a view versus a copy # Warning Copy-on-Write will become the new default in pandas 3.0. This means that chained indexing will never work. As a consequence, the SettingWithCopyWarning wonât be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with ` pd.options.mode.copy_on_write = True ` even before pandas 3.0 is available. When setting values in a pandas object, care must be taken to avoid what is called chained indexing . Here is an example. In [382]: dfmi = pd . DataFrame ([ list ( 'abcd' ), .....: list ( 'efgh' ), .....: list ( 'ijkl' ), .....: list ( 'mnop' )], .....: columns = pd . MultiIndex . from_product ([[ 'one' , 'two' ], .....: [ 'first' , 'second' ]])) .....: In [383]: dfmi Out[383]: one two first second first second 0 a b c d 1 e f g h 2 i j k l 3 m n o p Compare these two access methods: In [384]: dfmi [ 'one' ][ 'second' ] Out[384]: 0 b 1 f 2 j 3 n Name: second, dtype: object In [385]: dfmi . loc [:, ( 'one' , 'second' )] Out[385]: 0 b 1 f 2 j 3 n Name: (one, second), dtype: object These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 ( .loc ) is much preferred over method 1 (chained [] ). dfmi['one'] selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation dfmi_with_one['second'] selects the series indexed by 'second' . This is indicated by the variable dfmi_with_one because pandas sees these operations as separate events. e.g. separate calls to __getitem__ , so it has to treat them as linear operations, they happen one after another. Contrast this to df.loc[:,('one','second')] which passes a nested tuple of (slice(None),('one','second')) to a single call to __getitem__ . This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired. Why does assignment fail when using chained indexing? # Warning Copy-on-Write will become the new default in pandas 3.0. This means than chained indexing will never work. As a consequence, the SettingWithCopyWarning wonât be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with ` pd.options.mode.copy_on_write = True ` even before pandas 3.0 is available. The problem in the previous section is just a performance issue. Whatâs up with the SettingWithCopy warning? We donât usually throw warnings around when you do something that might cost a few extra milliseconds! But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code: dfmi . loc [:, ( 'one' , 'second' )] = value # becomes dfmi . loc . __setitem__ (( slice ( None ), ( 'one' , 'second' )), value ) But this code is handled differently: dfmi [ 'one' ][ 'second' ] = value # becomes dfmi . __getitem__ ( 'one' ) . __setitem__ ( 'second' , value ) See that __getitem__ in there? Outside of simple cases, itâs very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the __setitem__ will modify dfmi or a temporary object that gets thrown out immediately afterward. Thatâs what SettingWithCopy is warning you about! Note You may be wondering whether we should be concerned about the loc property in the first example. But dfmi.loc is guaranteed to be dfmi itself with modified indexing behavior, so dfmi.loc.__getitem__ / dfmi.loc.__setitem__ operate on dfmi directly. Of course, dfmi.loc.__getitem__(idx) may be a view or a copy of dfmi . Sometimes a SettingWithCopy warning will arise at times when thereâs no obvious chained indexing going on. These are the bugs that SettingWithCopy is designed to catch! pandas is probably trying to warn you that youâve done this: def do_something ( df ): foo = df [[ 'bar' , 'baz' ]] # Is foo a view? A copy? Nobody knows! # ... many lines here ... # We don't know whether this will modify df or not! foo [ 'quux' ] = value return foo Yikes! Evaluation order matters # Warning Copy-on-Write will become the new default in pandas 3.0. This means than chained indexing will never work. As a consequence, the SettingWithCopyWarning wonât be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with ` pd.options.mode.copy_on_write = True ` even before pandas 3.0 is available. When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice. pandas has the SettingWithCopyWarning because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected. If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option mode.chained_assignment to one of these values: 'warn' , the default, means a SettingWithCopyWarning is printed. 'raise' means pandas will raise a SettingWithCopyError you have to deal with. None will suppress the warnings entirely. In [386]: dfb = pd . DataFrame ({ 'a' : [ 'one' , 'one' , 'two' , .....: 'three' , 'two' , 'one' , 'six' ], .....: 'c' : np . arange ( 7 )}) .....: # This will show the SettingWithCopyWarning # but the frame values will be set In [387]: dfb [ 'c' ][ dfb [ 'a' ] . str . startswith ( 'o' )] = 42 This however is operating on a copy and will not work. In [388]: with pd . option_context ( 'mode.chained_assignment' , 'warn' ): .....: dfb [ dfb [ 'a' ] . str . startswith ( 'o' )][ 'c' ] = 42 .....: A chained assignment can also crop up in setting in a mixed dtype frame. Note These setting rules apply to all of .loc/.iloc . The following is the recommended access method using .loc for multiple items (using mask ) and a single item using a fixed index: In [389]: dfc = pd . DataFrame ({ 'a' : [ 'one' , 'one' , 'two' , .....: 'three' , 'two' , 'one' , 'six' ], .....: 'c' : np . arange ( 7 )}) .....: In [390]: dfd = dfc . copy () # Setting multiple items using a mask In [391]: mask = dfd [ 'a' ] . str . startswith ( 'o' ) In [392]: dfd . loc [ mask , 'c' ] = 42 In [393]: dfd Out[393]: a c 0 one 42 1 one 42 2 two 2 3 three 3 4 two 4 5 one 42 6 six 6 # Setting a single item In [394]: dfd = dfc . copy () In [395]: dfd . loc [ 2 , 'a' ] = 11 In [396]: dfd Out[396]: a c 0 one 0 1 one 1 2 11 2 3 three 3 4 two 4 5 one 5 6 six 6 The following can work at times, but it is not guaranteed to, and therefore should be avoided: In [397]: dfd = dfc . copy () In [398]: dfd [ 'a' ][ 2 ] = 111 In [399]: dfd Out[399]: a c 0 one 0 1 one 1 2 111 2 3 three 3 4 two 4 5 one 5 6 six 6 Last, the subsequent example will not work at all, and so should be avoided: In [400]: with pd . option_context ( 'mode.chained_assignment' , 'raise' ): .....: dfd . loc [ 0 ][ 'a' ] = 1111 .....: --------------------------------------------------------------------------- SettingWithCopyError Traceback (most recent call last) <ipython-input-400-32ce785aaa5b> in ? () 1 with pd . option_context ( 'mode.chained_assignment' , 'raise' ): ----> 2 dfd . loc [ 0 ][ 'a' ] = 1111 ~/work/pandas/pandas/pandas/core/series.py in ? (self, key, value) 1296 ) 1297 1298 check_dict_or_set_indexers ( key ) 1299 key = com . apply_if_callable ( key , self ) -> 1300 cacher_needs_updating = self . _check_is_chained_assignment_possible () 1301 1302 if key is Ellipsis : 1303 key = slice ( None ) ~/work/pandas/pandas/pandas/core/series.py in ? (self) 1501 ref = self . _get_cacher () 1502 if ref is not None and ref . _is_mixed_type : 1503 self . _check_setitem_copy ( t = \"referent\" , force = True ) 1504 return True -> 1505 return super () . _check_is_chained_assignment_possible () ~/work/pandas/pandas/pandas/core/generic.py in ? (self) 4417 single - dtype meaning that the cacher should be updated following 4418 setting . 4419 \"\"\" 4420 if self._is_copy: -> 4421 self._check_setitem_copy(t=\"referent\") 4422 return False ~/work/pandas/pandas/pandas/core/generic.py in ? (self, t, force) 4491 \"indexing.html#returning-a-view-versus-a-copy\" 4492 ) 4493 4494 if value == \"raise\": -> 4495 raise SettingWithCopyError(t) 4496 if value == \"warn\": 4497 warnings.warn(t, SettingWithCopyWarning, stacklevel=find_stack_level()) SettingWithCopyError : A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Warning The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported. previous PyArrow Functionality next MultiIndex / advanced indexing On this page Different choices for indexing Basics Attribute access Slicing ranges Selection by label Slicing with labels Selection by position Selection by callable Combining positional and label-based indexing Reindexing Selecting random samples Setting with enlargement Fast scalar value getting and setting Boolean indexing Indexing with isin The where() Method and Masking Mask Setting with enlargement conditionally using numpy() The query() Method MultiIndex query() Syntax query() Use Cases query() Python versus pandas Syntax Comparison The in and not in operators Special use of the == operator with list objects Boolean operators Performance of query() Duplicate data Dictionary-like get() method Looking up values by index/column labels Index objects Setting metadata Set operations on Index objects Missing values Set / reset index Set an index Reset the index Adding an ad hoc index Returning a view versus a copy Why does assignment fail when using chained indexing? Evaluation order matters Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/indexing.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Sparse data... Sparse data structures # pandas provides data structures for efficiently storing sparse data. These are not necessarily sparse in the typical âmostly 0â. Rather, you can view these objects as being âcompressedâ where any data matching a specific value ( NaN / missing value, though any value can be chosen, including 0) is omitted. The compressed values are not actually stored in the array. In [1]: arr = np . random . randn ( 10 ) In [2]: arr [ 2 : - 2 ] = np . nan In [3]: ts = pd . Series ( pd . arrays . SparseArray ( arr )) In [4]: ts Out[4]: 0 0.469112 1 -0.282863 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 -0.861849 9 -2.104569 dtype: Sparse[float64, nan] Notice the dtype, Sparse[float64, nan] . The nan means that elements in the array that are nan arenât actually stored, only the non- nan elements are. Those non- nan elements have a float64 dtype. The sparse objects exist for memory efficiency reasons. Suppose you had a large, mostly NA DataFrame : In [5]: df = pd . DataFrame ( np . random . randn ( 10000 , 4 )) In [6]: df . iloc [: 9998 ] = np . nan In [7]: sdf = df . astype ( pd . SparseDtype ( \"float\" , np . nan )) In [8]: sdf . head () Out[8]: 0 1 2 3 0 NaN NaN NaN NaN 1 NaN NaN NaN NaN 2 NaN NaN NaN NaN 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN In [9]: sdf . dtypes Out[9]: 0 Sparse[float64, nan] 1 Sparse[float64, nan] 2 Sparse[float64, nan] 3 Sparse[float64, nan] dtype: object In [10]: sdf . sparse . density Out[10]: 0.0002 As you can see, the density (% of values that have not been âcompressedâ) is extremely low. This sparse object takes up much less memory on disk (pickled) and in the Python interpreter. In [11]: 'dense : {:0.2f} bytes' . format ( df . memory_usage () . sum () / 1e3 ) Out[11]: 'dense : 320.13 bytes' In [12]: 'sparse: {:0.2f} bytes' . format ( sdf . memory_usage () . sum () / 1e3 ) Out[12]: 'sparse: 0.22 bytes' Functionally, their behavior should be nearly identical to their dense counterparts. SparseArray # arrays.SparseArray is a ExtensionArray for storing an array of sparse values (see dtypes for more on extension arrays). It is a 1-dimensional ndarray-like object storing only values distinct from the fill_value : In [13]: arr = np . random . randn ( 10 ) In [14]: arr [ 2 : 5 ] = np . nan In [15]: arr [ 7 : 8 ] = np . nan In [16]: sparr = pd . arrays . SparseArray ( arr ) In [17]: sparr Out[17]: [-1.9556635297215477, -1.6588664275960427, nan, nan, nan, 1.1589328886422277, 0.14529711373305043, nan, 0.6060271905134522, 1.3342113401317768] Fill: nan IntIndex Indices: array([0, 1, 5, 6, 8, 9], dtype=int32) A sparse array can be converted to a regular (dense) ndarray with numpy.asarray() In [18]: np . asarray ( sparr ) Out[18]: array([-1.9557, -1.6589, nan, nan, nan, 1.1589, 0.1453, nan, 0.606 , 1.3342]) SparseDtype # The SparseArray.dtype property stores two pieces of information The dtype of the non-sparse values The scalar fill value In [19]: sparr . dtype Out[19]: Sparse[float64, nan] A SparseDtype may be constructed by passing only a dtype In [20]: pd . SparseDtype ( np . dtype ( 'datetime64[ns]' )) Out[20]: Sparse[datetime64[ns], numpy.datetime64('NaT')] in which case a default fill value will be used (for NumPy dtypes this is often the âmissingâ value for that dtype). To override this default an explicit fill value may be passed instead In [21]: pd . SparseDtype ( np . dtype ( 'datetime64[ns]' ), ....: fill_value = pd . Timestamp ( '2017-01-01' )) ....: Out[21]: Sparse[datetime64[ns], Timestamp('2017-01-01 00:00:00')] Finally, the string alias 'Sparse[dtype]' may be used to specify a sparse dtype in many places In [22]: pd . array ([ 1 , 0 , 0 , 2 ], dtype = 'Sparse[int]' ) Out[22]: [1, 0, 0, 2] Fill: 0 IntIndex Indices: array([0, 3], dtype=int32) Sparse accessor # pandas provides a .sparse accessor, similar to .str for string data, .cat for categorical data, and .dt for datetime-like data. This namespace provides attributes and methods that are specific to sparse data. In [23]: s = pd . Series ([ 0 , 0 , 1 , 2 ], dtype = \"Sparse[int]\" ) In [24]: s . sparse . density Out[24]: 0.5 In [25]: s . sparse . fill_value Out[25]: 0 This accessor is available only on data with SparseDtype , and on the Series class itself for creating a Series with sparse data from a scipy COO matrix with. A .sparse accessor has been added for DataFrame as well. See Sparse accessor for more. Sparse calculation # You can apply NumPy ufuncs to arrays.SparseArray and get a arrays.SparseArray as a result. In [26]: arr = pd . arrays . SparseArray ([ 1. , np . nan , np . nan , - 2. , np . nan ]) In [27]: np . abs ( arr ) Out[27]: [1.0, nan, nan, 2.0, nan] Fill: nan IntIndex Indices: array([0, 3], dtype=int32) The ufunc is also applied to fill_value . This is needed to get the correct dense result. In [28]: arr = pd . arrays . SparseArray ([ 1. , - 1 , - 1 , - 2. , - 1 ], fill_value =- 1 ) In [29]: np . abs ( arr ) Out[29]: [1, 1, 1, 2.0, 1] Fill: 1 IntIndex Indices: array([3], dtype=int32) In [30]: np . abs ( arr ) . to_dense () Out[30]: array([1., 1., 1., 2., 1.]) Conversion To convert data from sparse to dense, use the .sparse accessors In [31]: sdf . sparse . to_dense () Out[31]: 0 1 2 3 0 NaN NaN NaN NaN 1 NaN NaN NaN NaN 2 NaN NaN NaN NaN 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN ... ... ... ... ... 9995 NaN NaN NaN NaN 9996 NaN NaN NaN NaN 9997 NaN NaN NaN NaN 9998 0.509184 -0.774928 -1.369894 -0.382141 9999 0.280249 -1.648493 1.490865 -0.890819 [10000 rows x 4 columns] From dense to sparse, use DataFrame.astype() with a SparseDtype . In [32]: dense = pd . DataFrame ({ \"A\" : [ 1 , 0 , 0 , 1 ]}) In [33]: dtype = pd . SparseDtype ( int , fill_value = 0 ) In [34]: dense . astype ( dtype ) Out[34]: A 0 1 1 0 2 0 3 1 Interaction with scipy.sparse # Use DataFrame.sparse.from_spmatrix() to create a DataFrame with sparse values from a sparse matrix. In [35]: from scipy.sparse import csr_matrix In [36]: arr = np . random . random ( size = ( 1000 , 5 )) In [37]: arr [ arr < .9 ] = 0 In [38]: sp_arr = csr_matrix ( arr ) In [39]: sp_arr Out[39]: <Compressed Sparse Row sparse matrix of dtype 'float64' with 517 stored elements and shape (1000, 5)> In [40]: sdf = pd . DataFrame . sparse . from_spmatrix ( sp_arr ) In [41]: sdf . head () Out[41]: 0 1 2 3 4 0 0.95638 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 3 0 0 0 0 0 4 0.999552 0 0 0.956153 0 In [42]: sdf . dtypes Out[42]: 0 Sparse[float64, 0] 1 Sparse[float64, 0] 2 Sparse[float64, 0] 3 Sparse[float64, 0] 4 Sparse[float64, 0] dtype: object All sparse formats are supported, but matrices that are not in COOrdinate format will be converted, copying data as needed. To convert back to sparse SciPy matrix in COO format, you can use the DataFrame.sparse.to_coo() method: In [43]: sdf . sparse . to_coo () Out[43]: <COOrdinate sparse matrix of dtype 'float64' with 517 stored elements and shape (1000, 5)> Series.sparse.to_coo() is implemented for transforming a Series with sparse values indexed by a MultiIndex to a scipy.sparse.coo_matrix . The method requires a MultiIndex with two or more levels. In [44]: s = pd . Series ([ 3.0 , np . nan , 1.0 , 3.0 , np . nan , np . nan ]) In [45]: s . index = pd . MultiIndex . from_tuples ( ....: [ ....: ( 1 , 2 , \"a\" , 0 ), ....: ( 1 , 2 , \"a\" , 1 ), ....: ( 1 , 1 , \"b\" , 0 ), ....: ( 1 , 1 , \"b\" , 1 ), ....: ( 2 , 1 , \"b\" , 0 ), ....: ( 2 , 1 , \"b\" , 1 ), ....: ], ....: names = [ \"A\" , \"B\" , \"C\" , \"D\" ], ....: ) ....: In [46]: ss = s . astype ( 'Sparse' ) In [47]: ss Out[47]: A B C D 1 2 a 0 3.0 1 NaN 1 b 0 1.0 1 3.0 2 1 b 0 NaN 1 NaN dtype: Sparse[float64, nan] In the example below, we transform the Series to a sparse representation of a 2-d array by specifying that the first and second MultiIndex levels define labels for the rows and the third and fourth levels define labels for the columns. We also specify that the column and row labels should be sorted in the final sparse representation. In [48]: A , rows , columns = ss . sparse . to_coo ( ....: row_levels = [ \"A\" , \"B\" ], column_levels = [ \"C\" , \"D\" ], sort_labels = True ....: ) ....: In [49]: A Out[49]: <COOrdinate sparse matrix of dtype 'float64' with 3 stored elements and shape (3, 4)> In [50]: A . todense () Out[50]: matrix([[0., 0., 1., 3.], [3., 0., 0., 0.], [0., 0., 0., 0.]]) In [51]: rows Out[51]: [(1, 1), (1, 2), (2, 1)] In [52]: columns Out[52]: [('a', 0), ('a', 1), ('b', 0), ('b', 1)] Specifying different row and column labels (and not sorting them) yields a different sparse matrix: In [53]: A , rows , columns = ss . sparse . to_coo ( ....: row_levels = [ \"A\" , \"B\" , \"C\" ], column_levels = [ \"D\" ], sort_labels = False ....: ) ....: In [54]: A Out[54]: <COOrdinate sparse matrix of dtype 'float64' with 3 stored elements and shape (3, 2)> In [55]: A . todense () Out[55]: matrix([[3., 0.], [1., 3.], [0., 0.]]) In [56]: rows Out[56]: [(1, 2, 'a'), (1, 1, 'b'), (2, 1, 'b')] In [57]: columns Out[57]: [(0,), (1,)] A convenience method Series.sparse.from_coo() is implemented for creating a Series with sparse values from a scipy.sparse.coo_matrix . In [58]: from scipy import sparse In [59]: A = sparse . coo_matrix (([ 3.0 , 1.0 , 2.0 ], ([ 1 , 0 , 0 ], [ 0 , 2 , 3 ])), shape = ( 3 , 4 )) In [60]: A Out[60]: <COOrdinate sparse matrix of dtype 'float64' with 3 stored elements and shape (3, 4)> In [61]: A . todense () Out[61]: matrix([[0., 0., 1., 2.], [3., 0., 0., 0.], [0., 0., 0., 0.]]) The default behaviour (with dense_index=False ) simply returns a Series containing only the non-null entries. In [62]: ss = pd . Series . sparse . from_coo ( A ) In [63]: ss Out[63]: 0 2 1.0 3 2.0 1 0 3.0 dtype: Sparse[float64, nan] Specifying dense_index=True will result in an index that is the Cartesian product of the row and columns coordinates of the matrix. Note that this will consume a significant amount of memory (relative to dense_index=False ) if the sparse matrix is large (and sparse) enough. In [64]: ss_dense = pd . Series . sparse . from_coo ( A , dense_index = True ) In [65]: ss_dense Out[65]: 1 0 3.0 2 NaN 3 NaN 0 0 NaN 2 1.0 3 2.0 0 NaN 2 1.0 3 2.0 dtype: Sparse[float64, nan] previous Scaling to large datasets next Migration guide for the new string data type (pandas 3.0) On this page SparseArray SparseDtype Sparse accessor Sparse calculation Interaction with scipy.sparse Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/sparse.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Chart visualization Chart visualization # Note The examples below assume that youâre using Jupyter . This section demonstrates visualization through charting. For information on visualization of tabular data please see the section on Table Visualization . We use the standard convention for referencing the matplotlib API: In [1]: import matplotlib.pyplot as plt In [2]: plt . close ( \"all\" ) We provide the basics in pandas to easily create decent looking plots. See the ecosystem page for visualization libraries that go beyond the basics documented here. Note All calls to np.random are seeded with 123456. Basic plotting: plot # We will demonstrate the basics, see the cookbook for some advanced strategies. The plot method on Series and DataFrame is just a simple wrapper around plt.plot() : In [3]: np . random . seed ( 123456 ) In [4]: ts = pd . Series ( np . random . randn ( 1000 ), index = pd . date_range ( \"1/1/2000\" , periods = 1000 )) In [5]: ts = ts . cumsum () In [6]: ts . plot (); If the index consists of dates, it calls gcf().autofmt_xdate() to try to format the x-axis nicely as per above. On DataFrame, plot() is a convenience to plot all of the columns with labels: In [7]: df = pd . DataFrame ( np . random . randn ( 1000 , 4 ), index = ts . index , columns = list ( \"ABCD\" )) In [8]: df = df . cumsum () In [9]: plt . figure (); In [10]: df . plot (); You can plot one column versus another using the x and y keywords in plot() : In [11]: df3 = pd . DataFrame ( np . random . randn ( 1000 , 2 ), columns = [ \"B\" , \"C\" ]) . cumsum () In [12]: df3 [ \"A\" ] = pd . Series ( list ( range ( len ( df )))) In [13]: df3 . plot ( x = \"A\" , y = \"B\" ); Note For more formatting and styling options, see formatting below. Other plots # Plotting methods allow for a handful of plot styles other than the default line plot. These methods can be provided as the kind keyword argument to plot() , and include: âbarâ or âbarhâ for bar plots âhistâ for histogram âboxâ for boxplot âkdeâ or âdensityâ for density plots âareaâ for area plots âscatterâ for scatter plots âhexbinâ for hexagonal bin plots âpieâ for pie plots For example, a bar plot can be created the following way: In [14]: plt . figure (); In [15]: df . iloc [ 5 ] . plot ( kind = \"bar\" ); You can also create these other plots using the methods DataFrame.plot.<kind> instead of providing the kind keyword argument. This makes it easier to discover plot methods and the specific arguments they use: In [16]: df = pd . DataFrame () In [17]: df . plot .< TAB > # noqa: E225, E999 df.plot.area df.plot.barh df.plot.density df.plot.hist df.plot.line df.plot.scatter df.plot.bar df.plot.box df.plot.hexbin df.plot.kde df.plot.pie In addition to these kind s, there are the DataFrame.hist() , and DataFrame.boxplot() methods, which use a separate interface. Finally, there are several plotting functions in pandas.plotting that take a Series or DataFrame as an argument. These include: Scatter Matrix Andrews Curves Parallel Coordinates Lag Plot Autocorrelation Plot Bootstrap Plot RadViz Plots may also be adorned with errorbars or tables . Bar plots # For labeled, non-time series data, you may wish to produce a bar plot: In [18]: plt . figure (); In [19]: df . iloc [ 5 ] . plot . bar (); In [20]: plt . axhline ( 0 , color = \"k\" ); Calling a DataFrameâs plot.bar() method produces a multiple bar plot: In [21]: df2 = pd . DataFrame ( np . random . rand ( 10 , 4 ), columns = [ \"a\" , \"b\" , \"c\" , \"d\" ]) In [22]: df2 . plot . bar (); To produce a stacked bar plot, pass stacked=True : In [23]: df2 . plot . bar ( stacked = True ); To get horizontal bar plots, use the barh method: In [24]: df2 . plot . barh ( stacked = True ); Histograms # Histograms can be drawn by using the DataFrame.plot.hist() and Series.plot.hist() methods. In [25]: df4 = pd . DataFrame ( ....: { ....: \"a\" : np . random . randn ( 1000 ) + 1 , ....: \"b\" : np . random . randn ( 1000 ), ....: \"c\" : np . random . randn ( 1000 ) - 1 , ....: }, ....: columns = [ \"a\" , \"b\" , \"c\" ], ....: ) ....: In [26]: plt . figure (); In [27]: df4 . plot . hist ( alpha = 0.5 ); A histogram can be stacked using stacked=True . Bin size can be changed using the bins keyword. In [28]: plt . figure (); In [29]: df4 . plot . hist ( stacked = True , bins = 20 ); You can pass other keywords supported by matplotlib hist . For example, horizontal and cumulative histograms can be drawn by orientation='horizontal' and cumulative=True . In [30]: plt . figure (); In [31]: df4 [ \"a\" ] . plot . hist ( orientation = \"horizontal\" , cumulative = True ); See the hist method and the matplotlib hist documentation for more. The existing interface DataFrame.hist to plot histogram still can be used. In [32]: plt . figure (); In [33]: df [ \"A\" ] . diff () . hist (); DataFrame.hist() plots the histograms of the columns on multiple subplots: In [34]: plt . figure (); In [35]: df . diff () . hist ( color = \"k\" , alpha = 0.5 , bins = 50 ); The by keyword can be specified to plot grouped histograms: In [36]: data = pd . Series ( np . random . randn ( 1000 )) In [37]: data . hist ( by = np . random . randint ( 0 , 4 , 1000 ), figsize = ( 6 , 4 )); In addition, the by keyword can also be specified in DataFrame.plot.hist() . Changed in version 1.4.0. In [38]: data = pd . DataFrame ( ....: { ....: \"a\" : np . random . choice ([ \"x\" , \"y\" , \"z\" ], 1000 ), ....: \"b\" : np . random . choice ([ \"e\" , \"f\" , \"g\" ], 1000 ), ....: \"c\" : np . random . randn ( 1000 ), ....: \"d\" : np . random . randn ( 1000 ) - 1 , ....: }, ....: ) ....: In [39]: data . plot . hist ( by = [ \"a\" , \"b\" ], figsize = ( 10 , 5 )); Box plots # Boxplot can be drawn calling Series.plot.box() and DataFrame.plot.box() , or DataFrame.boxplot() to visualize the distribution of values within each column. For instance, here is a boxplot representing five trials of 10 observations of a uniform random variable on [0,1). In [40]: df = pd . DataFrame ( np . random . rand ( 10 , 5 ), columns = [ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" ]) In [41]: df . plot . box (); Boxplot can be colorized by passing color keyword. You can pass a dict whose keys are boxes , whiskers , medians and caps . If some keys are missing in the dict , default colors are used for the corresponding artists. Also, boxplot has sym keyword to specify fliers style. When you pass other type of arguments via color keyword, it will be directly passed to matplotlib for all the boxes , whiskers , medians and caps colorization. The colors are applied to every boxes to be drawn. If you want more complicated colorization, you can get each drawn artists by passing return_type . In [42]: color = { ....: \"boxes\" : \"DarkGreen\" , ....: \"whiskers\" : \"DarkOrange\" , ....: \"medians\" : \"DarkBlue\" , ....: \"caps\" : \"Gray\" , ....: } ....: In [43]: df . plot . box ( color = color , sym = \"r+\" ); Also, you can pass other keywords supported by matplotlib boxplot . For example, horizontal and custom-positioned boxplot can be drawn by vert=False and positions keywords. In [44]: df . plot . box ( vert = False , positions = [ 1 , 4 , 5 , 6 , 8 ]); See the boxplot method and the matplotlib boxplot documentation for more. The existing interface DataFrame.boxplot to plot boxplot still can be used. In [45]: df = pd . DataFrame ( np . random . rand ( 10 , 5 )) In [46]: plt . figure (); In [47]: bp = df . boxplot () You can create a stratified boxplot using the by keyword argument to create groupings. For instance, In [48]: df = pd . DataFrame ( np . random . rand ( 10 , 2 ), columns = [ \"Col1\" , \"Col2\" ]) In [49]: df [ \"X\" ] = pd . Series ([ \"A\" , \"A\" , \"A\" , \"A\" , \"A\" , \"B\" , \"B\" , \"B\" , \"B\" , \"B\" ]) In [50]: plt . figure (); In [51]: bp = df . boxplot ( by = \"X\" ) You can also pass a subset of columns to plot, as well as group by multiple columns: In [52]: df = pd . DataFrame ( np . random . rand ( 10 , 3 ), columns = [ \"Col1\" , \"Col2\" , \"Col3\" ]) In [53]: df [ \"X\" ] = pd . Series ([ \"A\" , \"A\" , \"A\" , \"A\" , \"A\" , \"B\" , \"B\" , \"B\" , \"B\" , \"B\" ]) In [54]: df [ \"Y\" ] = pd . Series ([ \"A\" , \"B\" , \"A\" , \"B\" , \"A\" , \"B\" , \"A\" , \"B\" , \"A\" , \"B\" ]) In [55]: plt . figure (); In [56]: bp = df . boxplot ( column = [ \"Col1\" , \"Col2\" ], by = [ \"X\" , \"Y\" ]) You could also create groupings with DataFrame.plot.box() , for instance: Changed in version 1.4.0. In [57]: df = pd . DataFrame ( np . random . rand ( 10 , 3 ), columns = [ \"Col1\" , \"Col2\" , \"Col3\" ]) In [58]: df [ \"X\" ] = pd . Series ([ \"A\" , \"A\" , \"A\" , \"A\" , \"A\" , \"B\" , \"B\" , \"B\" , \"B\" , \"B\" ]) In [59]: plt . figure (); In [60]: bp = df . plot . box ( column = [ \"Col1\" , \"Col2\" ], by = \"X\" ) In boxplot , the return type can be controlled by the return_type , keyword. The valid choices are {\"axes\", \"dict\", \"both\", None} . Faceting, created by DataFrame.boxplot with the by keyword, will affect the output type as well: return_type Faceted Output type None No axes None Yes 2-D ndarray of axes 'axes' No axes 'axes' Yes Series of axes 'dict' No dict of artists 'dict' Yes Series of dicts of artists 'both' No namedtuple 'both' Yes Series of namedtuples Groupby.boxplot always returns a Series of return_type . In [61]: np . random . seed ( 1234 ) In [62]: df_box = pd . DataFrame ( np . random . randn ( 50 , 2 )) In [63]: df_box [ \"g\" ] = np . random . choice ([ \"A\" , \"B\" ], size = 50 ) In [64]: df_box . loc [ df_box [ \"g\" ] == \"B\" , 1 ] += 3 In [65]: bp = df_box . boxplot ( by = \"g\" ) The subplots above are split by the numeric columns first, then the value of the g column. Below the subplots are first split by the value of g , then by the numeric columns. In [66]: bp = df_box . groupby ( \"g\" ) . boxplot () Area plot # You can create area plots with Series.plot.area() and DataFrame.plot.area() . Area plots are stacked by default. To produce stacked area plot, each column must be either all positive or all negative values. When input data contains NaN , it will be automatically filled by 0. If you want to drop or fill by different values, use dataframe.dropna() or dataframe.fillna() before calling plot . In [67]: df = pd . DataFrame ( np . random . rand ( 10 , 4 ), columns = [ \"a\" , \"b\" , \"c\" , \"d\" ]) In [68]: df . plot . area (); To produce an unstacked plot, pass stacked=False . Alpha value is set to 0.5 unless otherwise specified: In [69]: df . plot . area ( stacked = False ); Scatter plot # Scatter plot can be drawn by using the DataFrame.plot.scatter() method. Scatter plot requires numeric columns for the x and y axes. These can be specified by the x and y keywords. In [70]: df = pd . DataFrame ( np . random . rand ( 50 , 4 ), columns = [ \"a\" , \"b\" , \"c\" , \"d\" ]) In [71]: df [ \"species\" ] = pd . Categorical ( ....: [ \"setosa\" ] * 20 + [ \"versicolor\" ] * 20 + [ \"virginica\" ] * 10 ....: ) ....: In [72]: df . plot . scatter ( x = \"a\" , y = \"b\" ); To plot multiple column groups in a single axes, repeat plot method specifying target ax . It is recommended to specify color and label keywords to distinguish each groups. In [73]: ax = df . plot . scatter ( x = \"a\" , y = \"b\" , color = \"DarkBlue\" , label = \"Group 1\" ) In [74]: df . plot . scatter ( x = \"c\" , y = \"d\" , color = \"DarkGreen\" , label = \"Group 2\" , ax = ax ); The keyword c may be given as the name of a column to provide colors for each point: In [75]: df . plot . scatter ( x = \"a\" , y = \"b\" , c = \"c\" , s = 50 ); If a categorical column is passed to c , then a discrete colorbar will be produced: Added in version 1.3.0. In [76]: df . plot . scatter ( x = \"a\" , y = \"b\" , c = \"species\" , cmap = \"viridis\" , s = 50 ); You can pass other keywords supported by matplotlib scatter . The example below shows a bubble chart using a column of the DataFrame as the bubble size. In [77]: df . plot . scatter ( x = \"a\" , y = \"b\" , s = df [ \"c\" ] * 200 ); See the scatter method and the matplotlib scatter documentation for more. Hexagonal bin plot # You can create hexagonal bin plots with DataFrame.plot.hexbin() . Hexbin plots can be a useful alternative to scatter plots if your data are too dense to plot each point individually. In [78]: df = pd . DataFrame ( np . random . randn ( 1000 , 2 ), columns = [ \"a\" , \"b\" ]) In [79]: df [ \"b\" ] = df [ \"b\" ] + np . arange ( 1000 ) In [80]: df . plot . hexbin ( x = \"a\" , y = \"b\" , gridsize = 25 ); A useful keyword argument is gridsize ; it controls the number of hexagons in the x-direction, and defaults to 100. A larger gridsize means more, smaller bins. By default, a histogram of the counts around each (x, y) point is computed. You can specify alternative aggregations by passing values to the C and reduce_C_function arguments. C specifies the value at each (x, y) point and reduce_C_function is a function of one argument that reduces all the values in a bin to a single number (e.g. mean , max , sum , std ). In this example the positions are given by columns a and b , while the value is given by column z . The bins are aggregated with NumPyâs max function. In [81]: df = pd . DataFrame ( np . random . randn ( 1000 , 2 ), columns = [ \"a\" , \"b\" ]) In [82]: df [ \"b\" ] = df [ \"b\" ] + np . arange ( 1000 ) In [83]: df [ \"z\" ] = np . random . uniform ( 0 , 3 , 1000 ) In [84]: df . plot . hexbin ( x = \"a\" , y = \"b\" , C = \"z\" , reduce_C_function = np . max , gridsize = 25 ); See the hexbin method and the matplotlib hexbin documentation for more. Pie plot # You can create a pie plot with DataFrame.plot.pie() or Series.plot.pie() . If your data includes any NaN , they will be automatically filled with 0. A ValueError will be raised if there are any negative values in your data. In [85]: series = pd . Series ( 3 * np . random . rand ( 4 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" ], name = \"series\" ) In [86]: series . plot . pie ( figsize = ( 6 , 6 )); For pie plots itâs best to use square figures, i.e. a figure aspect ratio 1. You can create the figure with equal width and height, or force the aspect ratio to be equal after plotting by calling ax.set_aspect('equal') on the returned axes object. Note that pie plot with DataFrame requires that you either specify a target column by the y argument or subplots=True . When y is specified, pie plot of selected column will be drawn. If subplots=True is specified, pie plots for each column are drawn as subplots. A legend will be drawn in each pie plots by default; specify legend=False to hide it. In [87]: df = pd . DataFrame ( ....: 3 * np . random . rand ( 4 , 2 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" ], columns = [ \"x\" , \"y\" ] ....: ) ....: In [88]: df . plot . pie ( subplots = True , figsize = ( 8 , 4 )); You can use the labels and colors keywords to specify the labels and colors of each wedge. Warning Most pandas plots use the label and color arguments (note the lack of âsâ on those). To be consistent with matplotlib.pyplot.pie() you must use labels and colors . If you want to hide wedge labels, specify labels=None . If fontsize is specified, the value will be applied to wedge labels. Also, other keywords supported by matplotlib.pyplot.pie() can be used. In [89]: series . plot . pie ( ....: labels = [ \"AA\" , \"BB\" , \"CC\" , \"DD\" ], ....: colors = [ \"r\" , \"g\" , \"b\" , \"c\" ], ....: autopct = \" %.2f \" , ....: fontsize = 20 , ....: figsize = ( 6 , 6 ), ....: ); ....: If you pass values whose sum total is less than 1.0 they will be rescaled so that they sum to 1. In [90]: series = pd . Series ([ 0.1 ] * 4 , index = [ \"a\" , \"b\" , \"c\" , \"d\" ], name = \"series2\" ) In [91]: series . plot . pie ( figsize = ( 6 , 6 )); See the matplotlib pie documentation for more. Plotting with missing data # pandas tries to be pragmatic about plotting DataFrames or Series that contain missing data. Missing values are dropped, left out, or filled depending on the plot type. Plot Type NaN Handling Line Leave gaps at NaNs Line (stacked) Fill 0âs Bar Fill 0âs Scatter Drop NaNs Histogram Drop NaNs (column-wise) Box Drop NaNs (column-wise) Area Fill 0âs KDE Drop NaNs (column-wise) Hexbin Drop NaNs Pie Fill 0âs If any of these defaults are not what you want, or if you want to be explicit about how missing values are handled, consider using fillna() or dropna() before plotting. Plotting tools # These functions can be imported from pandas.plotting and take a Series or DataFrame as an argument. Scatter matrix plot # You can create a scatter plot matrix using the scatter_matrix method in pandas.plotting : In [92]: from pandas.plotting import scatter_matrix In [93]: df = pd . DataFrame ( np . random . randn ( 1000 , 4 ), columns = [ \"a\" , \"b\" , \"c\" , \"d\" ]) In [94]: scatter_matrix ( df , alpha = 0.2 , figsize = ( 6 , 6 ), diagonal = \"kde\" ); Density plot # You can create density plots using the Series.plot.kde() and DataFrame.plot.kde() methods. In [95]: ser = pd . Series ( np . random . randn ( 1000 )) In [96]: ser . plot . kde (); Andrews curves # Andrews curves allow one to plot multivariate data as a large number of curves that are created using the attributes of samples as coefficients for Fourier series, see the Wikipedia entry for more information. By coloring these curves differently for each class it is possible to visualize data clustering. Curves belonging to samples of the same class will usually be closer together and form larger structures. Note : The âIrisâ dataset is available here . In [97]: from pandas.plotting import andrews_curves In [98]: data = pd . read_csv ( \"data/iris.data\" ) In [99]: plt . figure (); In [100]: andrews_curves ( data , \"Name\" ); Parallel coordinates # Parallel coordinates is a plotting technique for plotting multivariate data, see the Wikipedia entry for an introduction. Parallel coordinates allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together. In [101]: from pandas.plotting import parallel_coordinates In [102]: data = pd . read_csv ( \"data/iris.data\" ) In [103]: plt . figure (); In [104]: parallel_coordinates ( data , \"Name\" ); Lag plot # Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:] . In [105]: from pandas.plotting import lag_plot In [106]: plt . figure (); In [107]: spacing = np . linspace ( - 99 * np . pi , 99 * np . pi , num = 1000 ) In [108]: data = pd . Series ( 0.1 * np . random . rand ( 1000 ) + 0.9 * np . sin ( spacing )) In [109]: lag_plot ( data ); Autocorrelation plot # Autocorrelation plots are often used for checking randomness in time series. This is done by computing autocorrelations for data values at varying time lags. If time series is random, such autocorrelations should be near zero for any and all time-lag separations. If time series is non-random then one or more of the autocorrelations will be significantly non-zero. The horizontal lines displayed in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. See the Wikipedia entry for more about autocorrelation plots. In [110]: from pandas.plotting import autocorrelation_plot In [111]: plt . figure (); In [112]: spacing = np . linspace ( - 9 * np . pi , 9 * np . pi , num = 1000 ) In [113]: data = pd . Series ( 0.7 * np . random . rand ( 1000 ) + 0.3 * np . sin ( spacing )) In [114]: autocorrelation_plot ( data ); Bootstrap plot # Bootstrap plots are used to visually assess the uncertainty of a statistic, such as mean, median, midrange, etc. A random subset of a specified size is selected from a data set, the statistic in question is computed for this subset and the process is repeated a specified number of times. Resulting plots and histograms are what constitutes the bootstrap plot. In [115]: from pandas.plotting import bootstrap_plot In [116]: data = pd . Series ( np . random . rand ( 1000 )) In [117]: bootstrap_plot ( data , size = 50 , samples = 500 , color = \"grey\" ); RadViz # RadViz is a way of visualizing multi-variate data. It is based on a simple spring tension minimization algorithm. Basically you set up a bunch of points in a plane. In our case they are equally spaced on a unit circle. Each point represents a single attribute. You then pretend that each sample in the data set is attached to each of these points by a spring, the stiffness of which is proportional to the numerical value of that attribute (they are normalized to unit interval). The point in the plane, where our sample settles to (where the forces acting on our sample are at an equilibrium) is where a dot representing our sample will be drawn. Depending on which class that sample belongs it will be colored differently. See the R package Radviz for more information. Note : The âIrisâ dataset is available here . In [118]: from pandas.plotting import radviz In [119]: data = pd . read_csv ( \"data/iris.data\" ) In [120]: plt . figure (); In [121]: radviz ( data , \"Name\" ); Plot formatting # Setting the plot style # From version 1.5 and up, matplotlib offers a range of pre-configured plotting styles. Setting the style can be used to easily give plots the general look that you want. Setting the style is as easy as calling matplotlib.style.use(my_plot_style) before creating your plot. For example you could write matplotlib.style.use('ggplot') for ggplot-style plots. You can see the various available style names at matplotlib.style.available and itâs very easy to try them out. General plot style arguments # Most plotting methods have a set of keyword arguments that control the layout and formatting of the returned plot: In [122]: plt . figure (); In [123]: ts . plot ( style = \"k--\" , label = \"Series\" ); For each kind of plot (e.g. line , bar , scatter ) any additional arguments keywords are passed along to the corresponding matplotlib function ( ax.plot() , ax.bar() , ax.scatter() ). These can be used to control additional styling, beyond what pandas provides. Controlling the legend # You may set the legend argument to False to hide the legend, which is shown by default. In [124]: df = pd . DataFrame ( np . random . randn ( 1000 , 4 ), index = ts . index , columns = list ( \"ABCD\" )) In [125]: df = df . cumsum () In [126]: df . plot ( legend = False ); Controlling the labels # You may set the xlabel and ylabel arguments to give the plot custom labels for x and y axis. By default, pandas will pick up index name as xlabel, while leaving it empty for ylabel. In [127]: df . plot (); In [128]: df . plot ( xlabel = \"new x\" , ylabel = \"new y\" ); Scales # You may pass logy to get a log-scale Y axis. In [129]: ts = pd . Series ( np . random . randn ( 1000 ), index = pd . date_range ( \"1/1/2000\" , periods = 1000 )) In [130]: ts = np . exp ( ts . cumsum ()) In [131]: ts . plot ( logy = True ); See also the logx and loglog keyword arguments. Plotting on a secondary y-axis # To plot data on a secondary y-axis, use the secondary_y keyword: In [132]: df [ \"A\" ] . plot (); In [133]: df [ \"B\" ] . plot ( secondary_y = True , style = \"g\" ); To plot some columns in a DataFrame , give the column names to the secondary_y keyword: In [134]: plt . figure (); In [135]: ax = df . plot ( secondary_y = [ \"A\" , \"B\" ]) In [136]: ax . set_ylabel ( \"CD scale\" ); In [137]: ax . right_ax . set_ylabel ( \"AB scale\" ); Note that the columns plotted on the secondary y-axis is automatically marked with â(right)â in the legend. To turn off the automatic marking, use the mark_right=False keyword: In [138]: plt . figure (); In [139]: df . plot ( secondary_y = [ \"A\" , \"B\" ], mark_right = False ); Custom formatters for timeseries plots # pandas provides custom formatters for timeseries plots. These change the formatting of the axis labels for dates and times. By default, the custom formatters are applied only to plots created by pandas with DataFrame.plot() or Series.plot() . To have them apply to all plots, including those made by matplotlib, set the option pd.options.plotting.matplotlib.register_converters = True or use pandas.plotting.register_matplotlib_converters() . Suppressing tick resolution adjustment # pandas includes automatic tick resolution adjustment for regular frequency time-series data. For limited cases where pandas cannot infer the frequency information (e.g., in an externally created twinx ), you can choose to suppress this behavior for alignment purposes. Here is the default behavior, notice how the x-axis tick labeling is performed: In [140]: plt . figure (); In [141]: df [ \"A\" ] . plot (); Using the x_compat parameter, you can suppress this behavior: In [142]: plt . figure (); In [143]: df [ \"A\" ] . plot ( x_compat = True ); If you have more than one plot that needs to be suppressed, the use method in pandas.plotting.plot_params can be used in a with statement: In [144]: plt . figure (); In [145]: with pd . plotting . plot_params . use ( \"x_compat\" , True ): .....: df [ \"A\" ] . plot ( color = \"r\" ) .....: df [ \"B\" ] . plot ( color = \"g\" ) .....: df [ \"C\" ] . plot ( color = \"b\" ) .....: Automatic date tick adjustment # TimedeltaIndex now uses the native matplotlib tick locator methods, it is useful to call the automatic date tick adjustment from matplotlib for figures whose ticklabels overlap. See the autofmt_xdate method and the matplotlib documentation for more. Subplots # Each Series in a DataFrame can be plotted on a different axis with the subplots keyword: In [146]: df . plot ( subplots = True , figsize = ( 6 , 6 )); Using layout and targeting multiple axes # The layout of subplots can be specified by the layout keyword. It can accept (rows, columns) . The layout keyword can be used in hist and boxplot also. If the input is invalid, a ValueError will be raised. The number of axes which can be contained by rows x columns specified by layout must be larger than the number of required subplots. If layout can contain more axes than required, blank axes are not drawn. Similar to a NumPy arrayâs reshape method, you can use -1 for one dimension to automatically calculate the number of rows or columns needed, given the other. In [147]: df . plot ( subplots = True , layout = ( 2 , 3 ), figsize = ( 6 , 6 ), sharex = False ); The above example is identical to using: In [148]: df . plot ( subplots = True , layout = ( 2 , - 1 ), figsize = ( 6 , 6 ), sharex = False ); The required number of columns (3) is inferred from the number of series to plot and the given number of rows (2). You can pass multiple axes created beforehand as list-like via ax keyword. This allows more complicated layouts. The passed axes must be the same number as the subplots being drawn. When multiple axes are passed via the ax keyword, layout , sharex and sharey keywords donât affect to the output. You should explicitly pass sharex=False and sharey=False , otherwise you will see a warning. In [149]: fig , axes = plt . subplots ( 4 , 4 , figsize = ( 9 , 9 )) In [150]: plt . subplots_adjust ( wspace = 0.5 , hspace = 0.5 ) In [151]: target1 = [ axes [ 0 ][ 0 ], axes [ 1 ][ 1 ], axes [ 2 ][ 2 ], axes [ 3 ][ 3 ]] In [152]: target2 = [ axes [ 3 ][ 0 ], axes [ 2 ][ 1 ], axes [ 1 ][ 2 ], axes [ 0 ][ 3 ]] In [153]: df . plot ( subplots = True , ax = target1 , legend = False , sharex = False , sharey = False ); In [154]: ( - df ) . plot ( subplots = True , ax = target2 , legend = False , sharex = False , sharey = False ); Another option is passing an ax argument to Series.plot() to plot on a particular axis: In [155]: np . random . seed ( 123456 ) In [156]: ts = pd . Series ( np . random . randn ( 1000 ), index = pd . date_range ( \"1/1/2000\" , periods = 1000 )) In [157]: ts = ts . cumsum () In [158]: df = pd . DataFrame ( np . random . randn ( 1000 , 4 ), index = ts . index , columns = list ( \"ABCD\" )) In [159]: df = df . cumsum () In [160]: fig , axes = plt . subplots ( nrows = 2 , ncols = 2 ) In [161]: plt . subplots_adjust ( wspace = 0.2 , hspace = 0.5 ) In [162]: df [ \"A\" ] . plot ( ax = axes [ 0 , 0 ]); In [163]: axes [ 0 , 0 ] . set_title ( \"A\" ); In [164]: df [ \"B\" ] . plot ( ax = axes [ 0 , 1 ]); In [165]: axes [ 0 , 1 ] . set_title ( \"B\" ); In [166]: df [ \"C\" ] . plot ( ax = axes [ 1 , 0 ]); In [167]: axes [ 1 , 0 ] . set_title ( \"C\" ); In [168]: df [ \"D\" ] . plot ( ax = axes [ 1 , 1 ]); In [169]: axes [ 1 , 1 ] . set_title ( \"D\" ); Plotting with error bars # Plotting with error bars is supported in DataFrame.plot() and Series.plot() . Horizontal and vertical error bars can be supplied to the xerr and yerr keyword arguments to plot() . The error values can be specified using a variety of formats: As a DataFrame or dict of errors with column names matching the columns attribute of the plotting DataFrame or matching the name attribute of the Series . As a str indicating which of the columns of plotting DataFrame contain the error values. As raw values ( list , tuple , or np.ndarray ). Must be the same length as the plotting DataFrame / Series . Here is an example of one way to easily plot group means with standard deviations from the raw data. # Generate the data In [170]: ix3 = pd . MultiIndex . from_arrays ( .....: [ .....: [ \"a\" , \"a\" , \"a\" , \"a\" , \"a\" , \"b\" , \"b\" , \"b\" , \"b\" , \"b\" ], .....: [ \"foo\" , \"foo\" , \"foo\" , \"bar\" , \"bar\" , \"foo\" , \"foo\" , \"bar\" , \"bar\" , \"bar\" ], .....: ], .....: names = [ \"letter\" , \"word\" ], .....: ) .....: In [171]: df3 = pd . DataFrame ( .....: { .....: \"data1\" : [ 9 , 3 , 2 , 4 , 3 , 2 , 4 , 6 , 3 , 2 ], .....: \"data2\" : [ 9 , 6 , 5 , 7 , 5 , 4 , 5 , 6 , 5 , 1 ], .....: }, .....: index = ix3 , .....: ) .....: # Group by index labels and take the means and standard deviations # for each group In [172]: gp3 = df3 . groupby ( level = ( \"letter\" , \"word\" )) In [173]: means = gp3 . mean () In [174]: errors = gp3 . std () In [175]: means Out[175]: data1 data2 letter word a bar 3.500000 6.000000 foo 4.666667 6.666667 b bar 3.666667 4.000000 foo 3.000000 4.500000 In [176]: errors Out[176]: data1 data2 letter word a bar 0.707107 1.414214 foo 3.785939 2.081666 b bar 2.081666 2.645751 foo 1.414214 0.707107 # Plot In [177]: fig , ax = plt . subplots () In [178]: means . plot . bar ( yerr = errors , ax = ax , capsize = 4 , rot = 0 ); Asymmetrical error bars are also supported, however raw error values must be provided in this case. For a N length Series , a 2xN array should be provided indicating lower and upper (or left and right) errors. For a MxN DataFrame , asymmetrical errors should be in a Mx2xN array. Here is an example of one way to plot the min/max range using asymmetrical error bars. In [179]: mins = gp3 . min () In [180]: maxs = gp3 . max () # errors should be positive, and defined in the order of lower, upper In [181]: errors = [[ means [ c ] - mins [ c ], maxs [ c ] - means [ c ]] for c in df3 . columns ] # Plot In [182]: fig , ax = plt . subplots () In [183]: means . plot . bar ( yerr = errors , ax = ax , capsize = 4 , rot = 0 ); Plotting tables # Plotting with matplotlib table is now supported in DataFrame.plot() and Series.plot() with a table keyword. The table keyword can accept bool , DataFrame or Series . The simple way to draw a table is to specify table=True . Data will be transposed to meet matplotlibâs default layout. In [184]: np . random . seed ( 123456 ) In [185]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 6.5 )) In [186]: df = pd . DataFrame ( np . random . rand ( 5 , 3 ), columns = [ \"a\" , \"b\" , \"c\" ]) In [187]: ax . xaxis . tick_top () # Display x-axis ticks on top. In [188]: df . plot ( table = True , ax = ax ); Also, you can pass a different DataFrame or Series to the table keyword. The data will be drawn as displayed in print method (not transposed automatically). If required, it should be transposed manually as seen in the example below. In [189]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 6.75 )) In [190]: ax . xaxis . tick_top () # Display x-axis ticks on top. In [191]: df . plot ( table = np . round ( df . T , 2 ), ax = ax ); There also exists a helper function pandas.plotting.table , which creates a table from DataFrame or Series , and adds it to an matplotlib.Axes instance. This function can accept keywords which the matplotlib table has. In [192]: from pandas.plotting import table In [193]: fig , ax = plt . subplots ( 1 , 1 ) In [194]: table ( ax , np . round ( df . describe (), 2 ), loc = \"upper right\" , colWidths = [ 0.2 , 0.2 , 0.2 ]); In [195]: df . plot ( ax = ax , ylim = ( 0 , 2 ), legend = None ); Note : You can get table instances on the axes using axes.tables property for further decorations. See the matplotlib table documentation for more. Colormaps # A potential issue when plotting a large number of columns is that it can be difficult to distinguish some series due to repetition in the default colors. To remedy this, DataFrame plotting supports the use of the colormap argument, which accepts either a Matplotlib colormap or a string that is a name of a colormap registered with Matplotlib. A visualization of the default matplotlib colormaps is available here . As matplotlib does not directly support colormaps for line-based plots, the colors are selected based on an even spacing determined by the number of columns in the DataFrame . There is no consideration made for background color, so some colormaps will produce lines that are not easily visible. To use the cubehelix colormap, we can pass colormap='cubehelix' . In [196]: np . random . seed ( 123456 ) In [197]: df = pd . DataFrame ( np . random . randn ( 1000 , 10 ), index = ts . index ) In [198]: df = df . cumsum () In [199]: plt . figure (); In [200]: df . plot ( colormap = \"cubehelix\" ); Alternatively, we can pass the colormap itself: In [201]: from matplotlib import cm In [202]: plt . figure (); In [203]: df . plot ( colormap = cm . cubehelix ); Colormaps can also be used other plot types, like bar charts: In [204]: np . random . seed ( 123456 ) In [205]: dd = pd . DataFrame ( np . random . randn ( 10 , 10 )) . map ( abs ) In [206]: dd = dd . cumsum () In [207]: plt . figure (); In [208]: dd . plot . bar ( colormap = \"Greens\" ); Parallel coordinates charts: In [209]: plt . figure (); In [210]: parallel_coordinates ( data , \"Name\" , colormap = \"gist_rainbow\" ); Andrews curves charts: In [211]: plt . figure (); In [212]: andrews_curves ( data , \"Name\" , colormap = \"winter\" ); Plotting directly with Matplotlib # In some situations it may still be preferable or necessary to prepare plots directly with matplotlib, for instance when a certain type of plot or customization is not (yet) supported by pandas. Series and DataFrame objects behave like arrays and can therefore be passed directly to matplotlib functions without explicit casts. pandas also automatically registers formatters and locators that recognize date indices, thereby extending date and time support to practically all plot types available in matplotlib. Although this formatting does not provide the same level of refinement you would get when plotting via pandas, it can be faster when plotting a large number of points. In [213]: np . random . seed ( 123456 ) In [214]: price = pd . Series ( .....: np . random . randn ( 150 ) . cumsum (), .....: index = pd . date_range ( \"2000-1-1\" , periods = 150 , freq = \"B\" ), .....: ) .....: In [215]: ma = price . rolling ( 20 ) . mean () In [216]: mstd = price . rolling ( 20 ) . std () In [217]: plt . figure (); In [218]: plt . plot ( price . index , price , \"k\" ); In [219]: plt . plot ( ma . index , ma , \"b\" ); In [220]: plt . fill_between ( mstd . index , ma - 2 * mstd , ma + 2 * mstd , color = \"b\" , alpha = 0.2 ); Plotting backends # pandas can be extended with third-party plotting backends. The main idea is letting users select a plotting backend different than the provided one based on Matplotlib. This can be done by passing âbackend.moduleâ as the argument backend in plot function. For example: >>> Series ([ 1 , 2 , 3 ]) . plot ( backend = \"backend.module\" ) Alternatively, you can also set this option globally, do you donât need to specify the keyword in each plot call. For example: >>> pd . set_option ( \"plotting.backend\" , \"backend.module\" ) >>> pd . Series ([ 1 , 2 , 3 ]) . plot () Or: >>> pd . options . plotting . backend = \"backend.module\" >>> pd . Series ([ 1 , 2 , 3 ]) . plot () This would be more or less equivalent to: >>> import backend.module >>> backend . module . plot ( pd . Series ([ 1 , 2 , 3 ])) The backend module can then use other visualization tools (Bokeh, Altair, hvplot,â¦) to generate the plots. Some libraries implementing a backend for pandas are listed on the ecosystem page . Developers guide can be found at https://pandas.pydata.org/docs/dev/development/extending.html#plotting-backends previous Nullable Boolean data type next Table Visualization On this page Basic plotting: plot Other plots Bar plots Histograms Box plots Area plot Scatter plot Hexagonal bin plot Pie plot Plotting with missing data Plotting tools Scatter matrix plot Density plot Andrews curves Parallel coordinates Lag plot Autocorrelation plot Bootstrap plot RadViz Plot formatting Setting the plot style General plot style arguments Controlling the legend Controlling the labels Scales Plotting on a secondary y-axis Custom formatters for timeseries plots Suppressing tick resolution adjustment Automatic date tick adjustment Subplots Using layout and targeting multiple axes Plotting with error bars Plotting tables Colormaps Plotting directly with Matplotlib Plotting backends Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/visualization.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Index objects Index objects # Index # Many of these methods or variants thereof are available on the objects that contain an index (Series/DataFrame) and those should most likely be used before calling these methods directly. Index ([data,Â dtype,Â copy,Â name,Â tupleize_cols]) Immutable sequence used for indexing and alignment. Properties # Index.values Return an array representing the data in the Index. Index.is_monotonic_increasing Return a boolean if the values are equal or increasing. Index.is_monotonic_decreasing Return a boolean if the values are equal or decreasing. Index.is_unique Return if the index has unique values. Index.has_duplicates Check if the Index has duplicate values. Index.hasnans Return True if there are any NaNs. Index.dtype Return the dtype object of the underlying data. Index.inferred_type Return a string of the type inferred from the values. Index.shape Return a tuple of the shape of the underlying data. Index.name Return Index or MultiIndex name. Index.names Index.nbytes Return the number of bytes in the underlying data. Index.ndim Number of dimensions of the underlying data, by definition 1. Index.size Return the number of elements in the underlying data. Index.empty Index.T Return the transpose, which is by definition self. Index.memory_usage ([deep]) Memory usage of the values. Modifying and computations # Index.all (*args,Â **kwargs) Return whether all elements are Truthy. Index.any (*args,Â **kwargs) Return whether any element is Truthy. Index.argmin ([axis,Â skipna]) Return int position of the smallest value in the Series. Index.argmax ([axis,Â skipna]) Return int position of the largest value in the Series. Index.copy ([name,Â deep]) Make a copy of this object. Index.delete (loc) Make new Index with passed location(-s) deleted. Index.drop (labels[,Â errors]) Make new Index with passed list of labels deleted. Index.drop_duplicates (*[,Â keep]) Return Index with duplicate values removed. Index.duplicated ([keep]) Indicate duplicate index values. Index.equals (other) Determine if two Index object are equal. Index.factorize ([sort,Â use_na_sentinel]) Encode the object as an enumerated type or categorical variable. Index.identical (other) Similar to equals, but checks that object attributes and types are also equal. Index.insert (loc,Â item) Make new Index inserting new item at location. Index.is_ (other) More flexible, faster check like is but that works through views. Index.is_boolean () (DEPRECATED) Check if the Index only consists of booleans. Index.is_categorical () (DEPRECATED) Check if the Index holds categorical data. Index.is_floating () (DEPRECATED) Check if the Index is a floating type. Index.is_integer () (DEPRECATED) Check if the Index only consists of integers. Index.is_interval () (DEPRECATED) Check if the Index holds Interval objects. Index.is_numeric () (DEPRECATED) Check if the Index only consists of numeric data. Index.is_object () (DEPRECATED) Check if the Index is of the object dtype. Index.min ([axis,Â skipna]) Return the minimum value of the Index. Index.max ([axis,Â skipna]) Return the maximum value of the Index. Index.reindex (target[,Â method,Â level,Â ...]) Create index with target's values. Index.rename (name,Â *[,Â inplace]) Alter Index or MultiIndex name. Index.repeat (repeats[,Â axis]) Repeat elements of a Index. Index.where (cond[,Â other]) Replace values where the condition is False. Index.take (indices[,Â axis,Â allow_fill,Â ...]) Return a new Index of the values selected by the indices. Index.putmask (mask,Â value) Return a new Index of the values set with the mask. Index.unique ([level]) Return unique values in the index. Index.nunique ([dropna]) Return number of unique elements in the object. Index.value_counts ([normalize,Â sort,Â ...]) Return a Series containing counts of unique values. Compatibility with MultiIndex # Index.set_names (names,Â *[,Â level,Â inplace]) Set Index or MultiIndex name. Index.droplevel ([level]) Return index with requested level(s) removed. Missing values # Index.fillna ([value,Â downcast]) Fill NA/NaN values with the specified value. Index.dropna ([how]) Return Index without NA/NaN values. Index.isna () Detect missing values. Index.notna () Detect existing (non-missing) values. Conversion # Index.astype (dtype[,Â copy]) Create an Index with values cast to dtypes. Index.item () Return the first element of the underlying data as a Python scalar. Index.map (mapper[,Â na_action]) Map values using an input mapping or function. Index.ravel ([order]) Return a view on self. Index.to_list () Return a list of the values. Index.to_series ([index,Â name]) Create a Series with both index and values equal to the index keys. Index.to_frame ([index,Â name]) Create a DataFrame with a column containing the Index. Index.view ([cls]) Sorting # Index.argsort (*args,Â **kwargs) Return the integer indices that would sort the index. Index.searchsorted (value[,Â side,Â sorter]) Find indices where elements should be inserted to maintain order. Index.sort_values (*[,Â return_indexer,Â ...]) Return a sorted copy of the index. Time-specific operations # Index.shift ([periods,Â freq]) Shift index by desired number of time frequency increments. Combining / joining / set operations # Index.append (other) Append a collection of Index options together. Index.join (other,Â *[,Â how,Â level,Â ...]) Compute join_index and indexers to conform data structures to the new index. Index.intersection (other[,Â sort]) Form the intersection of two Index objects. Index.union (other[,Â sort]) Form the union of two Index objects. Index.difference (other[,Â sort]) Return a new Index with elements of index not in other . Index.symmetric_difference (other[,Â ...]) Compute the symmetric difference of two Index objects. Selecting # Index.asof (label) Return the label from the index, or, if not present, the previous one. Index.asof_locs (where,Â mask) Return the locations (indices) of labels in the index. Index.get_indexer (target[,Â method,Â limit,Â ...]) Compute indexer and mask for new index given the current index. Index.get_indexer_for (target) Guaranteed return of an indexer even when non-unique. Index.get_indexer_non_unique (target) Compute indexer and mask for new index given the current index. Index.get_level_values (level) Return an Index of values for requested level. Index.get_loc (key) Get integer location, slice or boolean mask for requested label. Index.get_slice_bound (label,Â side) Calculate slice bound that corresponds to given label. Index.isin (values[,Â level]) Return a boolean array where the index values are in values . Index.slice_indexer ([start,Â end,Â step]) Compute the slice indexer for input labels and step. Index.slice_locs ([start,Â end,Â step]) Compute slice locations for input labels. Numeric Index # RangeIndex ([start,Â stop,Â step,Â dtype,Â copy,Â ...]) Immutable Index implementing a monotonic integer range. RangeIndex.start The value of the start parameter ( 0 if this was not supplied). RangeIndex.stop The value of the stop parameter. RangeIndex.step The value of the step parameter ( 1 if this was not supplied). RangeIndex.from_range (data[,Â name,Â dtype]) Create pandas.RangeIndex from a range object. CategoricalIndex # CategoricalIndex ([data,Â categories,Â ...]) Index based on an underlying Categorical . Categorical components # CategoricalIndex.codes The category codes of this categorical index. CategoricalIndex.categories The categories of this categorical. CategoricalIndex.ordered Whether the categories have an ordered relationship. CategoricalIndex.rename_categories (*args,Â ...) Rename categories. CategoricalIndex.reorder_categories (*args,Â ...) Reorder categories as specified in new_categories. CategoricalIndex.add_categories (*args,Â **kwargs) Add new categories. CategoricalIndex.remove_categories (*args,Â ...) Remove the specified categories. CategoricalIndex.remove_unused_categories (...) Remove categories which are not used. CategoricalIndex.set_categories (*args,Â **kwargs) Set the categories to the specified new categories. CategoricalIndex.as_ordered (*args,Â **kwargs) Set the Categorical to be ordered. CategoricalIndex.as_unordered (*args,Â **kwargs) Set the Categorical to be unordered. Modifying and computations # CategoricalIndex.map (mapper[,Â na_action]) Map values using input an input mapping or function. CategoricalIndex.equals (other) Determine if two CategoricalIndex objects contain the same elements. IntervalIndex # IntervalIndex (data[,Â closed,Â dtype,Â copy,Â ...]) Immutable index of intervals that are closed on the same side. IntervalIndex components # IntervalIndex.from_arrays (left,Â right[,Â ...]) Construct from two arrays defining the left and right bounds. IntervalIndex.from_tuples (data[,Â closed,Â ...]) Construct an IntervalIndex from an array-like of tuples. IntervalIndex.from_breaks (breaks[,Â closed,Â ...]) Construct an IntervalIndex from an array of splits. IntervalIndex.left IntervalIndex.right IntervalIndex.mid IntervalIndex.closed String describing the inclusive side the intervals. IntervalIndex.length IntervalIndex.values Return an array representing the data in the Index. IntervalIndex.is_empty Indicates if an interval is empty, meaning it contains no points. IntervalIndex.is_non_overlapping_monotonic Return a boolean whether the IntervalArray is non-overlapping and monotonic. IntervalIndex.is_overlapping Return True if the IntervalIndex has overlapping intervals, else False. IntervalIndex.get_loc (key) Get integer location, slice or boolean mask for requested label. IntervalIndex.get_indexer (target[,Â method,Â ...]) Compute indexer and mask for new index given the current index. IntervalIndex.set_closed (*args,Â **kwargs) Return an identical IntervalArray closed on the specified side. IntervalIndex.contains (*args,Â **kwargs) Check elementwise if the Intervals contain the value. IntervalIndex.overlaps (*args,Â **kwargs) Check elementwise if an Interval overlaps the values in the IntervalArray. IntervalIndex.to_tuples (*args,Â **kwargs) Return an ndarray (if self is IntervalArray) or Index (if self is IntervalIndex) of tuples of the form (left, right). MultiIndex # MultiIndex ([levels,Â codes,Â sortorder,Â ...]) A multi-level, or hierarchical, index object for pandas objects. MultiIndex constructors # MultiIndex.from_arrays (arrays[,Â sortorder,Â ...]) Convert arrays to MultiIndex. MultiIndex.from_tuples (tuples[,Â sortorder,Â ...]) Convert list of tuples to MultiIndex. MultiIndex.from_product (iterables[,Â ...]) Make a MultiIndex from the cartesian product of multiple iterables. MultiIndex.from_frame (df[,Â sortorder,Â names]) Make a MultiIndex from a DataFrame. MultiIndex properties # MultiIndex.names Names of levels in MultiIndex. MultiIndex.levels Levels of the MultiIndex. MultiIndex.codes MultiIndex.nlevels Integer number of levels in this MultiIndex. MultiIndex.levshape A tuple with the length of each level. MultiIndex.dtypes Return the dtypes as a Series for the underlying MultiIndex. MultiIndex components # MultiIndex.set_levels (levels,Â *[,Â level,Â ...]) Set new levels on MultiIndex. MultiIndex.set_codes (codes,Â *[,Â level,Â ...]) Set new codes on MultiIndex. MultiIndex.to_flat_index () Convert a MultiIndex to an Index of Tuples containing the level values. MultiIndex.to_frame ([index,Â name,Â ...]) Create a DataFrame with the levels of the MultiIndex as columns. MultiIndex.sortlevel ([level,Â ascending,Â ...]) Sort MultiIndex at the requested level. MultiIndex.droplevel ([level]) Return index with requested level(s) removed. MultiIndex.swaplevel ([i,Â j]) Swap level i with level j. MultiIndex.reorder_levels (order) Rearrange levels using input order. MultiIndex.remove_unused_levels () Create new MultiIndex from current that removes unused levels. MultiIndex.drop (codes[,Â level,Â errors]) Make a new pandas.MultiIndex with the passed list of codes deleted. MultiIndex.copy ([names,Â deep,Â name]) Make a copy of this object. MultiIndex.append (other) Append a collection of Index options together. MultiIndex.truncate ([before,Â after]) Slice index between two labels / tuples, return new MultiIndex. MultiIndex selecting # MultiIndex.get_loc (key) Get location for a label or a tuple of labels. MultiIndex.get_locs (seq) Get location for a sequence of labels. MultiIndex.get_loc_level (key[,Â level,Â ...]) Get location and sliced index for requested label(s)/level(s). MultiIndex.get_indexer (target[,Â method,Â ...]) Compute indexer and mask for new index given the current index. MultiIndex.get_level_values (level) Return vector of label values for requested level. IndexSlice Create an object to more easily perform multi-index slicing. DatetimeIndex # DatetimeIndex ([data,Â freq,Â tz,Â normalize,Â ...]) Immutable ndarray-like of datetime64 data. Time/date components # DatetimeIndex.year The year of the datetime. DatetimeIndex.month The month as January=1, December=12. DatetimeIndex.day The day of the datetime. DatetimeIndex.hour The hours of the datetime. DatetimeIndex.minute The minutes of the datetime. DatetimeIndex.second The seconds of the datetime. DatetimeIndex.microsecond The microseconds of the datetime. DatetimeIndex.nanosecond The nanoseconds of the datetime. DatetimeIndex.date Returns numpy array of python datetime.date objects. DatetimeIndex.time Returns numpy array of datetime.time objects. DatetimeIndex.timetz Returns numpy array of datetime.time objects with timezones. DatetimeIndex.dayofyear The ordinal day of the year. DatetimeIndex.day_of_year The ordinal day of the year. DatetimeIndex.dayofweek The day of the week with Monday=0, Sunday=6. DatetimeIndex.day_of_week The day of the week with Monday=0, Sunday=6. DatetimeIndex.weekday The day of the week with Monday=0, Sunday=6. DatetimeIndex.quarter The quarter of the date. DatetimeIndex.tz Return the timezone. DatetimeIndex.freq DatetimeIndex.freqstr Return the frequency object as a string if it's set, otherwise None. DatetimeIndex.is_month_start Indicates whether the date is the first day of the month. DatetimeIndex.is_month_end Indicates whether the date is the last day of the month. DatetimeIndex.is_quarter_start Indicator for whether the date is the first day of a quarter. DatetimeIndex.is_quarter_end Indicator for whether the date is the last day of a quarter. DatetimeIndex.is_year_start Indicate whether the date is the first day of a year. DatetimeIndex.is_year_end Indicate whether the date is the last day of the year. DatetimeIndex.is_leap_year Boolean indicator if the date belongs to a leap year. DatetimeIndex.inferred_freq Tries to return a string representing a frequency generated by infer_freq. Selecting # DatetimeIndex.indexer_at_time (time[,Â asof]) Return index locations of values at particular time of day. DatetimeIndex.indexer_between_time (...[,Â ...]) Return index locations of values between particular times of day. Time-specific operations # DatetimeIndex.normalize (*args,Â **kwargs) Convert times to midnight. DatetimeIndex.strftime (date_format) Convert to Index using specified date_format. DatetimeIndex.snap ([freq]) Snap time stamps to nearest occurring frequency. DatetimeIndex.tz_convert (tz) Convert tz-aware Datetime Array/Index from one time zone to another. DatetimeIndex.tz_localize (tz[,Â ambiguous,Â ...]) Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index. DatetimeIndex.round (*args,Â **kwargs) Perform round operation on the data to the specified freq . DatetimeIndex.floor (*args,Â **kwargs) Perform floor operation on the data to the specified freq . DatetimeIndex.ceil (*args,Â **kwargs) Perform ceil operation on the data to the specified freq . DatetimeIndex.month_name (*args,Â **kwargs) Return the month names with specified locale. DatetimeIndex.day_name (*args,Â **kwargs) Return the day names with specified locale. Conversion # DatetimeIndex.as_unit (*args,Â **kwargs) Convert to a dtype with the given unit resolution. DatetimeIndex.to_period (*args,Â **kwargs) Cast to PeriodArray/PeriodIndex at a particular frequency. DatetimeIndex.to_pydatetime (*args,Â **kwargs) Return an ndarray of datetime.datetime objects. DatetimeIndex.to_series ([index,Â name]) Create a Series with both index and values equal to the index keys. DatetimeIndex.to_frame ([index,Â name]) Create a DataFrame with a column containing the Index. Methods # DatetimeIndex.mean (*[,Â skipna,Â axis]) Return the mean value of the Array. DatetimeIndex.std (*args,Â **kwargs) Return sample standard deviation over requested axis. TimedeltaIndex # TimedeltaIndex ([data,Â unit,Â freq,Â closed,Â ...]) Immutable Index of timedelta64 data. Components # TimedeltaIndex.days Number of days for each element. TimedeltaIndex.seconds Number of seconds (>= 0 and less than 1 day) for each element. TimedeltaIndex.microseconds Number of microseconds (>= 0 and less than 1 second) for each element. TimedeltaIndex.nanoseconds Number of nanoseconds (>= 0 and less than 1 microsecond) for each element. TimedeltaIndex.components Return a DataFrame of the individual resolution components of the Timedeltas. TimedeltaIndex.inferred_freq Tries to return a string representing a frequency generated by infer_freq. Conversion # TimedeltaIndex.as_unit (unit) Convert to a dtype with the given unit resolution. TimedeltaIndex.to_pytimedelta (*args,Â **kwargs) Return an ndarray of datetime.timedelta objects. TimedeltaIndex.to_series ([index,Â name]) Create a Series with both index and values equal to the index keys. TimedeltaIndex.round (*args,Â **kwargs) Perform round operation on the data to the specified freq . TimedeltaIndex.floor (*args,Â **kwargs) Perform floor operation on the data to the specified freq . TimedeltaIndex.ceil (*args,Â **kwargs) Perform ceil operation on the data to the specified freq . TimedeltaIndex.to_frame ([index,Â name]) Create a DataFrame with a column containing the Index. Methods # TimedeltaIndex.mean (*[,Â skipna,Â axis]) Return the mean value of the Array. PeriodIndex # PeriodIndex ([data,Â ordinal,Â freq,Â dtype,Â ...]) Immutable ndarray holding ordinal values indicating regular periods in time. Properties # PeriodIndex.day The days of the period. PeriodIndex.dayofweek The day of the week with Monday=0, Sunday=6. PeriodIndex.day_of_week The day of the week with Monday=0, Sunday=6. PeriodIndex.dayofyear The ordinal day of the year. PeriodIndex.day_of_year The ordinal day of the year. PeriodIndex.days_in_month The number of days in the month. PeriodIndex.daysinmonth The number of days in the month. PeriodIndex.end_time Get the Timestamp for the end of the period. PeriodIndex.freq PeriodIndex.freqstr Return the frequency object as a string if it's set, otherwise None. PeriodIndex.hour The hour of the period. PeriodIndex.is_leap_year Logical indicating if the date belongs to a leap year. PeriodIndex.minute The minute of the period. PeriodIndex.month The month as January=1, December=12. PeriodIndex.quarter The quarter of the date. PeriodIndex.qyear PeriodIndex.second The second of the period. PeriodIndex.start_time Get the Timestamp for the start of the period. PeriodIndex.week The week ordinal of the year. PeriodIndex.weekday The day of the week with Monday=0, Sunday=6. PeriodIndex.weekofyear The week ordinal of the year. PeriodIndex.year The year of the period. Methods # PeriodIndex.asfreq ([freq,Â how]) Convert the PeriodArray to the specified frequency freq . PeriodIndex.strftime (*args,Â **kwargs) Convert to Index using specified date_format. PeriodIndex.to_timestamp ([freq,Â how]) Cast to DatetimeArray/Index. PeriodIndex.from_fields (*[,Â year,Â quarter,Â ...]) PeriodIndex.from_ordinals (ordinals,Â *,Â freq) previous pandas.api.types.is_scalar next pandas.Index On this page Index Properties Modifying and computations Compatibility with MultiIndex Missing values Conversion Sorting Time-specific operations Combining / joining / set operations Selecting Numeric Index CategoricalIndex Categorical components Modifying and computations IntervalIndex IntervalIndex components MultiIndex MultiIndex constructors MultiIndex properties MultiIndex components MultiIndex selecting DatetimeIndex Time/date components Selecting Time-specific operations Conversion Methods TimedeltaIndex Components Conversion Methods PeriodIndex Properties Methods Show Source",
    "url": "https://pandas.pydata.org/docs/reference/indexing.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference API reference # This page gives an overview of all public pandas objects, functions and methods. All classes and functions exposed in pandas.* namespace are public. The following subpackages are public. pandas.errors : Custom exception and warnings classes that are raised by pandas. pandas.plotting : Plotting public API. pandas.testing : Functions that are useful for writing tests involving pandas objects. pandas.api.extensions : Functions and classes for extending pandas objects. pandas.api.indexers : Functions and classes for rolling window indexers. pandas.api.interchange : DataFrame interchange protocol. pandas.api.types : Datatype classes and functions. pandas.api.typing : Classes that may be necessary for type-hinting. These are classes that are encountered as intermediate results but should not be instantiated directly by users. These classes are not to be confused with classes from the pandas-stubs package which has classes in addition to those that occur in pandas for type-hinting. In addition, public functions in pandas.io and pandas.tseries submodules are mentioned in the documentation. Warning The pandas.core , pandas.compat , and pandas.util top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed. Input/output Pickling Flat file Clipboard Excel JSON HTML XML Latex HDFStore: PyTables (HDF5) Feather Parquet ORC SAS SPSS SQL Google BigQuery STATA General functions Data manipulations Top-level missing data Top-level dealing with numeric data Top-level dealing with datetimelike data Top-level dealing with Interval data Top-level evaluation Datetime formats Hashing Importing from other DataFrame libraries Series Constructor Attributes Conversion Indexing, iteration Binary operator functions Function application, GroupBy & window Computations / descriptive stats Reindexing / selection / label manipulation Missing data handling Reshaping, sorting Combining / comparing / joining / merging Time Series-related Accessors Plotting Serialization / IO / conversion DataFrame Constructor Attributes and underlying data Conversion Indexing, iteration Binary operator functions Function application, GroupBy & window Computations / descriptive stats Reindexing / selection / label manipulation Missing data handling Reshaping, sorting, transposing Combining / comparing / joining / merging Time Series-related Flags Metadata Plotting Sparse accessor Serialization / IO / conversion pandas arrays, scalars, and data types Objects Utilities Index objects Index Numeric Index CategoricalIndex IntervalIndex MultiIndex DatetimeIndex TimedeltaIndex PeriodIndex Date offsets DateOffset BusinessDay BusinessHour CustomBusinessDay CustomBusinessHour MonthEnd MonthBegin BusinessMonthEnd BusinessMonthBegin CustomBusinessMonthEnd CustomBusinessMonthBegin SemiMonthEnd SemiMonthBegin Week WeekOfMonth LastWeekOfMonth BQuarterEnd BQuarterBegin QuarterEnd QuarterBegin BYearEnd BYearBegin YearEnd YearBegin FY5253 FY5253Quarter Easter Tick Day Hour Minute Second Milli Micro Nano Frequencies pandas.tseries.frequencies.to_offset Window Rolling window functions Weighted window functions Expanding window functions Exponentially-weighted window functions Window indexer GroupBy Indexing, iteration Function application helper Function application DataFrameGroupBy computations / descriptive stats SeriesGroupBy computations / descriptive stats Plotting and visualization Resampling Indexing, iteration Function application Upsampling Computations / descriptive stats Style Styler constructor Styler properties Style application Builtin styles Style export and import Plotting pandas.plotting.andrews_curves pandas.plotting.autocorrelation_plot pandas.plotting.bootstrap_plot pandas.plotting.boxplot pandas.plotting.deregister_matplotlib_converters pandas.plotting.lag_plot pandas.plotting.parallel_coordinates pandas.plotting.plot_params pandas.plotting.radviz pandas.plotting.register_matplotlib_converters pandas.plotting.scatter_matrix pandas.plotting.table Options and settings Working with options Numeric formatting Extensions pandas.api.extensions.register_extension_dtype pandas.api.extensions.register_dataframe_accessor pandas.api.extensions.register_series_accessor pandas.api.extensions.register_index_accessor pandas.api.extensions.ExtensionDtype pandas.api.extensions.ExtensionArray pandas.arrays.NumpyExtensionArray pandas.api.indexers.check_array_indexer Testing Assertion functions Exceptions and warnings Bug report function Test suite runner Missing values pandas.NA pandas.NaT previous Cookbook next Input/output Show Source",
    "url": "https://pandas.pydata.org/docs/reference/index.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting Plotting # The following functions are contained in the pandas.plotting module. andrews_curves (frame,Â class_column[,Â ax,Â ...]) Generate a matplotlib plot for visualizing clusters of multivariate data. autocorrelation_plot (series[,Â ax]) Autocorrelation plot for time series. bootstrap_plot (series[,Â fig,Â size,Â samples]) Bootstrap plot on mean, median and mid-range statistics. boxplot (data[,Â column,Â by,Â ax,Â fontsize,Â ...]) Make a box plot from DataFrame columns. deregister_matplotlib_converters () Remove pandas formatters and converters. lag_plot (series[,Â lag,Â ax]) Lag plot for time series. parallel_coordinates (frame,Â class_column[,Â ...]) Parallel coordinates plotting. plot_params Stores pandas plotting options. radviz (frame,Â class_column[,Â ax,Â color,Â ...]) Plot a multidimensional dataset in 2D. register_matplotlib_converters () Register pandas formatters and converters with matplotlib. scatter_matrix (frame[,Â alpha,Â figsize,Â ax,Â ...]) Draw a matrix of scatter plots. table (ax,Â data,Â **kwargs) Helper function to convert DataFrame and Series to matplotlib.table. previous pandas.io.formats.style.Styler.use next pandas.plotting.andrews_curves Show Source",
    "url": "https://pandas.pydata.org/docs/reference/plotting.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Series Series # Constructor # Series ([data,Â index,Â dtype,Â name,Â copy,Â ...]) One-dimensional ndarray with axis labels (including time series). Attributes # Axes Series.index The index (axis labels) of the Series. Series.array The ExtensionArray of the data backing this Series or Index. Series.values Return Series as ndarray or ndarray-like depending on the dtype. Series.dtype Return the dtype object of the underlying data. Series.shape Return a tuple of the shape of the underlying data. Series.nbytes Return the number of bytes in the underlying data. Series.ndim Number of dimensions of the underlying data, by definition 1. Series.size Return the number of elements in the underlying data. Series.T Return the transpose, which is by definition self. Series.memory_usage ([index,Â deep]) Return the memory usage of the Series. Series.hasnans Return True if there are any NaNs. Series.empty Indicator whether Series/DataFrame is empty. Series.dtypes Return the dtype object of the underlying data. Series.name Return the name of the Series. Series.flags Get the properties associated with this pandas object. Series.set_flags (*[,Â copy,Â ...]) Return a new object with updated flags. Conversion # Series.astype (dtype[,Â copy,Â errors]) Cast a pandas object to a specified dtype dtype . Series.convert_dtypes ([infer_objects,Â ...]) Convert columns to the best possible dtypes using dtypes supporting pd.NA . Series.infer_objects ([copy]) Attempt to infer better dtypes for object columns. Series.copy ([deep]) Make a copy of this object's indices and data. Series.bool () (DEPRECATED) Return the bool of a single element Series or DataFrame. Series.to_numpy ([dtype,Â copy,Â na_value]) A NumPy ndarray representing the values in this Series or Index. Series.to_period ([freq,Â copy]) Convert Series from DatetimeIndex to PeriodIndex. Series.to_timestamp ([freq,Â how,Â copy]) Cast to DatetimeIndex of Timestamps, at beginning of period. Series.to_list () Return a list of the values. Series.__array__ ([dtype,Â copy]) Return the values as a NumPy array. Indexing, iteration # Series.get (key[,Â default]) Get item from object for given key (ex: DataFrame column). Series.at Access a single value for a row/column label pair. Series.iat Access a single value for a row/column pair by integer position. Series.loc Access a group of rows and columns by label(s) or a boolean array. Series.iloc (DEPRECATED) Purely integer-location based indexing for selection by position. Series.__iter__ () Return an iterator of the values. Series.items () Lazily iterate over (index, value) tuples. Series.keys () Return alias for index. Series.pop (item) Return item and drops from series. Series.item () Return the first element of the underlying data as a Python scalar. Series.xs (key[,Â axis,Â level,Â drop_level]) Return cross-section from the Series/DataFrame. For more information on .at , .iat , .loc , and .iloc , see the indexing documentation . Binary operator functions # Series.add (other[,Â level,Â fill_value,Â axis]) Return Addition of series and other, element-wise (binary operator add ). Series.sub (other[,Â level,Â fill_value,Â axis]) Return Subtraction of series and other, element-wise (binary operator sub ). Series.mul (other[,Â level,Â fill_value,Â axis]) Return Multiplication of series and other, element-wise (binary operator mul ). Series.div (other[,Â level,Â fill_value,Â axis]) Return Floating division of series and other, element-wise (binary operator truediv ). Series.truediv (other[,Â level,Â fill_value,Â axis]) Return Floating division of series and other, element-wise (binary operator truediv ). Series.floordiv (other[,Â level,Â fill_value,Â axis]) Return Integer division of series and other, element-wise (binary operator floordiv ). Series.mod (other[,Â level,Â fill_value,Â axis]) Return Modulo of series and other, element-wise (binary operator mod ). Series.pow (other[,Â level,Â fill_value,Â axis]) Return Exponential power of series and other, element-wise (binary operator pow ). Series.radd (other[,Â level,Â fill_value,Â axis]) Return Addition of series and other, element-wise (binary operator radd ). Series.rsub (other[,Â level,Â fill_value,Â axis]) Return Subtraction of series and other, element-wise (binary operator rsub ). Series.rmul (other[,Â level,Â fill_value,Â axis]) Return Multiplication of series and other, element-wise (binary operator rmul ). Series.rdiv (other[,Â level,Â fill_value,Â axis]) Return Floating division of series and other, element-wise (binary operator rtruediv ). Series.rtruediv (other[,Â level,Â fill_value,Â axis]) Return Floating division of series and other, element-wise (binary operator rtruediv ). Series.rfloordiv (other[,Â level,Â fill_value,Â ...]) Return Integer division of series and other, element-wise (binary operator rfloordiv ). Series.rmod (other[,Â level,Â fill_value,Â axis]) Return Modulo of series and other, element-wise (binary operator rmod ). Series.rpow (other[,Â level,Â fill_value,Â axis]) Return Exponential power of series and other, element-wise (binary operator rpow ). Series.combine (other,Â func[,Â fill_value]) Combine the Series with a Series or scalar according to func . Series.combine_first (other) Update null elements with value in the same location in 'other'. Series.round ([decimals]) Round each value in a Series to the given number of decimals. Series.lt (other[,Â level,Â fill_value,Â axis]) Return Less than of series and other, element-wise (binary operator lt ). Series.gt (other[,Â level,Â fill_value,Â axis]) Return Greater than of series and other, element-wise (binary operator gt ). Series.le (other[,Â level,Â fill_value,Â axis]) Return Less than or equal to of series and other, element-wise (binary operator le ). Series.ge (other[,Â level,Â fill_value,Â axis]) Return Greater than or equal to of series and other, element-wise (binary operator ge ). Series.ne (other[,Â level,Â fill_value,Â axis]) Return Not equal to of series and other, element-wise (binary operator ne ). Series.eq (other[,Â level,Â fill_value,Â axis]) Return Equal to of series and other, element-wise (binary operator eq ). Series.product ([axis,Â skipna,Â numeric_only,Â ...]) Return the product of the values over the requested axis. Series.dot (other) Compute the dot product between the Series and the columns of other. Function application, GroupBy & window # Series.apply (func[,Â convert_dtype,Â args,Â by_row]) Invoke function on values of Series. Series.agg ([func,Â axis]) Aggregate using one or more operations over the specified axis. Series.aggregate ([func,Â axis]) Aggregate using one or more operations over the specified axis. Series.transform (func[,Â axis]) Call func on self producing a Series with the same axis shape as self. Series.map (arg[,Â na_action]) Map values of Series according to an input mapping or function. Series.groupby ([by,Â axis,Â level,Â as_index,Â ...]) Group Series using a mapper or by a Series of columns. Series.rolling (window[,Â min_periods,Â ...]) Provide rolling window calculations. Series.expanding ([min_periods,Â axis,Â method]) Provide expanding window calculations. Series.ewm ([com,Â span,Â halflife,Â alpha,Â ...]) Provide exponentially weighted (EW) calculations. Series.pipe (func,Â *args,Â **kwargs) Apply chainable functions that expect Series or DataFrames. Computations / descriptive stats # Series.abs () Return a Series/DataFrame with absolute numeric value of each element. Series.all ([axis,Â bool_only,Â skipna]) Return whether all elements are True, potentially over an axis. Series.any (*[,Â axis,Â bool_only,Â skipna]) Return whether any element is True, potentially over an axis. Series.autocorr ([lag]) Compute the lag-N autocorrelation. Series.between (left,Â right[,Â inclusive]) Return boolean Series equivalent to left <= series <= right. Series.clip ([lower,Â upper,Â axis,Â inplace]) Trim values at input threshold(s). Series.corr (other[,Â method,Â min_periods]) Compute correlation with other Series, excluding missing values. Series.count () Return number of non-NA/null observations in the Series. Series.cov (other[,Â min_periods,Â ddof]) Compute covariance with Series, excluding missing values. Series.cummax ([axis,Â skipna]) Return cumulative maximum over a DataFrame or Series axis. Series.cummin ([axis,Â skipna]) Return cumulative minimum over a DataFrame or Series axis. Series.cumprod ([axis,Â skipna]) Return cumulative product over a DataFrame or Series axis. Series.cumsum ([axis,Â skipna]) Return cumulative sum over a DataFrame or Series axis. Series.describe ([percentiles,Â include,Â exclude]) Generate descriptive statistics. Series.diff ([periods]) First discrete difference of element. Series.factorize ([sort,Â use_na_sentinel]) Encode the object as an enumerated type or categorical variable. Series.kurt ([axis,Â skipna,Â numeric_only]) Return unbiased kurtosis over requested axis. Series.max ([axis,Â skipna,Â numeric_only]) Return the maximum of the values over the requested axis. Series.mean ([axis,Â skipna,Â numeric_only]) Return the mean of the values over the requested axis. Series.median ([axis,Â skipna,Â numeric_only]) Return the median of the values over the requested axis. Series.min ([axis,Â skipna,Â numeric_only]) Return the minimum of the values over the requested axis. Series.mode ([dropna]) Return the mode(s) of the Series. Series.nlargest ([n,Â keep]) Return the largest n elements. Series.nsmallest ([n,Â keep]) Return the smallest n elements. Series.pct_change ([periods,Â fill_method,Â ...]) Fractional change between the current and a prior element. Series.prod ([axis,Â skipna,Â numeric_only,Â ...]) Return the product of the values over the requested axis. Series.quantile ([q,Â interpolation]) Return value at the given quantile. Series.rank ([axis,Â method,Â numeric_only,Â ...]) Compute numerical data ranks (1 through n) along axis. Series.sem ([axis,Â skipna,Â ddof,Â numeric_only]) Return unbiased standard error of the mean over requested axis. Series.skew ([axis,Â skipna,Â numeric_only]) Return unbiased skew over requested axis. Series.std ([axis,Â skipna,Â ddof,Â numeric_only]) Return sample standard deviation over requested axis. Series.sum ([axis,Â skipna,Â numeric_only,Â ...]) Return the sum of the values over the requested axis. Series.var ([axis,Â skipna,Â ddof,Â numeric_only]) Return unbiased variance over requested axis. Series.kurtosis ([axis,Â skipna,Â numeric_only]) Return unbiased kurtosis over requested axis. Series.unique () Return unique values of Series object. Series.nunique ([dropna]) Return number of unique elements in the object. Series.is_unique Return boolean if values in the object are unique. Series.is_monotonic_increasing Return boolean if values in the object are monotonically increasing. Series.is_monotonic_decreasing Return boolean if values in the object are monotonically decreasing. Series.value_counts ([normalize,Â sort,Â ...]) Return a Series containing counts of unique values. Reindexing / selection / label manipulation # Series.align (other[,Â join,Â axis,Â level,Â ...]) Align two objects on their axes with the specified join method. Series.case_when (caselist) Replace values where the conditions are True. Series.drop ([labels,Â axis,Â index,Â columns,Â ...]) Return Series with specified index labels removed. Series.droplevel (level[,Â axis]) Return Series/DataFrame with requested index / column level(s) removed. Series.drop_duplicates (*[,Â keep,Â inplace,Â ...]) Return Series with duplicate values removed. Series.duplicated ([keep]) Indicate duplicate Series values. Series.equals (other) Test whether two objects contain the same elements. Series.first (offset) (DEPRECATED) Select initial periods of time series data based on a date offset. Series.head ([n]) Return the first n rows. Series.idxmax ([axis,Â skipna]) Return the row label of the maximum value. Series.idxmin ([axis,Â skipna]) Return the row label of the minimum value. Series.isin (values) Whether elements in Series are contained in values . Series.last (offset) (DEPRECATED) Select final periods of time series data based on a date offset. Series.reindex ([index,Â axis,Â method,Â copy,Â ...]) Conform Series to new index with optional filling logic. Series.reindex_like (other[,Â method,Â copy,Â ...]) Return an object with matching indices as other object. Series.rename ([index,Â axis,Â copy,Â inplace,Â ...]) Alter Series index labels or name. Series.rename_axis ([mapper,Â index,Â axis,Â ...]) Set the name of the axis for the index or columns. Series.reset_index ([level,Â drop,Â name,Â ...]) Generate a new DataFrame or Series with the index reset. Series.sample ([n,Â frac,Â replace,Â weights,Â ...]) Return a random sample of items from an axis of object. Series.set_axis (labels,Â *[,Â axis,Â copy]) Assign desired index to given axis. Series.take (indices[,Â axis]) Return the elements in the given positional indices along an axis. Series.tail ([n]) Return the last n rows. Series.truncate ([before,Â after,Â axis,Â copy]) Truncate a Series or DataFrame before and after some index value. Series.where (cond[,Â other,Â inplace,Â axis,Â level]) Replace values where the condition is False. Series.mask (cond[,Â other,Â inplace,Â axis,Â level]) Replace values where the condition is True. Series.add_prefix (prefix[,Â axis]) Prefix labels with string prefix . Series.add_suffix (suffix[,Â axis]) Suffix labels with string suffix . Series.filter ([items,Â like,Â regex,Â axis]) Subset the dataframe rows or columns according to the specified index labels. Missing data handling # Series.backfill (*[,Â axis,Â inplace,Â limit,Â ...]) (DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap. Series.bfill (*[,Â axis,Â inplace,Â limit,Â ...]) Fill NA/NaN values by using the next valid observation to fill the gap. Series.dropna (*[,Â axis,Â inplace,Â how,Â ...]) Return a new Series with missing values removed. Series.ffill (*[,Â axis,Â inplace,Â limit,Â ...]) Fill NA/NaN values by propagating the last valid observation to next valid. Series.fillna ([value,Â method,Â axis,Â ...]) Fill NA/NaN values using the specified method. Series.interpolate ([method,Â axis,Â limit,Â ...]) Fill NaN values using an interpolation method. Series.isna () Detect missing values. Series.isnull () Series.isnull is an alias for Series.isna. Series.notna () Detect existing (non-missing) values. Series.notnull () Series.notnull is an alias for Series.notna. Series.pad (*[,Â axis,Â inplace,Â limit,Â downcast]) (DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid. Series.replace ([to_replace,Â value,Â inplace,Â ...]) Replace values given in to_replace with value . Reshaping, sorting # Series.argsort ([axis,Â kind,Â order,Â stable]) Return the integer indices that would sort the Series values. Series.argmin ([axis,Â skipna]) Return int position of the smallest value in the Series. Series.argmax ([axis,Â skipna]) Return int position of the largest value in the Series. Series.reorder_levels (order) Rearrange index levels using input order. Series.sort_values (*[,Â axis,Â ascending,Â ...]) Sort by the values. Series.sort_index (*[,Â axis,Â level,Â ...]) Sort Series by index labels. Series.swaplevel ([i,Â j,Â copy]) Swap levels i and j in a MultiIndex . Series.unstack ([level,Â fill_value,Â sort]) Unstack, also known as pivot, Series with MultiIndex to produce DataFrame. Series.explode ([ignore_index]) Transform each element of a list-like to a row. Series.searchsorted (value[,Â side,Â sorter]) Find indices where elements should be inserted to maintain order. Series.ravel ([order]) (DEPRECATED) Return the flattened underlying data as an ndarray or ExtensionArray. Series.repeat (repeats[,Â axis]) Repeat elements of a Series. Series.squeeze ([axis]) Squeeze 1 dimensional axis objects into scalars. Series.view ([dtype]) (DEPRECATED) Create a new view of the Series. Combining / comparing / joining / merging # Series.compare (other[,Â align_axis,Â ...]) Compare to another Series and show the differences. Series.update (other) Modify Series in place using values from passed Series. Time Series-related # Series.asfreq (freq[,Â method,Â how,Â ...]) Convert time series to specified frequency. Series.asof (where[,Â subset]) Return the last row(s) without any NaNs before where . Series.shift ([periods,Â freq,Â axis,Â ...]) Shift index by desired number of periods with an optional time freq . Series.first_valid_index () Return index for first non-NA value or None, if no non-NA value is found. Series.last_valid_index () Return index for last non-NA value or None, if no non-NA value is found. Series.resample (rule[,Â axis,Â closed,Â label,Â ...]) Resample time-series data. Series.tz_convert (tz[,Â axis,Â level,Â copy]) Convert tz-aware axis to target time zone. Series.tz_localize (tz[,Â axis,Â level,Â copy,Â ...]) Localize tz-naive index of a Series or DataFrame to target time zone. Series.at_time (time[,Â asof,Â axis]) Select values at particular time of day (e.g., 9:30AM). Series.between_time (start_time,Â end_time[,Â ...]) Select values between particular times of the day (e.g., 9:00-9:30 AM). Accessors # pandas provides dtype-specific methods under various accessors. These are separate namespaces within Series that only apply to specific data types. Series.str alias of StringMethods Series.cat alias of CategoricalAccessor Series.dt alias of CombinedDatetimelikeProperties Series.sparse alias of SparseAccessor DataFrame.sparse alias of SparseFrameAccessor Index.str alias of StringMethods Data Type Accessor Datetime, Timedelta, Period dt String str Categorical cat Sparse sparse Datetimelike properties # Series.dt can be used to access the values of the series as datetimelike and return several properties. These can be accessed like Series.dt.<property> . Datetime properties # Series.dt.date Returns numpy array of python datetime.date objects. Series.dt.time Returns numpy array of datetime.time objects. Series.dt.timetz Returns numpy array of datetime.time objects with timezones. Series.dt.year The year of the datetime. Series.dt.month The month as January=1, December=12. Series.dt.day The day of the datetime. Series.dt.hour The hours of the datetime. Series.dt.minute The minutes of the datetime. Series.dt.second The seconds of the datetime. Series.dt.microsecond The microseconds of the datetime. Series.dt.nanosecond The nanoseconds of the datetime. Series.dt.dayofweek The day of the week with Monday=0, Sunday=6. Series.dt.day_of_week The day of the week with Monday=0, Sunday=6. Series.dt.weekday The day of the week with Monday=0, Sunday=6. Series.dt.dayofyear The ordinal day of the year. Series.dt.day_of_year The ordinal day of the year. Series.dt.days_in_month The number of days in the month. Series.dt.quarter The quarter of the date. Series.dt.is_month_start Indicates whether the date is the first day of the month. Series.dt.is_month_end Indicates whether the date is the last day of the month. Series.dt.is_quarter_start Indicator for whether the date is the first day of a quarter. Series.dt.is_quarter_end Indicator for whether the date is the last day of a quarter. Series.dt.is_year_start Indicate whether the date is the first day of a year. Series.dt.is_year_end Indicate whether the date is the last day of the year. Series.dt.is_leap_year Boolean indicator if the date belongs to a leap year. Series.dt.daysinmonth The number of days in the month. Series.dt.days_in_month The number of days in the month. Series.dt.tz Return the timezone. Series.dt.freq Return the frequency object for this PeriodArray. Series.dt.unit Datetime methods # Series.dt.isocalendar () Calculate year, week, and day according to the ISO 8601 standard. Series.dt.to_period (*args,Â **kwargs) Cast to PeriodArray/PeriodIndex at a particular frequency. Series.dt.to_pydatetime () (DEPRECATED) Return the data as an array of datetime.datetime objects. Series.dt.tz_localize (*args,Â **kwargs) Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index. Series.dt.tz_convert (*args,Â **kwargs) Convert tz-aware Datetime Array/Index from one time zone to another. Series.dt.normalize (*args,Â **kwargs) Convert times to midnight. Series.dt.strftime (*args,Â **kwargs) Convert to Index using specified date_format. Series.dt.round (*args,Â **kwargs) Perform round operation on the data to the specified freq . Series.dt.floor (*args,Â **kwargs) Perform floor operation on the data to the specified freq . Series.dt.ceil (*args,Â **kwargs) Perform ceil operation on the data to the specified freq . Series.dt.month_name (*args,Â **kwargs) Return the month names with specified locale. Series.dt.day_name (*args,Â **kwargs) Return the day names with specified locale. Series.dt.as_unit (*args,Â **kwargs) Period properties # Series.dt.qyear Series.dt.start_time Get the Timestamp for the start of the period. Series.dt.end_time Get the Timestamp for the end of the period. Timedelta properties # Series.dt.days Number of days for each element. Series.dt.seconds Number of seconds (>= 0 and less than 1 day) for each element. Series.dt.microseconds Number of microseconds (>= 0 and less than 1 second) for each element. Series.dt.nanoseconds Number of nanoseconds (>= 0 and less than 1 microsecond) for each element. Series.dt.components Return a Dataframe of the components of the Timedeltas. Series.dt.unit Timedelta methods # Series.dt.to_pytimedelta () Return an array of native datetime.timedelta objects. Series.dt.total_seconds (*args,Â **kwargs) Return total duration of each element expressed in seconds. Series.dt.as_unit (*args,Â **kwargs) String handling # Series.str can be used to access the values of the series as strings and apply several methods to it. These can be accessed like Series.str.<function/property> . Series.str.capitalize () Convert strings in the Series/Index to be capitalized. Series.str.casefold () Convert strings in the Series/Index to be casefolded. Series.str.cat ([others,Â sep,Â na_rep,Â join]) Concatenate strings in the Series/Index with given separator. Series.str.center (width[,Â fillchar]) Pad left and right side of strings in the Series/Index. Series.str.contains (pat[,Â case,Â flags,Â na,Â ...]) Test if pattern or regex is contained within a string of a Series or Index. Series.str.count (pat[,Â flags]) Count occurrences of pattern in each string of the Series/Index. Series.str.decode (encoding[,Â errors,Â dtype]) Decode character string in the Series/Index using indicated encoding. Series.str.encode (encoding[,Â errors]) Encode character string in the Series/Index using indicated encoding. Series.str.endswith (pat[,Â na]) Test if the end of each string element matches a pattern. Series.str.extract (pat[,Â flags,Â expand]) Extract capture groups in the regex pat as columns in a DataFrame. Series.str.extractall (pat[,Â flags]) Extract capture groups in the regex pat as columns in DataFrame. Series.str.find (sub[,Â start,Â end]) Return lowest indexes in each strings in the Series/Index. Series.str.findall (pat[,Â flags]) Find all occurrences of pattern or regular expression in the Series/Index. Series.str.fullmatch (pat[,Â case,Â flags,Â na]) Determine if each string entirely matches a regular expression. Series.str.get (i) Extract element from each component at specified position or with specified key. Series.str.index (sub[,Â start,Â end]) Return lowest indexes in each string in Series/Index. Series.str.join (sep) Join lists contained as elements in the Series/Index with passed delimiter. Series.str.len () Compute the length of each element in the Series/Index. Series.str.ljust (width[,Â fillchar]) Pad right side of strings in the Series/Index. Series.str.lower () Convert strings in the Series/Index to lowercase. Series.str.lstrip ([to_strip]) Remove leading characters. Series.str.match (pat[,Â case,Â flags,Â na]) Determine if each string starts with a match of a regular expression. Series.str.normalize (form) Return the Unicode normal form for the strings in the Series/Index. Series.str.pad (width[,Â side,Â fillchar]) Pad strings in the Series/Index up to width. Series.str.partition ([sep,Â expand]) Split the string at the first occurrence of sep . Series.str.removeprefix (prefix) Remove a prefix from an object series. Series.str.removesuffix (suffix) Remove a suffix from an object series. Series.str.repeat (repeats) Duplicate each string in the Series or Index. Series.str.replace (pat,Â repl[,Â n,Â case,Â ...]) Replace each occurrence of pattern/regex in the Series/Index. Series.str.rfind (sub[,Â start,Â end]) Return highest indexes in each strings in the Series/Index. Series.str.rindex (sub[,Â start,Â end]) Return highest indexes in each string in Series/Index. Series.str.rjust (width[,Â fillchar]) Pad left side of strings in the Series/Index. Series.str.rpartition ([sep,Â expand]) Split the string at the last occurrence of sep . Series.str.rstrip ([to_strip]) Remove trailing characters. Series.str.slice ([start,Â stop,Â step]) Slice substrings from each element in the Series or Index. Series.str.slice_replace ([start,Â stop,Â repl]) Replace a positional slice of a string with another value. Series.str.split ([pat,Â n,Â expand,Â regex]) Split strings around given separator/delimiter. Series.str.rsplit ([pat,Â n,Â expand]) Split strings around given separator/delimiter. Series.str.startswith (pat[,Â na]) Test if the start of each string element matches a pattern. Series.str.strip ([to_strip]) Remove leading and trailing characters. Series.str.swapcase () Convert strings in the Series/Index to be swapcased. Series.str.title () Convert strings in the Series/Index to titlecase. Series.str.translate (table) Map all characters in the string through the given mapping table. Series.str.upper () Convert strings in the Series/Index to uppercase. Series.str.wrap (width,Â **kwargs) Wrap strings in Series/Index at specified line width. Series.str.zfill (width) Pad strings in the Series/Index by prepending '0' characters. Series.str.isalnum () Check whether all characters in each string are alphanumeric. Series.str.isalpha () Check whether all characters in each string are alphabetic. Series.str.isdigit () Check whether all characters in each string are digits. Series.str.isspace () Check whether all characters in each string are whitespace. Series.str.islower () Check whether all characters in each string are lowercase. Series.str.isupper () Check whether all characters in each string are uppercase. Series.str.istitle () Check whether all characters in each string are titlecase. Series.str.isnumeric () Check whether all characters in each string are numeric. Series.str.isdecimal () Check whether all characters in each string are decimal. Series.str.get_dummies ([sep]) Return DataFrame of dummy/indicator variables for Series. Categorical accessor # Categorical-dtype specific methods and attributes are available under the Series.cat accessor. Series.cat.categories The categories of this categorical. Series.cat.ordered Whether the categories have an ordered relationship. Series.cat.codes Return Series of codes as well as the index. Series.cat.rename_categories (*args,Â **kwargs) Rename categories. Series.cat.reorder_categories (*args,Â **kwargs) Reorder categories as specified in new_categories. Series.cat.add_categories (*args,Â **kwargs) Add new categories. Series.cat.remove_categories (*args,Â **kwargs) Remove the specified categories. Series.cat.remove_unused_categories (*args,Â ...) Remove categories which are not used. Series.cat.set_categories (*args,Â **kwargs) Set the categories to the specified new categories. Series.cat.as_ordered (*args,Â **kwargs) Set the Categorical to be ordered. Series.cat.as_unordered (*args,Â **kwargs) Set the Categorical to be unordered. Sparse accessor # Sparse-dtype specific methods and attributes are provided under the Series.sparse accessor. Series.sparse.npoints The number of non- fill_value points. Series.sparse.density The percent of non- fill_value points, as decimal. Series.sparse.fill_value Elements in data that are fill_value are not stored. Series.sparse.sp_values An ndarray containing the non- fill_value values. Series.sparse.from_coo (A[,Â dense_index]) Create a Series with sparse values from a scipy.sparse.coo_matrix. Series.sparse.to_coo ([row_levels,Â ...]) Create a scipy.sparse.coo_matrix from a Series with MultiIndex. List accessor # Arrow list-dtype specific methods and attributes are provided under the Series.list accessor. Series.list.flatten () Flatten list values. Series.list.len () Return the length of each list in the Series. Series.list.__getitem__ (key) Index or slice lists in the Series. Struct accessor # Arrow struct-dtype specific methods and attributes are provided under the Series.struct accessor. Series.struct.dtypes Return the dtype object of each child field of the struct. Series.struct.field (name_or_index) Extract a child field of a struct as a Series. Series.struct.explode () Extract all child fields of a struct as a DataFrame. Flags # Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in Series.attrs . Flags (obj,Â *,Â allows_duplicate_labels) Flags that apply to pandas objects. Metadata # Series.attrs is a dictionary for storing global metadata for this Series. Warning Series.attrs is considered experimental and may change without warning. Series.attrs Dictionary of global attributes of this dataset. Plotting # Series.plot is both a callable method and a namespace attribute for specific plotting methods of the form Series.plot.<kind> . Series.plot ([kind,Â ax,Â figsize,Â ....]) Series plotting accessor and method Series.plot.area ([x,Â y,Â stacked]) Draw a stacked area plot. Series.plot.bar ([x,Â y]) Vertical bar plot. Series.plot.barh ([x,Â y]) Make a horizontal bar plot. Series.plot.box ([by]) Make a box plot of the DataFrame columns. Series.plot.density ([bw_method,Â ind]) Generate Kernel Density Estimate plot using Gaussian kernels. Series.plot.hist ([by,Â bins]) Draw one histogram of the DataFrame's columns. Series.plot.kde ([bw_method,Â ind]) Generate Kernel Density Estimate plot using Gaussian kernels. Series.plot.line ([x,Â y]) Plot Series or DataFrame as lines. Series.plot.pie (**kwargs) Generate a pie plot. Series.hist ([by,Â ax,Â grid,Â xlabelsize,Â ...]) Draw histogram of the input series using matplotlib. Serialization / IO / conversion # Series.to_pickle (path,Â *[,Â compression,Â ...]) Pickle (serialize) object to file. Series.to_csv ([path_or_buf,Â sep,Â na_rep,Â ...]) Write object to a comma-separated values (csv) file. Series.to_dict (*[,Â into]) Convert Series to {label -> value} dict or dict-like object. Series.to_excel (excel_writer,Â *[,Â ...]) Write object to an Excel sheet. Series.to_frame ([name]) Convert Series to DataFrame. Series.to_xarray () Return an xarray object from the pandas object. Series.to_hdf (path_or_buf,Â *,Â key[,Â mode,Â ...]) Write the contained data to an HDF5 file using HDFStore. Series.to_sql (name,Â con,Â *[,Â schema,Â ...]) Write records stored in a DataFrame to a SQL database. Series.to_json ([path_or_buf,Â orient,Â ...]) Convert the object to a JSON string. Series.to_string ([buf,Â na_rep,Â ...]) Render a string representation of the Series. Series.to_clipboard (*[,Â excel,Â sep]) Copy object to the system clipboard. Series.to_latex ([buf,Â columns,Â header,Â ...]) Render object to a LaTeX tabular, longtable, or nested table. Series.to_markdown ([buf,Â mode,Â index,Â ...]) Print Series in Markdown-friendly format. previous pandas.api.interchange.from_dataframe next pandas.Series On this page Constructor Attributes Conversion Indexing, iteration Binary operator functions Function application, GroupBy & window Computations / descriptive stats Reindexing / selection / label manipulation Missing data handling Reshaping, sorting Combining / comparing / joining / merging Time Series-related Accessors Datetimelike properties Datetime properties Datetime methods Period properties Timedelta properties Timedelta methods String handling Categorical accessor Sparse accessor List accessor Struct accessor Flags Metadata Plotting Serialization / IO / conversion Show Source",
    "url": "https://pandas.pydata.org/docs/reference/series.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.autocorrelation_plot # pandas.plotting. autocorrelation_plot ( series , ax = None , ** kwargs ) [source] # Autocorrelation plot for time series. Parameters : series Series The time series to visualize. ax Matplotlib axis object, optional The matplotlib axis object to use. **kwargs Options to pass to matplotlib plotting method. Returns : matplotlib.axes.Axes Examples The horizontal lines in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. >>> spacing = np . linspace ( - 9 * np . pi , 9 * np . pi , num = 1000 ) >>> s = pd . Series ( 0.7 * np . random . rand ( 1000 ) + 0.3 * np . sin ( spacing )) >>> pd . plotting . autocorrelation_plot ( s ) previous pandas.plotting.andrews_curves next pandas.plotting.bootstrap_plot On this page autocorrelation_plot() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.plot_params # pandas.plotting. plot_params = {'xaxis.compat': False} # Stores pandas plotting options. Allows for parameter aliasing so you can just use parameter names that are the same as the plot function parameters, but is stored in a canonical format that makes it easy to breakdown into groups later. Examples >>> np . random . seed ( 42 ) >>> df = pd . DataFrame ({ 'A' : np . random . randn ( 10 ), ... 'B' : np . random . randn ( 10 )}, ... index = pd . date_range ( \"1/1/2000\" , ... freq = '4MS' , periods = 10 )) >>> with pd . plotting . plot_params . use ( \"x_compat\" , True ): ... _ = df [ \"A\" ] . plot ( color = \"r\" ) ... _ = df [ \"B\" ] . plot ( color = \"g\" ) previous pandas.plotting.parallel_coordinates next pandas.plotting.radviz On this page plot_params Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.plot_params.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Style Style # Styler objects are returned by pandas.DataFrame.style . Styler constructor # Styler (data[,Â precision,Â table_styles,Â ...]) Helps style a DataFrame or Series according to the data with HTML and CSS. Styler.from_custom_template (searchpath[,Â ...]) Factory function for creating a subclass of Styler . Styler properties # Styler.env Styler.template_html Styler.template_html_style Styler.template_html_table Styler.template_latex Styler.template_string Styler.loader Style application # Styler.apply (func[,Â axis,Â subset]) Apply a CSS-styling function column-wise, row-wise, or table-wise. Styler.map (func[,Â subset]) Apply a CSS-styling function elementwise. Styler.apply_index (func[,Â axis,Â level]) Apply a CSS-styling function to the index or column headers, level-wise. Styler.map_index (func[,Â axis,Â level]) Apply a CSS-styling function to the index or column headers, elementwise. Styler.format ([formatter,Â subset,Â na_rep,Â ...]) Format the text display value of cells. Styler.format_index ([formatter,Â axis,Â ...]) Format the text display value of index labels or column headers. Styler.relabel_index (labels[,Â axis,Â level]) Relabel the index, or column header, keys to display a set of specified values. Styler.hide ([subset,Â axis,Â level,Â names]) Hide the entire index / column headers, or specific rows / columns from display. Styler.concat (other) Append another Styler to combine the output into a single table. Styler.set_td_classes (classes) Set the class attribute of <td> HTML elements. Styler.set_table_styles ([table_styles,Â ...]) Set the table styles included within the <style> HTML element. Styler.set_table_attributes (attributes) Set the table attributes added to the <table> HTML element. Styler.set_tooltips (ttips[,Â props,Â css_class]) Set the DataFrame of strings on Styler generating :hover tooltips. Styler.set_caption (caption) Set the text added to a <caption> HTML element. Styler.set_sticky ([axis,Â pixel_size,Â levels]) Add CSS to permanently display the index or column headers in a scrolling frame. Styler.set_properties ([subset]) Set defined CSS-properties to each <td> HTML element for the given subset. Styler.set_uuid (uuid) Set the uuid applied to id attributes of HTML elements. Styler.clear () Reset the Styler , removing any previously applied styles. Styler.pipe (func,Â *args,Â **kwargs) Apply func(self, *args, **kwargs) , and return the result. Builtin styles # Styler.highlight_null ([color,Â subset,Â props]) Highlight missing values with a style. Styler.highlight_max ([subset,Â color,Â axis,Â ...]) Highlight the maximum with a style. Styler.highlight_min ([subset,Â color,Â axis,Â ...]) Highlight the minimum with a style. Styler.highlight_between ([subset,Â color,Â ...]) Highlight a defined range with a style. Styler.highlight_quantile ([subset,Â color,Â ...]) Highlight values defined by a quantile with a style. Styler.background_gradient ([cmap,Â low,Â ...]) Color the background in a gradient style. Styler.text_gradient ([cmap,Â low,Â high,Â ...]) Color the text in a gradient style. Styler.bar ([subset,Â axis,Â color,Â cmap,Â ...]) Draw bar chart in the cell backgrounds. Style export and import # Styler.to_html ([buf,Â table_uuid,Â ...]) Write Styler to a file, buffer or string in HTML-CSS format. Styler.to_latex ([buf,Â column_format,Â ...]) Write Styler to a file, buffer or string in LaTeX format. Styler.to_excel (excel_writer[,Â sheet_name,Â ...]) Write Styler to an Excel sheet. Styler.to_string ([buf,Â encoding,Â ...]) Write Styler to a file, buffer or string in text format. Styler.export () Export the styles applied to the current Styler. Styler.use (styles) Set the styles on the current Styler. previous pandas.core.resample.Resampler.quantile next pandas.io.formats.style.Styler On this page Styler constructor Styler properties Style application Builtin styles Style export and import Show Source",
    "url": "https://pandas.pydata.org/docs/reference/style.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Migration... Migration guide for the new string data type (pandas 3.0) # The upcoming pandas 3.0 release introduces a new, default string data type. This will most likely cause some work when upgrading to pandas 3.0, and this page provides an overview of the issues you might run into and gives guidance on how to address them. This new dtype is already available in the pandas 2.3 release, and you can enable it with: pd . options . future . infer_string = True This allows you to test your code before the final 3.0 release. Background # Historically, pandas has always used the NumPy object dtype as the default to store text data. This has two primary drawbacks. First, object dtype is not specific to strings: any Python object can be stored in an object -dtype array, not just strings, and seeing object as the dtype for a column with strings is confusing for users. Second, this is not always very efficient (both performance wise and for memory usage). Since pandas 1.0, an opt-in string data type has been available, but this has not yet been made the default, and uses the pd.NA scalar to represent missing values. Pandas 3.0 changes the default dtype for strings to a new string data type, a variant of the existing optional string data type but using NaN as the missing value indicator, to be consistent with the other default data types. To improve performance, the new string data type will use the pyarrow package by default, if installed (and otherwise it uses object dtype under the hood as a fallback). See PDEP-14: Dedicated string data type for pandas 3.0 for more background and details. Brief introduction to the new default string dtype # By default, pandas will infer this new string dtype instead of object dtype for string data (when creating pandas objects, such as in constructors or IO functions). Being a default dtype means that the string dtype will be used in IO methods or constructors when the dtype is being inferred and the input is inferred to be string data: >>> pd . Series ([ \"a\" , \"b\" , None ]) 0 a 1 b 2 NaN dtype: str It can also be specified explicitly using the \"str\" alias: >>> pd . Series ([ \"a\" , \"b\" , None ], dtype = \"str\" ) 0 a 1 b 2 NaN dtype: str Similarly, functions like read_csv() , read_parquet() , and others will now use the new string dtype when reading string data. In contrast to the current object dtype, the new string dtype will only store strings. This also means that it will raise an error if you try to store a non-string value in it (see below for more details). Missing values with the new string dtype are always represented as NaN ( np.nan ), and the missing value behavior is similar to other default dtypes. This new string dtype should otherwise behave the same as the existing object dtype users are used to. For example, all string-specific methods through the str accessor will work the same: >>> ser = pd . Series ([ \"a\" , \"b\" , None ], dtype = \"str\" ) >>> ser . str . upper () 0 A 1 B 2 NaN dtype: str Note The new default string dtype is an instance of the pandas.StringDtype class. The dtype can be constructed as pd.StringDtype(na_value=np.nan) , but for general usage we recommend to use the shorter \"str\" alias. Overview of behavior differences and how to address them # The dtype is no longer a numpy âobjectâ dtype # When inferring or reading string data, the data type of the resulting DataFrame column or Series will silently start being the new \"str\" dtype instead of the numpy \"object\" dtype, and this can have some impact on your code. The new string dtype is a pandas data type (âextension dtypeâ), and no longer a numpy np.dtype instance. Therefore, passing the dtype of a string column to numpy functions will no longer work (e.g. passing it to a dtype= argument of a numpy function, or using np.issubdtype to check the dtype). Checking the dtype # When checking the dtype, code might currently do something like: >>> ser = pd . Series ([ \"a\" , \"b\" , \"c\" ]) >>> ser . dtype == \"object\" to check for columns with string data (by checking for the dtype being \"object\" ). This will no longer work in pandas 3+, since ser.dtype will now be \"str\" with the new default string dtype, and the above check will return False . To check for columns with string data, you should instead use: >>> ser . dtype == \"str\" How to write compatible code For code that should work on both pandas 2.x and 3.x, you can use the pandas.api.types.is_string_dtype() function: >>> pd . api . types . is_string_dtype ( ser . dtype ) True This will return True for both the object dtype and the string dtypes. Hardcoded use of object dtype # If you have code where the dtype is hardcoded in constructors, like >>> pd . Series ([ \"a\" , \"b\" , \"c\" ], dtype = \"object\" ) this will keep using the object dtype. You will want to update this code to ensure you get the benefits of the new string dtype. How to write compatible code? First, in many cases it can be sufficient to remove the specific data type, and let pandas do the inference. But if you want to be specific, you can specify the \"str\" dtype: >>> pd . Series ([ \"a\" , \"b\" , \"c\" ], dtype = \"str\" ) This is actually compatible with pandas 2.x as well, since in pandas < 3, dtype=\"str\" was essentially treated as an alias for object dtype. Attention While using dtype=\"str\" in constructors is compatible with pandas 2.x, specifying it as the dtype in astype() runs into the issue of also stringifying missing values in pandas 2.x. See the section astype(str) preserving missing values for more details. For selecting string columns with select_dtypes() in a pandas 2.x and 3.x compatible way, it is not possible to use \"str\" . While this works for pandas 3.x, it raises an error in pandas 2.x. As an alternative, you can select both object (for pandas 2.x) and \"string\" (for pandas 3.x; which will also select the default str dtype and does not error on pandas 2.x): # can use ``include=[\"str\"]`` for pandas >= 3 >>> df . select_dtypes ( include = [ \"object\" , \"string\" ]) The missing value sentinel is now always NaN # When using object dtype, multiple possible missing value sentinels are supported, including None and np.nan . With the new default string dtype, the missing value sentinel is always NaN ( np.nan ): # with object dtype, None is preserved as None and seen as missing >>> ser = pd . Series ([ \"a\" , \"b\" , None ], dtype = \"object\" ) >>> ser 0 a 1 b 2 None dtype : object >>> print ( ser [ 2 ]) None # with the new string dtype, any missing value like None is coerced to NaN >>> ser = pd . Series ([ \"a\" , \"b\" , None ], dtype = \"str\" ) >>> ser 0 a 1 b 2 NaN dtype : str >>> print ( ser [ 2 ]) nan Generally this should be no problem when relying on missing value behavior in pandas methods (for example, ser.isna() will give the same result as before). But when you relied on the exact value of None being present, that can impact your code. How to write compatible code? When checking for a missing value, instead of checking for the exact value of None or np.nan , you should use the pandas.isna() function. This is the most robust way to check for missing values, as it will work regardless of the dtype and the exact missing value sentinel: >>> pd . isna ( ser [ 2 ]) True One caveat: this function works both on scalars and on array-likes, and in the latter case it will return an array of bools. When using it in a Boolean context (for example, if pd.isna(..): .. ) be sure to only pass a scalar to it. âsetitemâ operations will now raise an error for non-string data # With the new string dtype, any attempt to set a non-string value in a Series or DataFrame will raise an error: >>> ser = pd . Series ([ \"a\" , \"b\" , None ], dtype = \"str\" ) >>> ser [ 1 ] = 2.5 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ... TypeError: Invalid value '2.5' for dtype 'str'. Value should be a string or missing value, got 'float' instead. If you relied on the flexible nature of object dtype being able to hold any Python object, but your initial data was inferred as strings, your code might be impacted by this change. How to write compatible code? You can update your code to ensure you only set string values in such columns, or otherwise you can explicitly ensure the column has object dtype first. This can be done by specifying the dtype explicitly in the constructor, or by using the astype() method: >>> ser = pd . Series ([ \"a\" , \"b\" , None ], dtype = \"str\" ) >>> ser = ser . astype ( \"object\" ) >>> ser [ 1 ] = 2.5 This astype(\"object\") call will be redundant when using pandas 2.x, but this code will work for all versions. Invalid unicode input # Python allows to have a built-in str object that represents invalid unicode data. And since the object dtype can hold any Python object, you can have a pandas Series with such invalid unicode data: >>> ser = pd . Series ([ \" \\u2600 \" , \" \\ud83d \" ], dtype = object ) >>> ser 0 â 1 \\ud83d dtype: object However, when using the string dtype using pyarrow under the hood, this can only store valid unicode data, and otherwise it will raise an error: >>> ser = pd . Series ([ \" \\u2600 \" , \" \\ud83d \" ]) --------------------------------------------------------------------------- UnicodeEncodeError Traceback (most recent call last) ... UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83d' in position 0: surrogates not allowed If you want to keep the previous behaviour, you can explicitly specify dtype=object to keep working with object dtype. When you have byte data that you want to convert to strings using decode() , the decode() method now has a dtype parameter to be able to specify object dtype instead of the default of string dtype for this use case. Series.values() now returns an ExtensionArray # With object dtype, using .values on a Series will return the underlying NumPy array. >>> ser = pd . Series ([ \"a\" , \"b\" , np . nan ], dtype = \"object\" ) >>> type ( ser . values ) <class 'numpy.ndarray'> However with the new string dtype, the underlying ExtensionArray is returned instead. >>> ser = pd . Series ([ \"a\" , \"b\" , pd . NA ], dtype = \"str\" ) >>> ser . values <ArrowStringArray> ['a', 'b', nan] Length: 3, dtype: str If your code requires a NumPy array, you should use Series.to_numpy() . >>> ser = pd . Series ([ \"a\" , \"b\" , pd . NA ], dtype = \"str\" ) >>> ser . to_numpy () ['a' 'b' nan] In general, you should always prefer Series.to_numpy() to get a NumPy array or Series.array() to get an ExtensionArray over using Series.values() . Notable bug fixes # astype(str) preserving missing values # The stringifying of missing values is a long standing âbugâ or misfeature, as discussed in pandas-dev/pandas#25353 , but fixing it introduces a significant behaviour change. With pandas < 3, when using astype(str) or astype(\"str\") , the operation would convert every element to a string, including the missing values: # OLD behavior in pandas < 3 >>> ser = pd . Series ([ 1.5 , np . nan ]) >>> ser 0 1.5 1 NaN dtype : float64 >>> ser . astype ( \"str\" ) 0 1.5 1 nan dtype : object >>> ser . astype ( \"str\" ) . to_numpy () array ([ '1.5' , 'nan' ], dtype = object ) Note how NaN ( np.nan ) was converted to the string \"nan\" . This was not the intended behavior, and it was inconsistent with how other dtypes handled missing values. With pandas 3, this behavior has been fixed, and now astype(\"str\") will cast to the new string dtype, which preserves the missing values: # NEW behavior in pandas 3 >>> pd . options . future . infer_string = True >>> ser = pd . Series ([ 1.5 , np . nan ]) >>> ser . astype ( \"str\" ) 0 1.5 1 NaN dtype : str >>> ser . astype ( \"str\" ) . to_numpy () array ([ '1.5' , nan ], dtype = object ) If you want to preserve the old behaviour of converting every object to a string, you can use ser.map(str) instead. If you want do such conversion while preserving the missing values in a way that works with both pandas 2.x and 3.x, you can use ser.map(str, na_action=\"ignore\") (for pandas 3.x only, you can do ser.astype(\"str\") ). If you want to convert to object or string dtype for pandas 2.x and 3.x, respectively, without needing to stringify each individual element, you will have to use a conditional check on the pandas version. For example, to convert a categorical Series with string categories to its dense non-categorical version with object or string dtype: >>> import pandas as pd >>> ser = pd . Series ([ \"a\" , np . nan ], dtype = \"category\" ) >>> ser . astype ( object if pd . __version__ < \"3\" else \"str\" ) prod() raising for string data # In pandas < 3, calling the prod() method on a Series with string data would generally raise an error, except when the Series was empty or contained only a single string (potentially with missing values): >>> ser = pd . Series ([ \"a\" , None ], dtype = object ) >>> ser . prod () 'a' When the Series contains multiple strings, it will raise a TypeError . This behaviour stays the same in pandas 3 when using the flexible object dtype. But by virtue of using the new string dtype, this will generally consistently raise an error regardless of the number of strings: >>> ser = pd . Series ([ \"a\" , None ], dtype = \"str\" ) >>> ser . prod () --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ... TypeError: Cannot perform reduction 'prod' with string dtype previous Sparse data structures next Frequently Asked Questions (FAQ) On this page Background Brief introduction to the new default string dtype Overview of behavior differences and how to address them The dtype is no longer a numpy âobjectâ dtype Checking the dtype Hardcoded use of object dtype The missing value sentinel is now always NaN âsetitemâ operations will now raise an error for non-string data Invalid unicode input Series.values() now returns an ExtensionArray Notable bug fixes astype(str) preserving missing values prod() raising for string data Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/migration-3-strings.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference pandas... pandas arrays, scalars, and data types # Objects # For most data types, pandas uses NumPy arrays as the concrete objects contained with a Index , Series , or DataFrame . For some data types, pandas extends NumPyâs type system. String aliases for these types can be found at dtypes . Kind of Data pandas Data Type Scalar Array TZ-aware datetime DatetimeTZDtype Timestamp Datetimes Timedeltas (none) Timedelta Timedeltas Period (time spans) PeriodDtype Period Periods Intervals IntervalDtype Interval Intervals Nullable Integer Int64Dtype , â¦ (none) Nullable integer Nullable Float Float64Dtype , â¦ (none) Nullable float Categorical CategoricalDtype (none) Categoricals Sparse SparseDtype (none) Sparse Strings StringDtype str Strings Nullable Boolean BooleanDtype bool Nullable Boolean PyArrow ArrowDtype Python Scalars or NA PyArrow pandas and third-party libraries can extend NumPyâs type system (see Extension types ). The top-level array() method can be used to create a new array, which may be stored in a Series , Index , or as a column in a DataFrame . array (data[,Â dtype,Â copy]) Create an array. PyArrow # Warning This feature is experimental, and the API can change in a future release without warning. The arrays.ArrowExtensionArray is backed by a pyarrow.ChunkedArray with a pyarrow.DataType instead of a NumPy array and data type. The .dtype of a arrays.ArrowExtensionArray is an ArrowDtype . Pyarrow provides similar array and data type support as NumPy including first-class nullability support for all data types, immutability and more. The table below shows the equivalent pyarrow-backed ( pa ), pandas extension, and numpy ( np ) types that are recognized by pandas. Pyarrow-backed types below need to be passed into ArrowDtype to be recognized by pandas e.g. pd.ArrowDtype(pa.bool_()) PyArrow type pandas extension type NumPy type pyarrow.bool_() BooleanDtype np.bool_ pyarrow.int8() Int8Dtype np.int8 pyarrow.int16() Int16Dtype np.int16 pyarrow.int32() Int32Dtype np.int32 pyarrow.int64() Int64Dtype np.int64 pyarrow.uint8() UInt8Dtype np.uint8 pyarrow.uint16() UInt16Dtype np.uint16 pyarrow.uint32() UInt32Dtype np.uint32 pyarrow.uint64() UInt64Dtype np.uint64 pyarrow.float32() Float32Dtype np.float32 pyarrow.float64() Float64Dtype np.float64 pyarrow.time32() (none) (none) pyarrow.time64() (none) (none) pyarrow.timestamp() DatetimeTZDtype np.datetime64 pyarrow.date32() (none) (none) pyarrow.date64() (none) (none) pyarrow.duration() (none) np.timedelta64 pyarrow.binary() (none) (none) pyarrow.string() StringDtype np.str_ pyarrow.decimal128() (none) (none) pyarrow.list_() (none) (none) pyarrow.map_() (none) (none) pyarrow.dictionary() CategoricalDtype (none) Note Pyarrow-backed string support is provided by both pd.StringDtype(\"pyarrow\") and pd.ArrowDtype(pa.string()) . pd.StringDtype(\"pyarrow\") is described below in the string section and will be returned if the string alias \"string[pyarrow]\" is specified. pd.ArrowDtype(pa.string()) generally has better interoperability with ArrowDtype of different types. While individual values in an arrays.ArrowExtensionArray are stored as a PyArrow objects, scalars are returned as Python scalars corresponding to the data type, e.g. a PyArrow int64 will be returned as Python int, or NA for missing values. arrays.ArrowExtensionArray (values) Pandas ExtensionArray backed by a PyArrow ChunkedArray. ArrowDtype (pyarrow_dtype) An ExtensionDtype for PyArrow data types. For more information, please see the PyArrow user guide Datetimes # NumPy cannot natively represent timezone-aware datetimes. pandas supports this with the arrays.DatetimeArray extension array, which can hold timezone-naive or timezone-aware values. Timestamp , a subclass of datetime.datetime , is pandasâ scalar type for timezone-naive or timezone-aware datetime data. NaT is the missing value for datetime data. Timestamp ([ts_input,Â year,Â month,Â day,Â ...]) Pandas replacement for python datetime.datetime object. Properties # Timestamp.asm8 Return numpy datetime64 format in nanoseconds. Timestamp.day Timestamp.dayofweek Return day of the week. Timestamp.day_of_week Return day of the week. Timestamp.dayofyear Return the day of the year. Timestamp.day_of_year Return the day of the year. Timestamp.days_in_month Return the number of days in the month. Timestamp.daysinmonth Return the number of days in the month. Timestamp.fold Timestamp.hour Timestamp.is_leap_year Return True if year is a leap year. Timestamp.is_month_end Check if the date is the last day of the month. Timestamp.is_month_start Check if the date is the first day of the month. Timestamp.is_quarter_end Check if date is last day of the quarter. Timestamp.is_quarter_start Check if the date is the first day of the quarter. Timestamp.is_year_end Return True if date is last day of the year. Timestamp.is_year_start Return True if date is first day of the year. Timestamp.max Timestamp.microsecond Timestamp.min Timestamp.minute Timestamp.month Timestamp.nanosecond Timestamp.quarter Return the quarter of the year. Timestamp.resolution Timestamp.second Timestamp.tz Alias for tzinfo. Timestamp.tzinfo Timestamp.unit The abbreviation associated with self._creso. Timestamp.value Timestamp.week Return the week number of the year. Timestamp.weekofyear Return the week number of the year. Timestamp.year Methods # Timestamp.as_unit (unit[,Â round_ok]) Convert the underlying int64 representaton to the given unit. Timestamp.astimezone (tz) Convert timezone-aware Timestamp to another time zone. Timestamp.ceil (freq[,Â ambiguous,Â nonexistent]) Return a new Timestamp ceiled to this resolution. Timestamp.combine (date,Â time) Combine date, time into datetime with same date and time fields. Timestamp.ctime () Return ctime() style string. Timestamp.date () Return date object with same year, month and day. Timestamp.day_name ([locale]) Return the day name of the Timestamp with specified locale. Timestamp.dst () Return the daylight saving time (DST) adjustment. Timestamp.floor (freq[,Â ambiguous,Â nonexistent]) Return a new Timestamp floored to this resolution. Timestamp.fromordinal (ordinal[,Â tz]) Construct a timestamp from a a proleptic Gregorian ordinal. Timestamp.fromtimestamp (ts) Transform timestamp[, tz] to tz's local time from POSIX timestamp. Timestamp.isocalendar () Return a named tuple containing ISO year, week number, and weekday. Timestamp.isoformat ([sep,Â timespec]) Return the time formatted according to ISO 8601. Timestamp.isoweekday () Return the day of the week represented by the date. Timestamp.month_name ([locale]) Return the month name of the Timestamp with specified locale. Timestamp.normalize () Normalize Timestamp to midnight, preserving tz information. Timestamp.now ([tz]) Return new Timestamp object representing current time local to tz. Timestamp.replace ([year,Â month,Â day,Â hour,Â ...]) Implements datetime.replace, handles nanoseconds. Timestamp.round (freq[,Â ambiguous,Â nonexistent]) Round the Timestamp to the specified resolution. Timestamp.strftime (format) Return a formatted string of the Timestamp. Timestamp.strptime (string,Â format) Function is not implemented. Timestamp.time () Return time object with same time but with tzinfo=None. Timestamp.timestamp () Return POSIX timestamp as float. Timestamp.timetuple () Return time tuple, compatible with time.localtime(). Timestamp.timetz () Return time object with same time and tzinfo. Timestamp.to_datetime64 () Return a numpy.datetime64 object with same precision. Timestamp.to_numpy ([dtype,Â copy]) Convert the Timestamp to a NumPy datetime64. Timestamp.to_julian_date () Convert TimeStamp to a Julian Date. Timestamp.to_period ([freq]) Return an period of which this timestamp is an observation. Timestamp.to_pydatetime ([warn]) Convert a Timestamp object to a native Python datetime object. Timestamp.today ([tz]) Return the current time in the local timezone. Timestamp.toordinal () Return proleptic Gregorian ordinal. Timestamp.tz_convert (tz) Convert timezone-aware Timestamp to another time zone. Timestamp.tz_localize (tz[,Â ambiguous,Â ...]) Localize the Timestamp to a timezone. Timestamp.tzname () Return time zone name. Timestamp.utcfromtimestamp (ts) Construct a timezone-aware UTC datetime from a POSIX timestamp. Timestamp.utcnow () Return a new Timestamp representing UTC day and time. Timestamp.utcoffset () Return utc offset. Timestamp.utctimetuple () Return UTC time tuple, compatible with time.localtime(). Timestamp.weekday () Return the day of the week represented by the date. A collection of timestamps may be stored in a arrays.DatetimeArray . For timezone-aware data, the .dtype of a arrays.DatetimeArray is a DatetimeTZDtype . For timezone-naive data, np.dtype(\"datetime64[ns]\") is used. If the data are timezone-aware, then every value in the array must have the same timezone. arrays.DatetimeArray (values[,Â dtype,Â freq,Â copy]) Pandas ExtensionArray for tz-naive or tz-aware datetime data. DatetimeTZDtype ([unit,Â tz]) An ExtensionDtype for timezone-aware datetime data. Timedeltas # NumPy can natively represent timedeltas. pandas provides Timedelta for symmetry with Timestamp . NaT is the missing value for timedelta data. Timedelta ([value,Â unit]) Represents a duration, the difference between two dates or times. Properties # Timedelta.asm8 Return a numpy timedelta64 array scalar view. Timedelta.components Return a components namedtuple-like. Timedelta.days Returns the days of the timedelta. Timedelta.max Timedelta.microseconds Timedelta.min Timedelta.nanoseconds Return the number of nanoseconds (n), where 0 <= n < 1 microsecond. Timedelta.resolution Timedelta.seconds Return the total hours, minutes, and seconds of the timedelta as seconds. Timedelta.unit Timedelta.value Timedelta.view (dtype) Array view compatibility. Methods # Timedelta.as_unit (unit[,Â round_ok]) Convert the underlying int64 representation to the given unit. Timedelta.ceil (freq) Return a new Timedelta ceiled to this resolution. Timedelta.floor (freq) Return a new Timedelta floored to this resolution. Timedelta.isoformat () Format the Timedelta as ISO 8601 Duration. Timedelta.round (freq) Round the Timedelta to the specified resolution. Timedelta.to_pytimedelta () Convert a pandas Timedelta object into a python datetime.timedelta object. Timedelta.to_timedelta64 () Return a numpy.timedelta64 object with 'ns' precision. Timedelta.to_numpy ([dtype,Â copy]) Convert the Timedelta to a NumPy timedelta64. Timedelta.total_seconds () Total seconds in the duration. A collection of Timedelta may be stored in a TimedeltaArray . arrays.TimedeltaArray (values[,Â dtype,Â freq,Â ...]) Pandas ExtensionArray for timedelta data. Periods # pandas represents spans of times as Period objects. Period # Period ([value,Â freq,Â ordinal,Â year,Â month,Â ...]) Represents a period of time. Properties # Period.day Get day of the month that a Period falls on. Period.dayofweek Day of the week the period lies in, with Monday=0 and Sunday=6. Period.day_of_week Day of the week the period lies in, with Monday=0 and Sunday=6. Period.dayofyear Return the day of the year. Period.day_of_year Return the day of the year. Period.days_in_month Get the total number of days in the month that this period falls on. Period.daysinmonth Get the total number of days of the month that this period falls on. Period.end_time Get the Timestamp for the end of the period. Period.freq Period.freqstr Return a string representation of the frequency. Period.hour Get the hour of the day component of the Period. Period.is_leap_year Return True if the period's year is in a leap year. Period.minute Get minute of the hour component of the Period. Period.month Return the month this Period falls on. Period.ordinal Period.quarter Return the quarter this Period falls on. Period.qyear Fiscal year the Period lies in according to its starting-quarter. Period.second Get the second component of the Period. Period.start_time Get the Timestamp for the start of the period. Period.week Get the week of the year on the given Period. Period.weekday Day of the week the period lies in, with Monday=0 and Sunday=6. Period.weekofyear Get the week of the year on the given Period. Period.year Return the year this Period falls on. Methods # Period.asfreq (freq[,Â how]) Convert Period to desired frequency, at the start or end of the interval. Period.now (freq) Return the period of now's date. Period.strftime (fmt) Returns a formatted string representation of the Period . Period.to_timestamp ([freq,Â how]) Return the Timestamp representation of the Period. A collection of Period may be stored in a arrays.PeriodArray . Every period in a arrays.PeriodArray must have the same freq . arrays.PeriodArray (values[,Â dtype,Â freq,Â copy]) Pandas ExtensionArray for storing Period data. PeriodDtype (freq) An ExtensionDtype for Period data. Intervals # Arbitrary intervals can be represented as Interval objects. Interval Immutable object implementing an Interval, a bounded slice-like interval. Properties # Interval.closed String describing the inclusive side the intervals. Interval.closed_left Check if the interval is closed on the left side. Interval.closed_right Check if the interval is closed on the right side. Interval.is_empty Indicates if an interval is empty, meaning it contains no points. Interval.left Left bound for the interval. Interval.length Return the length of the Interval. Interval.mid Return the midpoint of the Interval. Interval.open_left Check if the interval is open on the left side. Interval.open_right Check if the interval is open on the right side. Interval.overlaps (other) Check whether two Interval objects overlap. Interval.right Right bound for the interval. A collection of intervals may be stored in an arrays.IntervalArray . arrays.IntervalArray (data[,Â closed,Â dtype,Â ...]) Pandas array for interval data that are closed on the same side. IntervalDtype ([subtype,Â closed]) An ExtensionDtype for Interval data. Nullable integer # numpy.ndarray cannot natively represent integer-data with missing values. pandas provides this through arrays.IntegerArray . arrays.IntegerArray (values,Â mask[,Â copy]) Array of integer (optional missing) values. Int8Dtype () An ExtensionDtype for int8 integer data. Int16Dtype () An ExtensionDtype for int16 integer data. Int32Dtype () An ExtensionDtype for int32 integer data. Int64Dtype () An ExtensionDtype for int64 integer data. UInt8Dtype () An ExtensionDtype for uint8 integer data. UInt16Dtype () An ExtensionDtype for uint16 integer data. UInt32Dtype () An ExtensionDtype for uint32 integer data. UInt64Dtype () An ExtensionDtype for uint64 integer data. Nullable float # arrays.FloatingArray (values,Â mask[,Â copy]) Array of floating (optional missing) values. Float32Dtype () An ExtensionDtype for float32 data. Float64Dtype () An ExtensionDtype for float64 data. Categoricals # pandas defines a custom data type for representing data that can take only a limited, fixed set of values. The dtype of a Categorical can be described by a CategoricalDtype . CategoricalDtype ([categories,Â ordered]) Type for categorical data with the categories and orderedness. CategoricalDtype.categories An Index containing the unique categories allowed. CategoricalDtype.ordered Whether the categories have an ordered relationship. Categorical data can be stored in a pandas.Categorical Categorical (values[,Â categories,Â ordered,Â ...]) Represent a categorical variable in classic R / S-plus fashion. The alternative Categorical.from_codes() constructor can be used when you have the categories and integer codes already: Categorical.from_codes (codes[,Â categories,Â ...]) Make a Categorical type from codes and categories or dtype. The dtype information is available on the Categorical Categorical.dtype The CategoricalDtype for this instance. Categorical.categories The categories of this categorical. Categorical.ordered Whether the categories have an ordered relationship. Categorical.codes The category codes of this categorical index. np.asarray(categorical) works by implementing the array interface. Be aware, that this converts the Categorical back to a NumPy array, so categories and order information is not preserved! Categorical.__array__ ([dtype,Â copy]) The numpy array interface. A Categorical can be stored in a Series or DataFrame . To create a Series of dtype category , use cat = s.astype(dtype) or Series(..., dtype=dtype) where dtype is either the string 'category' an instance of CategoricalDtype . If the Series is of dtype CategoricalDtype , Series.cat can be used to change the categorical data. See Categorical accessor for more. Sparse # Data where a single value is repeated many times (e.g. 0 or NaN ) may be stored efficiently as a arrays.SparseArray . arrays.SparseArray (data[,Â sparse_index,Â ...]) An ExtensionArray for storing sparse data. SparseDtype ([dtype,Â fill_value]) Dtype for data stored in SparseArray . The Series.sparse accessor may be used to access sparse-specific attributes and methods if the Series contains sparse values. See Sparse accessor and the user guide for more. Strings # When working with text data, where each valid element is a string or missing, we recommend using StringDtype (with the alias \"string\" ). arrays.StringArray (values[,Â copy]) Extension array for string data. arrays.ArrowStringArray (values) Extension array for string data in a pyarrow.ChunkedArray . StringDtype ([storage,Â na_value]) Extension dtype for string data. The Series.str accessor is available for Series backed by a arrays.StringArray . See String handling for more. Nullable Boolean # The boolean dtype (with the alias \"boolean\" ) provides support for storing boolean data ( True , False ) with missing values, which is not possible with a bool numpy.ndarray . arrays.BooleanArray (values,Â mask[,Â copy]) Array of boolean (True/False) data with missing values. BooleanDtype () Extension dtype for boolean data. Utilities # Constructors # api.types.union_categoricals (to_union[,Â ...]) Combine list-like of Categorical-like, unioning categories. api.types.infer_dtype (value[,Â skipna]) Return a string label of the type of a scalar or list-like of values. api.types.pandas_dtype (dtype) Convert input into a pandas only dtype object or a numpy dtype object. Data type introspection # api.types.is_any_real_numeric_dtype (arr_or_dtype) Check whether the provided array or dtype is of a real number dtype. api.types.is_bool_dtype (arr_or_dtype) Check whether the provided array or dtype is of a boolean dtype. api.types.is_categorical_dtype (arr_or_dtype) (DEPRECATED) Check whether an array-like or dtype is of the Categorical dtype. api.types.is_complex_dtype (arr_or_dtype) Check whether the provided array or dtype is of a complex dtype. api.types.is_datetime64_any_dtype (arr_or_dtype) Check whether the provided array or dtype is of the datetime64 dtype. api.types.is_datetime64_dtype (arr_or_dtype) Check whether an array-like or dtype is of the datetime64 dtype. api.types.is_datetime64_ns_dtype (arr_or_dtype) Check whether the provided array or dtype is of the datetime64[ns] dtype. api.types.is_datetime64tz_dtype (arr_or_dtype) (DEPRECATED) Check whether an array-like or dtype is of a DatetimeTZDtype dtype. api.types.is_extension_array_dtype (arr_or_dtype) Check if an object is a pandas extension array type. api.types.is_float_dtype (arr_or_dtype) Check whether the provided array or dtype is of a float dtype. api.types.is_int64_dtype (arr_or_dtype) (DEPRECATED) Check whether the provided array or dtype is of the int64 dtype. api.types.is_integer_dtype (arr_or_dtype) Check whether the provided array or dtype is of an integer dtype. api.types.is_interval_dtype (arr_or_dtype) (DEPRECATED) Check whether an array-like or dtype is of the Interval dtype. api.types.is_numeric_dtype (arr_or_dtype) Check whether the provided array or dtype is of a numeric dtype. api.types.is_object_dtype (arr_or_dtype) Check whether an array-like or dtype is of the object dtype. api.types.is_period_dtype (arr_or_dtype) (DEPRECATED) Check whether an array-like or dtype is of the Period dtype. api.types.is_signed_integer_dtype (arr_or_dtype) Check whether the provided array or dtype is of a signed integer dtype. api.types.is_string_dtype (arr_or_dtype) Check whether the provided array or dtype is of the string dtype. api.types.is_timedelta64_dtype (arr_or_dtype) Check whether an array-like or dtype is of the timedelta64 dtype. api.types.is_timedelta64_ns_dtype (arr_or_dtype) Check whether the provided array or dtype is of the timedelta64[ns] dtype. api.types.is_unsigned_integer_dtype (arr_or_dtype) Check whether the provided array or dtype is of an unsigned integer dtype. api.types.is_sparse (arr) (DEPRECATED) Check whether an array-like is a 1-D pandas sparse array. Iterable introspection # api.types.is_dict_like (obj) Check if the object is dict-like. api.types.is_file_like (obj) Check if the object is a file-like object. api.types.is_list_like (obj[,Â allow_sets]) Check if the object is list-like. api.types.is_named_tuple (obj) Check if the object is a named tuple. api.types.is_iterator (obj) Check if the object is an iterator. Scalar introspection # api.types.is_bool (obj) Return True if given object is boolean. api.types.is_complex (obj) Return True if given object is complex. api.types.is_float (obj) Return True if given object is float. api.types.is_hashable (obj) Return True if hash(obj) will succeed, False otherwise. api.types.is_integer (obj) Return True if given object is integer. api.types.is_interval (obj) api.types.is_number (obj) Check if the object is a number. api.types.is_re (obj) Check if the object is a regex pattern instance. api.types.is_re_compilable (obj) Check if the object can be compiled into a regex pattern instance. api.types.is_scalar (val) Return True if given object is scalar. previous pandas.DataFrame.__dataframe__ next pandas.array On this page Objects PyArrow Datetimes Properties Methods Timedeltas Properties Methods Periods Period Properties Methods Intervals Properties Nullable integer Nullable float Categoricals Sparse Strings Nullable Boolean Utilities Constructors Data type introspection Iterable introspection Scalar introspection Show Source",
    "url": "https://pandas.pydata.org/docs/reference/arrays.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference GroupBy GroupBy # pandas.api.typing.DataFrameGroupBy and pandas.api.typing.SeriesGroupBy instances are returned by groupby calls pandas.DataFrame.groupby() and pandas.Series.groupby() respectively. Indexing, iteration # DataFrameGroupBy.__iter__ () Groupby iterator. SeriesGroupBy.__iter__ () Groupby iterator. DataFrameGroupBy.groups Dict {group name -> group labels}. SeriesGroupBy.groups Dict {group name -> group labels}. DataFrameGroupBy.indices Dict {group name -> group indices}. SeriesGroupBy.indices Dict {group name -> group indices}. DataFrameGroupBy.get_group (name[,Â obj]) Construct DataFrame from group with provided name. SeriesGroupBy.get_group (name[,Â obj]) Construct DataFrame from group with provided name. Grouper (*args,Â **kwargs) A Grouper allows the user to specify a groupby instruction for an object. Function application helper # NamedAgg (column,Â aggfunc) Helper for column specific aggregation with control over output column names. Function application # SeriesGroupBy.apply (func,Â *args,Â **kwargs) Apply function func group-wise and combine the results together. DataFrameGroupBy.apply (func,Â *args[,Â ...]) Apply function func group-wise and combine the results together. SeriesGroupBy.agg ([func,Â engine,Â engine_kwargs]) Aggregate using one or more operations over the specified axis. DataFrameGroupBy.agg ([func,Â engine,Â ...]) Aggregate using one or more operations over the specified axis. SeriesGroupBy.aggregate ([func,Â engine,Â ...]) Aggregate using one or more operations over the specified axis. DataFrameGroupBy.aggregate ([func,Â engine,Â ...]) Aggregate using one or more operations over the specified axis. SeriesGroupBy.transform (func,Â *args[,Â ...]) Call function producing a same-indexed Series on each group. DataFrameGroupBy.transform (func,Â *args[,Â ...]) Call function producing a same-indexed DataFrame on each group. SeriesGroupBy.pipe (func,Â *args,Â **kwargs) Apply a func with arguments to this GroupBy object and return its result. DataFrameGroupBy.pipe (func,Â *args,Â **kwargs) Apply a func with arguments to this GroupBy object and return its result. DataFrameGroupBy.filter (func[,Â dropna]) Filter elements from groups that don't satisfy a criterion. SeriesGroupBy.filter (func[,Â dropna]) Filter elements from groups that don't satisfy a criterion. DataFrameGroupBy computations / descriptive stats # DataFrameGroupBy.all ([skipna]) Return True if all values in the group are truthful, else False. DataFrameGroupBy.any ([skipna]) Return True if any value in the group is truthful, else False. DataFrameGroupBy.bfill ([limit]) Backward fill the values. DataFrameGroupBy.corr ([method,Â min_periods,Â ...]) Compute pairwise correlation of columns, excluding NA/null values. DataFrameGroupBy.corrwith (other[,Â axis,Â ...]) Compute pairwise correlation. DataFrameGroupBy.count () Compute count of group, excluding missing values. DataFrameGroupBy.cov ([min_periods,Â ddof,Â ...]) Compute pairwise covariance of columns, excluding NA/null values. DataFrameGroupBy.cumcount ([ascending]) Number each item in each group from 0 to the length of that group - 1. DataFrameGroupBy.cummax ([axis,Â numeric_only]) Cumulative max for each group. DataFrameGroupBy.cummin ([axis,Â numeric_only]) Cumulative min for each group. DataFrameGroupBy.cumprod ([axis]) Cumulative product for each group. DataFrameGroupBy.cumsum ([axis]) Cumulative sum for each group. DataFrameGroupBy.describe ([percentiles,Â ...]) Generate descriptive statistics. DataFrameGroupBy.diff ([periods,Â axis]) First discrete difference of element. DataFrameGroupBy.ffill ([limit]) Forward fill the values. DataFrameGroupBy.fillna ([value,Â method,Â ...]) (DEPRECATED) Fill NA/NaN values using the specified method within groups. DataFrameGroupBy.first ([numeric_only,Â ...]) Compute the first entry of each column within each group. DataFrameGroupBy.head ([n]) Return first n rows of each group. DataFrameGroupBy.idxmax ([axis,Â skipna,Â ...]) Return index of first occurrence of maximum over requested axis. DataFrameGroupBy.idxmin ([axis,Â skipna,Â ...]) Return index of first occurrence of minimum over requested axis. DataFrameGroupBy.last ([numeric_only,Â ...]) Compute the last entry of each column within each group. DataFrameGroupBy.max ([numeric_only,Â ...]) Compute max of group values. DataFrameGroupBy.mean ([numeric_only,Â ...]) Compute mean of groups, excluding missing values. DataFrameGroupBy.median ([numeric_only]) Compute median of groups, excluding missing values. DataFrameGroupBy.min ([numeric_only,Â ...]) Compute min of group values. DataFrameGroupBy.ngroup ([ascending]) Number each group from 0 to the number of groups - 1. DataFrameGroupBy.nth Take the nth row from each group if n is an int, otherwise a subset of rows. DataFrameGroupBy.nunique ([dropna]) Return DataFrame with counts of unique elements in each position. DataFrameGroupBy.ohlc () Compute open, high, low and close values of a group, excluding missing values. DataFrameGroupBy.pct_change ([periods,Â ...]) Calculate pct_change of each value to previous entry in group. DataFrameGroupBy.prod ([numeric_only,Â min_count]) Compute prod of group values. DataFrameGroupBy.quantile ([q,Â ...]) Return group values at the given quantile, a la numpy.percentile. DataFrameGroupBy.rank ([method,Â ascending,Â ...]) Provide the rank of values within each group. DataFrameGroupBy.resample (rule,Â *args[,Â ...]) Provide resampling when using a TimeGrouper. DataFrameGroupBy.rolling (*args,Â **kwargs) Return a rolling grouper, providing rolling functionality per group. DataFrameGroupBy.sample ([n,Â frac,Â replace,Â ...]) Return a random sample of items from each group. DataFrameGroupBy.sem ([ddof,Â numeric_only]) Compute standard error of the mean of groups, excluding missing values. DataFrameGroupBy.shift ([periods,Â freq,Â ...]) Shift each group by periods observations. DataFrameGroupBy.size () Compute group sizes. DataFrameGroupBy.skew ([axis,Â skipna,Â ...]) Return unbiased skew within groups. DataFrameGroupBy.std ([ddof,Â engine,Â ...]) Compute standard deviation of groups, excluding missing values. DataFrameGroupBy.sum ([numeric_only,Â ...]) Compute sum of group values. DataFrameGroupBy.var ([ddof,Â engine,Â ...]) Compute variance of groups, excluding missing values. DataFrameGroupBy.tail ([n]) Return last n rows of each group. DataFrameGroupBy.take (indices[,Â axis]) Return the elements in the given positional indices in each group. DataFrameGroupBy.value_counts ([subset,Â ...]) Return a Series or DataFrame containing counts of unique rows. SeriesGroupBy computations / descriptive stats # SeriesGroupBy.all ([skipna]) Return True if all values in the group are truthful, else False. SeriesGroupBy.any ([skipna]) Return True if any value in the group is truthful, else False. SeriesGroupBy.bfill ([limit]) Backward fill the values. SeriesGroupBy.corr (other[,Â method,Â min_periods]) Compute correlation with other Series, excluding missing values. SeriesGroupBy.count () Compute count of group, excluding missing values. SeriesGroupBy.cov (other[,Â min_periods,Â ddof]) Compute covariance with Series, excluding missing values. SeriesGroupBy.cumcount ([ascending]) Number each item in each group from 0 to the length of that group - 1. SeriesGroupBy.cummax ([axis,Â numeric_only]) Cumulative max for each group. SeriesGroupBy.cummin ([axis,Â numeric_only]) Cumulative min for each group. SeriesGroupBy.cumprod ([axis]) Cumulative product for each group. SeriesGroupBy.cumsum ([axis]) Cumulative sum for each group. SeriesGroupBy.describe ([percentiles,Â ...]) Generate descriptive statistics. SeriesGroupBy.diff ([periods,Â axis]) First discrete difference of element. SeriesGroupBy.ffill ([limit]) Forward fill the values. SeriesGroupBy.fillna ([value,Â method,Â axis,Â ...]) (DEPRECATED) Fill NA/NaN values using the specified method within groups. SeriesGroupBy.first ([numeric_only,Â ...]) Compute the first entry of each column within each group. SeriesGroupBy.head ([n]) Return first n rows of each group. SeriesGroupBy.last ([numeric_only,Â ...]) Compute the last entry of each column within each group. SeriesGroupBy.idxmax ([axis,Â skipna]) Return the row label of the maximum value. SeriesGroupBy.idxmin ([axis,Â skipna]) Return the row label of the minimum value. SeriesGroupBy.is_monotonic_increasing Return whether each group's values are monotonically increasing. SeriesGroupBy.is_monotonic_decreasing Return whether each group's values are monotonically decreasing. SeriesGroupBy.max ([numeric_only,Â min_count,Â ...]) Compute max of group values. SeriesGroupBy.mean ([numeric_only,Â engine,Â ...]) Compute mean of groups, excluding missing values. SeriesGroupBy.median ([numeric_only]) Compute median of groups, excluding missing values. SeriesGroupBy.min ([numeric_only,Â min_count,Â ...]) Compute min of group values. SeriesGroupBy.ngroup ([ascending]) Number each group from 0 to the number of groups - 1. SeriesGroupBy.nlargest ([n,Â keep]) Return the largest n elements. SeriesGroupBy.nsmallest ([n,Â keep]) Return the smallest n elements. SeriesGroupBy.nth Take the nth row from each group if n is an int, otherwise a subset of rows. SeriesGroupBy.nunique ([dropna]) Return number of unique elements in the group. SeriesGroupBy.unique () Return unique values for each group. SeriesGroupBy.ohlc () Compute open, high, low and close values of a group, excluding missing values. SeriesGroupBy.pct_change ([periods,Â ...]) Calculate pct_change of each value to previous entry in group. SeriesGroupBy.prod ([numeric_only,Â min_count]) Compute prod of group values. SeriesGroupBy.quantile ([q,Â interpolation,Â ...]) Return group values at the given quantile, a la numpy.percentile. SeriesGroupBy.rank ([method,Â ascending,Â ...]) Provide the rank of values within each group. SeriesGroupBy.resample (rule,Â *args[,Â ...]) Provide resampling when using a TimeGrouper. SeriesGroupBy.rolling (*args,Â **kwargs) Return a rolling grouper, providing rolling functionality per group. SeriesGroupBy.sample ([n,Â frac,Â replace,Â ...]) Return a random sample of items from each group. SeriesGroupBy.sem ([ddof,Â numeric_only]) Compute standard error of the mean of groups, excluding missing values. SeriesGroupBy.shift ([periods,Â freq,Â axis,Â ...]) Shift each group by periods observations. SeriesGroupBy.size () Compute group sizes. SeriesGroupBy.skew ([axis,Â skipna,Â numeric_only]) Return unbiased skew within groups. SeriesGroupBy.std ([ddof,Â engine,Â ...]) Compute standard deviation of groups, excluding missing values. SeriesGroupBy.sum ([numeric_only,Â min_count,Â ...]) Compute sum of group values. SeriesGroupBy.var ([ddof,Â engine,Â ...]) Compute variance of groups, excluding missing values. SeriesGroupBy.tail ([n]) Return last n rows of each group. SeriesGroupBy.take (indices[,Â axis]) Return the elements in the given positional indices in each group. SeriesGroupBy.value_counts ([normalize,Â ...]) Plotting and visualization # DataFrameGroupBy.boxplot ([subplots,Â column,Â ...]) Make box plots from DataFrameGroupBy data. DataFrameGroupBy.hist ([column,Â by,Â grid,Â ...]) Make a histogram of the DataFrame's columns. SeriesGroupBy.hist ([by,Â ax,Â grid,Â ...]) Draw histogram of the input series using matplotlib. DataFrameGroupBy.plot Make plots of Series or DataFrame. SeriesGroupBy.plot Make plots of Series or DataFrame. previous pandas.api.indexers.VariableOffsetWindowIndexer next pandas.core.groupby.DataFrameGroupBy.__iter__ On this page Indexing, iteration Function application helper Function application DataFrameGroupBy computations / descriptive stats SeriesGroupBy computations / descriptive stats Plotting and visualization Show Source",
    "url": "https://pandas.pydata.org/docs/reference/groupby.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Windowing operations Windowing operations # pandas contains a compact set of APIs for performing windowing operations - an operation that performs an aggregation over a sliding partition of values. The API functions similarly to the groupby API in that Series and DataFrame call the windowing method with necessary parameters and then subsequently call the aggregation function. In [1]: s = pd . Series ( range ( 5 )) In [2]: s . rolling ( window = 2 ) . sum () Out[2]: 0 NaN 1 1.0 2 3.0 3 5.0 4 7.0 dtype: float64 The windows are comprised by looking back the length of the window from the current observation. The result above can be derived by taking the sum of the following windowed partitions of data: In [3]: for window in s . rolling ( window = 2 ): ...: print ( window ) ...: 0 0 dtype: int64 0 0 1 1 dtype: int64 1 1 2 2 dtype: int64 2 2 3 3 dtype: int64 3 3 4 4 dtype: int64 Overview # pandas supports 4 types of windowing operations: Rolling window: Generic fixed or variable sliding window over the values. Weighted window: Weighted, non-rectangular window supplied by the scipy.signal library. Expanding window: Accumulating window over the values. Exponentially Weighted window: Accumulating and exponentially weighted window over the values. Concept Method Returned Object Supports time-based windows Supports chained groupby Supports table method Supports online operations Rolling window rolling pandas.typing.api.Rolling Yes Yes Yes (as of version 1.3) No Weighted window rolling pandas.typing.api.Window No No No No Expanding window expanding pandas.typing.api.Expanding No Yes Yes (as of version 1.3) No Exponentially Weighted window ewm pandas.typing.api.ExponentialMovingWindow No Yes (as of version 1.2) No Yes (as of version 1.3) As noted above, some operations support specifying a window based on a time offset: In [4]: s = pd . Series ( range ( 5 ), index = pd . date_range ( '2020-01-01' , periods = 5 , freq = '1D' )) In [5]: s . rolling ( window = '2D' ) . sum () Out[5]: 2020-01-01 0.0 2020-01-02 1.0 2020-01-03 3.0 2020-01-04 5.0 2020-01-05 7.0 Freq: D, dtype: float64 Additionally, some methods support chaining a groupby operation with a windowing operation which will first group the data by the specified keys and then perform a windowing operation per group. In [6]: df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'a' , 'b' , 'a' ], 'B' : range ( 5 )}) In [7]: df . groupby ( 'A' ) . expanding () . sum () Out[7]: B A a 0 0.0 2 2.0 4 6.0 b 1 1.0 3 4.0 Note Windowing operations currently only support numeric data (integer and float) and will always return float64 values. Warning Some windowing aggregation, mean , sum , var and std methods may suffer from numerical imprecision due to the underlying windowing algorithms accumulating sums. When values differ with magnitude \\(1/np.finfo(np.double).eps\\) this results in truncation. It must be noted, that large values may have an impact on windows, which do not include these values. Kahan summation is used to compute the rolling sums to preserve accuracy as much as possible. Added in version 1.3.0. Some windowing operations also support the method='table' option in the constructor which performs the windowing operation over an entire DataFrame instead of a single column or row at a time. This can provide a useful performance benefit for a DataFrame with many columns or rows (with the corresponding axis argument) or the ability to utilize other columns during the windowing operation. The method='table' option can only be used if engine='numba' is specified in the corresponding method call. For example, a weighted mean calculation can be calculated with apply() by specifying a separate column of weights. In [8]: def weighted_mean ( x ): ...: arr = np . ones (( 1 , x . shape [ 1 ])) ...: arr [:, : 2 ] = ( x [:, : 2 ] * x [:, 2 ]) . sum ( axis = 0 ) / x [:, 2 ] . sum () ...: return arr ...: In [9]: df = pd . DataFrame ([[ 1 , 2 , 0.6 ], [ 2 , 3 , 0.4 ], [ 3 , 4 , 0.2 ], [ 4 , 5 , 0.7 ]]) In [10]: df . rolling ( 2 , method = \"table\" , min_periods = 0 ) . apply ( weighted_mean , raw = True , engine = \"numba\" ) # noqa: E501 Out[10]: 0 1 2 0 1.000000 2.000000 1.0 1 1.800000 2.000000 1.0 2 3.333333 2.333333 1.0 3 1.555556 7.000000 1.0 Added in version 1.3. Some windowing operations also support an online method after constructing a windowing object which returns a new object that supports passing in new DataFrame or Series objects to continue the windowing calculation with the new values (i.e. online calculations). The methods on this new windowing objects must call the aggregation method first to âprimeâ the initial state of the online calculation. Then, new DataFrame or Series objects can be passed in the update argument to continue the windowing calculation. In [11]: df = pd . DataFrame ([[ 1 , 2 , 0.6 ], [ 2 , 3 , 0.4 ], [ 3 , 4 , 0.2 ], [ 4 , 5 , 0.7 ]]) In [12]: df . ewm ( 0.5 ) . mean () Out[12]: 0 1 2 0 1.000000 2.000000 0.600000 1 1.750000 2.750000 0.450000 2 2.615385 3.615385 0.276923 3 3.550000 4.550000 0.562500 In [13]: online_ewm = df . head ( 2 ) . ewm ( 0.5 ) . online () In [14]: online_ewm . mean () Out[14]: 0 1 2 0 1.00 2.00 0.60 1 1.75 2.75 0.45 In [15]: online_ewm . mean ( update = df . tail ( 1 )) Out[15]: 0 1 2 3 3.307692 4.307692 0.623077 All windowing operations support a min_periods argument that dictates the minimum amount of non- np.nan values a window must have; otherwise, the resulting value is np.nan . min_periods defaults to 1 for time-based windows and window for fixed windows In [16]: s = pd . Series ([ np . nan , 1 , 2 , np . nan , np . nan , 3 ]) In [17]: s . rolling ( window = 3 , min_periods = 1 ) . sum () Out[17]: 0 NaN 1 1.0 2 3.0 3 3.0 4 2.0 5 3.0 dtype: float64 In [18]: s . rolling ( window = 3 , min_periods = 2 ) . sum () Out[18]: 0 NaN 1 NaN 2 3.0 3 3.0 4 NaN 5 NaN dtype: float64 # Equivalent to min_periods=3 In [19]: s . rolling ( window = 3 , min_periods = None ) . sum () Out[19]: 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN dtype: float64 Additionally, all windowing operations supports the aggregate method for returning a result of multiple aggregations applied to a window. In [20]: df = pd . DataFrame ({ \"A\" : range ( 5 ), \"B\" : range ( 10 , 15 )}) In [21]: df . expanding () . agg ([ \"sum\" , \"mean\" , \"std\" ]) Out[21]: A B sum mean std sum mean std 0 0.0 0.0 NaN 10.0 10.0 NaN 1 1.0 0.5 0.707107 21.0 10.5 0.707107 2 3.0 1.0 1.000000 33.0 11.0 1.000000 3 6.0 1.5 1.290994 46.0 11.5 1.290994 4 10.0 2.0 1.581139 60.0 12.0 1.581139 Rolling window # Generic rolling windows support specifying windows as a fixed number of observations or variable number of observations based on an offset. If a time based offset is provided, the corresponding time based index must be monotonic. In [22]: times = [ '2020-01-01' , '2020-01-03' , '2020-01-04' , '2020-01-05' , '2020-01-29' ] In [23]: s = pd . Series ( range ( 5 ), index = pd . DatetimeIndex ( times )) In [24]: s Out[24]: 2020-01-01 0 2020-01-03 1 2020-01-04 2 2020-01-05 3 2020-01-29 4 dtype: int64 # Window with 2 observations In [25]: s . rolling ( window = 2 ) . sum () Out[25]: 2020-01-01 NaN 2020-01-03 1.0 2020-01-04 3.0 2020-01-05 5.0 2020-01-29 7.0 dtype: float64 # Window with 2 days worth of observations In [26]: s . rolling ( window = '2D' ) . sum () Out[26]: 2020-01-01 0.0 2020-01-03 1.0 2020-01-04 3.0 2020-01-05 5.0 2020-01-29 4.0 dtype: float64 For all supported aggregation functions, see Rolling window functions . Centering windows # By default the labels are set to the right edge of the window, but a center keyword is available so the labels can be set at the center. In [27]: s = pd . Series ( range ( 10 )) In [28]: s . rolling ( window = 5 ) . mean () Out[28]: 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 In [29]: s . rolling ( window = 5 , center = True ) . mean () Out[29]: 0 NaN 1 NaN 2 2.0 3 3.0 4 4.0 5 5.0 6 6.0 7 7.0 8 NaN 9 NaN dtype: float64 This can also be applied to datetime-like indices. Added in version 1.3.0. In [30]: df = pd . DataFrame ( ....: { \"A\" : [ 0 , 1 , 2 , 3 , 4 ]}, index = pd . date_range ( \"2020\" , periods = 5 , freq = \"1D\" ) ....: ) ....: In [31]: df Out[31]: A 2020-01-01 0 2020-01-02 1 2020-01-03 2 2020-01-04 3 2020-01-05 4 In [32]: df . rolling ( \"2D\" , center = False ) . mean () Out[32]: A 2020-01-01 0.0 2020-01-02 0.5 2020-01-03 1.5 2020-01-04 2.5 2020-01-05 3.5 In [33]: df . rolling ( \"2D\" , center = True ) . mean () Out[33]: A 2020-01-01 0.5 2020-01-02 1.5 2020-01-03 2.5 2020-01-04 3.5 2020-01-05 4.0 Rolling window endpoints # The inclusion of the interval endpoints in rolling window calculations can be specified with the closed parameter: Value Behavior 'right' close right endpoint 'left' close left endpoint 'both' close both endpoints 'neither' open endpoints For example, having the right endpoint open is useful in many problems that require that there is no contamination from present information back to past information. This allows the rolling window to compute statistics âup to that point in timeâ, but not including that point in time. In [34]: df = pd . DataFrame ( ....: { \"x\" : 1 }, ....: index = [ ....: pd . Timestamp ( \"20130101 09:00:01\" ), ....: pd . Timestamp ( \"20130101 09:00:02\" ), ....: pd . Timestamp ( \"20130101 09:00:03\" ), ....: pd . Timestamp ( \"20130101 09:00:04\" ), ....: pd . Timestamp ( \"20130101 09:00:06\" ), ....: ], ....: ) ....: In [35]: df [ \"right\" ] = df . rolling ( \"2s\" , closed = \"right\" ) . x . sum () # default In [36]: df [ \"both\" ] = df . rolling ( \"2s\" , closed = \"both\" ) . x . sum () In [37]: df [ \"left\" ] = df . rolling ( \"2s\" , closed = \"left\" ) . x . sum () In [38]: df [ \"neither\" ] = df . rolling ( \"2s\" , closed = \"neither\" ) . x . sum () In [39]: df Out[39]: x right both left neither 2013-01-01 09:00:01 1 1.0 1.0 NaN NaN 2013-01-01 09:00:02 1 2.0 2.0 1.0 1.0 2013-01-01 09:00:03 1 2.0 3.0 2.0 1.0 2013-01-01 09:00:04 1 2.0 3.0 2.0 1.0 2013-01-01 09:00:06 1 1.0 2.0 1.0 NaN Custom window rolling # In addition to accepting an integer or offset as a window argument, rolling also accepts a BaseIndexer subclass that allows a user to define a custom method for calculating window bounds. The BaseIndexer subclass will need to define a get_window_bounds method that returns a tuple of two arrays, the first being the starting indices of the windows and second being the ending indices of the windows. Additionally, num_values , min_periods , center , closed and step will automatically be passed to get_window_bounds and the defined method must always accept these arguments. For example, if we have the following DataFrame In [40]: use_expanding = [ True , False , True , False , True ] In [41]: use_expanding Out[41]: [True, False, True, False, True] In [42]: df = pd . DataFrame ({ \"values\" : range ( 5 )}) In [43]: df Out[43]: values 0 0 1 1 2 2 3 3 4 4 and we want to use an expanding window where use_expanding is True otherwise a window of size 1, we can create the following BaseIndexer subclass: In [44]: from pandas.api.indexers import BaseIndexer In [45]: class CustomIndexer ( BaseIndexer ): ....: def get_window_bounds ( self , num_values , min_periods , center , closed , step ): ....: start = np . empty ( num_values , dtype = np . int64 ) ....: end = np . empty ( num_values , dtype = np . int64 ) ....: for i in range ( num_values ): ....: if self . use_expanding [ i ]: ....: start [ i ] = 0 ....: end [ i ] = i + 1 ....: else : ....: start [ i ] = i ....: end [ i ] = i + self . window_size ....: return start , end ....: In [46]: indexer = CustomIndexer ( window_size = 1 , use_expanding = use_expanding ) In [47]: df . rolling ( indexer ) . sum () Out[47]: values 0 0.0 1 1.0 2 3.0 3 3.0 4 10.0 You can view other examples of BaseIndexer subclasses here One subclass of note within those examples is the VariableOffsetWindowIndexer that allows rolling operations over a non-fixed offset like a BusinessDay . In [48]: from pandas.api.indexers import VariableOffsetWindowIndexer In [49]: df = pd . DataFrame ( range ( 10 ), index = pd . date_range ( \"2020\" , periods = 10 )) In [50]: offset = pd . offsets . BDay ( 1 ) In [51]: indexer = VariableOffsetWindowIndexer ( index = df . index , offset = offset ) In [52]: df Out[52]: 0 2020-01-01 0 2020-01-02 1 2020-01-03 2 2020-01-04 3 2020-01-05 4 2020-01-06 5 2020-01-07 6 2020-01-08 7 2020-01-09 8 2020-01-10 9 In [53]: df . rolling ( indexer ) . sum () Out[53]: 0 2020-01-01 0.0 2020-01-02 1.0 2020-01-03 2.0 2020-01-04 3.0 2020-01-05 7.0 2020-01-06 12.0 2020-01-07 6.0 2020-01-08 7.0 2020-01-09 8.0 2020-01-10 9.0 For some problems knowledge of the future is available for analysis. For example, this occurs when each data point is a full time series read from an experiment, and the task is to extract underlying conditions. In these cases it can be useful to perform forward-looking rolling window computations. FixedForwardWindowIndexer class is available for this purpose. This BaseIndexer subclass implements a closed fixed-width forward-looking rolling window, and we can use it as follows: In [54]: from pandas.api.indexers import FixedForwardWindowIndexer In [55]: indexer = FixedForwardWindowIndexer ( window_size = 2 ) In [56]: df . rolling ( indexer , min_periods = 1 ) . sum () Out[56]: 0 2020-01-01 1.0 2020-01-02 3.0 2020-01-03 5.0 2020-01-04 7.0 2020-01-05 9.0 2020-01-06 11.0 2020-01-07 13.0 2020-01-08 15.0 2020-01-09 17.0 2020-01-10 9.0 We can also achieve this by using slicing, applying rolling aggregation, and then flipping the result as shown in example below: In [57]: df = pd . DataFrame ( ....: data = [ ....: [ pd . Timestamp ( \"2018-01-01 00:00:00\" ), 100 ], ....: [ pd . Timestamp ( \"2018-01-01 00:00:01\" ), 101 ], ....: [ pd . Timestamp ( \"2018-01-01 00:00:03\" ), 103 ], ....: [ pd . Timestamp ( \"2018-01-01 00:00:04\" ), 111 ], ....: ], ....: columns = [ \"time\" , \"value\" ], ....: ) . set_index ( \"time\" ) ....: In [58]: df Out[58]: value time 2018-01-01 00:00:00 100 2018-01-01 00:00:01 101 2018-01-01 00:00:03 103 2018-01-01 00:00:04 111 In [59]: reversed_df = df [:: - 1 ] . rolling ( \"2s\" ) . sum ()[:: - 1 ] In [60]: reversed_df Out[60]: value time 2018-01-01 00:00:00 201.0 2018-01-01 00:00:01 101.0 2018-01-01 00:00:03 214.0 2018-01-01 00:00:04 111.0 Rolling apply # The apply() function takes an extra func argument and performs generic rolling computations. The func argument should be a single function that produces a single value from an ndarray input. raw specifies whether the windows are cast as Series objects ( raw=False ) or ndarray objects ( raw=True ). In [61]: def mad ( x ): ....: return np . fabs ( x - x . mean ()) . mean () ....: In [62]: s = pd . Series ( range ( 10 )) In [63]: s . rolling ( window = 4 ) . apply ( mad , raw = True ) Out[63]: 0 NaN 1 NaN 2 NaN 3 1.0 4 1.0 5 1.0 6 1.0 7 1.0 8 1.0 9 1.0 dtype: float64 Numba engine # Additionally, apply() can leverage Numba if installed as an optional dependency. The apply aggregation can be executed using Numba by specifying engine='numba' and engine_kwargs arguments ( raw must also be set to True ). See enhancing performance with Numba for general usage of the arguments and performance considerations. Numba will be applied in potentially two routines: If func is a standard Python function, the engine will JIT the passed function. func can also be a JITed function in which case the engine will not JIT the function again. The engine will JIT the for loop where the apply function is applied to each window. The engine_kwargs argument is a dictionary of keyword arguments that will be passed into the numba.jit decorator . These keyword arguments will be applied to both the passed function (if a standard Python function) and the apply for loop over each window. Added in version 1.3.0. mean , median , max , min , and sum also support the engine and engine_kwargs arguments. Binary window functions # cov() and corr() can compute moving window statistics about two Series or any combination of DataFrame / Series or DataFrame / DataFrame . Here is the behavior in each case: two Series : compute the statistic for the pairing. DataFrame / Series : compute the statistics for each column of the DataFrame with the passed Series, thus returning a DataFrame. DataFrame / DataFrame : by default compute the statistic for matching column names, returning a DataFrame. If the keyword argument pairwise=True is passed then computes the statistic for each pair of columns, returning a DataFrame with a MultiIndex whose values are the dates in question (see the next section ). For example: In [64]: df = pd . DataFrame ( ....: np . random . randn ( 10 , 4 ), ....: index = pd . date_range ( \"2020-01-01\" , periods = 10 ), ....: columns = [ \"A\" , \"B\" , \"C\" , \"D\" ], ....: ) ....: In [65]: df = df . cumsum () In [66]: df2 = df [: 4 ] In [67]: df2 . rolling ( window = 2 ) . corr ( df2 [ \"B\" ]) Out[67]: A B C D 2020-01-01 NaN NaN NaN NaN 2020-01-02 -1.0 1.0 -1.0 1.0 2020-01-03 1.0 1.0 1.0 -1.0 2020-01-04 -1.0 1.0 1.0 -1.0 Computing rolling pairwise covariances and correlations # In financial data analysis and other fields itâs common to compute covariance and correlation matrices for a collection of time series. Often one is also interested in moving-window covariance and correlation matrices. This can be done by passing the pairwise keyword argument, which in the case of DataFrame inputs will yield a MultiIndexed DataFrame whose index are the dates in question. In the case of a single DataFrame argument the pairwise argument can even be omitted: Note Missing values are ignored and each entry is computed using the pairwise complete observations. Assuming the missing data are missing at random this results in an estimate for the covariance matrix which is unbiased. However, for many applications this estimate may not be acceptable because the estimated covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimated correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices for more details. In [68]: covs = ( ....: df [[ \"B\" , \"C\" , \"D\" ]] ....: . rolling ( window = 4 ) ....: . cov ( df [[ \"A\" , \"B\" , \"C\" ]], pairwise = True ) ....: ) ....: In [69]: covs Out[69]: B C D 2020-01-01 A NaN NaN NaN B NaN NaN NaN C NaN NaN NaN 2020-01-02 A NaN NaN NaN B NaN NaN NaN ... ... ... ... 2020-01-09 B 0.342006 0.230190 0.052849 C 0.230190 1.575251 0.082901 2020-01-10 A -0.333945 0.006871 -0.655514 B 0.649711 0.430860 0.469271 C 0.430860 0.829721 0.055300 [30 rows x 3 columns] Weighted window # The win_type argument in .rolling generates a weighted windows that are commonly used in filtering and spectral estimation. win_type must be string that corresponds to a scipy.signal window function . Scipy must be installed in order to use these windows, and supplementary arguments that the Scipy window methods take must be specified in the aggregation function. In [70]: s = pd . Series ( range ( 10 )) In [71]: s . rolling ( window = 5 ) . mean () Out[71]: 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 In [72]: s . rolling ( window = 5 , win_type = \"triang\" ) . mean () Out[72]: 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 # Supplementary Scipy arguments passed in the aggregation function In [73]: s . rolling ( window = 5 , win_type = \"gaussian\" ) . mean ( std = 0.1 ) Out[73]: 0 NaN 1 NaN 2 NaN 3 NaN 4 2.0 5 3.0 6 4.0 7 5.0 8 6.0 9 7.0 dtype: float64 For all supported aggregation functions, see Weighted window functions . Expanding window # An expanding window yields the value of an aggregation statistic with all the data available up to that point in time. Since these calculations are a special case of rolling statistics, they are implemented in pandas such that the following two calls are equivalent: In [74]: df = pd . DataFrame ( range ( 5 )) In [75]: df . rolling ( window = len ( df ), min_periods = 1 ) . mean () Out[75]: 0 0 0.0 1 0.5 2 1.0 3 1.5 4 2.0 In [76]: df . expanding ( min_periods = 1 ) . mean () Out[76]: 0 0 0.0 1 0.5 2 1.0 3 1.5 4 2.0 For all supported aggregation functions, see Expanding window functions . Exponentially weighted window # An exponentially weighted window is similar to an expanding window but with each prior point being exponentially weighted down relative to the current point. In general, a weighted moving average is calculated as \\[y_t = \\frac{\\sum_{i=0}^t w_i x_{t-i}}{\\sum_{i=0}^t w_i},\\] where \\(x_t\\) is the input, \\(y_t\\) is the result and the \\(w_i\\) are the weights. For all supported aggregation functions, see Exponentially-weighted window functions . The EW functions support two variants of exponential weights. The default, adjust=True , uses the weights \\(w_i = (1 - \\alpha)^i\\) which gives \\[y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ... + (1 - \\alpha)^t x_{0}}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^t}\\] When adjust=False is specified, moving averages are calculated as \\[\\begin{split}y_0 &= x_0 \\\\ y_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t,\\end{split}\\] which is equivalent to using weights \\[\\begin{split}w_i = \\begin{cases} \\alpha (1 - \\alpha)^i & \\text{if } i < t \\\\ (1 - \\alpha)^i & \\text{if } i = t. \\end{cases}\\end{split}\\] Note These equations are sometimes written in terms of \\(\\alpha' = 1 - \\alpha\\) , e.g. \\[y_t = \\alpha' y_{t-1} + (1 - \\alpha') x_t.\\] The difference between the above two variants arises because we are dealing with series which have finite history. Consider a series of infinite history, with adjust=True : \\[y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ...} {1 + (1 - \\alpha) + (1 - \\alpha)^2 + ...}\\] Noting that the denominator is a geometric series with initial term equal to 1 and a ratio of \\(1 - \\alpha\\) we have \\[\\begin{split}y_t &= \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ...} {\\frac{1}{1 - (1 - \\alpha)}}\\\\ &= [x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ...] \\alpha \\\\ &= \\alpha x_t + [(1-\\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ...]\\alpha \\\\ &= \\alpha x_t + (1 - \\alpha)[x_{t-1} + (1 - \\alpha) x_{t-2} + ...]\\alpha\\\\ &= \\alpha x_t + (1 - \\alpha) y_{t-1}\\end{split}\\] which is the same expression as adjust=False above and therefore shows the equivalence of the two variants for infinite series. When adjust=False , we have \\(y_0 = x_0\\) and \\(y_t = \\alpha x_t + (1 - \\alpha) y_{t-1}\\) . Therefore, there is an assumption that \\(x_0\\) is not an ordinary value but rather an exponentially weighted moment of the infinite series up to that point. One must have \\(0 < \\alpha \\leq 1\\) , and while it is possible to pass \\(\\alpha\\) directly, itâs often easier to think about either the span , center of mass (com) or half-life of an EW moment: \\[\\begin{split}\\alpha = \\begin{cases} \\frac{2}{s + 1}, & \\text{for span}\\ s \\geq 1\\\\ \\frac{1}{1 + c}, & \\text{for center of mass}\\ c \\geq 0\\\\ 1 - \\exp^{\\frac{\\log 0.5}{h}}, & \\text{for half-life}\\ h > 0 \\end{cases}\\end{split}\\] One must specify precisely one of span , center of mass , half-life and alpha to the EW functions: Span corresponds to what is commonly called an âN-day EW moving averageâ. Center of mass has a more physical interpretation and can be thought of in terms of span: \\(c = (s - 1) / 2\\) . Half-life is the period of time for the exponential weight to reduce to one half. Alpha specifies the smoothing factor directly. You can also specify halflife in terms of a timedelta convertible unit to specify the amount of time it takes for an observation to decay to half its value when also specifying a sequence of times . In [77]: df = pd . DataFrame ({ \"B\" : [ 0 , 1 , 2 , np . nan , 4 ]}) In [78]: df Out[78]: B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 In [79]: times = [ \"2020-01-01\" , \"2020-01-03\" , \"2020-01-10\" , \"2020-01-15\" , \"2020-01-17\" ] In [80]: df . ewm ( halflife = \"4 days\" , times = pd . DatetimeIndex ( times )) . mean () Out[80]: B 0 0.000000 1 0.585786 2 1.523889 3 1.523889 4 3.233686 The following formula is used to compute exponentially weighted mean with an input vector of times: \\[y_t = \\frac{\\sum_{i=0}^t 0.5^\\frac{t_{t} - t_{i}}{\\lambda} x_{t-i}}{\\sum_{i=0}^t 0.5^\\frac{t_{t} - t_{i}}{\\lambda}},\\] ExponentialMovingWindow also has an ignore_na argument, which determines how intermediate null values affect the calculation of the weights. When ignore_na=False (the default), weights are calculated based on absolute positions, so that intermediate null values affect the result. When ignore_na=True , weights are calculated by ignoring intermediate null values. For example, assuming adjust=True , if ignore_na=False , the weighted average of 3, NaN, 5 would be calculated as \\[\\frac{(1-\\alpha)^2 \\cdot 3 + 1 \\cdot 5}{(1-\\alpha)^2 + 1}.\\] Whereas if ignore_na=True , the weighted average would be calculated as \\[\\frac{(1-\\alpha) \\cdot 3 + 1 \\cdot 5}{(1-\\alpha) + 1}.\\] The var() , std() , and cov() functions have a bias argument, specifying whether the result should contain biased or unbiased statistics. For example, if bias=True , ewmvar(x) is calculated as ewmvar(x) = ewma(x**2) - ewma(x)**2 ; whereas if bias=False (the default), the biased variance statistics are scaled by debiasing factors \\[\\frac{\\left(\\sum_{i=0}^t w_i\\right)^2}{\\left(\\sum_{i=0}^t w_i\\right)^2 - \\sum_{i=0}^t w_i^2}.\\] (For \\(w_i = 1\\) , this reduces to the usual \\(N / (N - 1)\\) factor, with \\(N = t + 1\\) .) See Weighted Sample Variance on Wikipedia for further details. previous Group by: split-apply-combine next Time series / date functionality On this page Overview Rolling window Centering windows Rolling window endpoints Custom window rolling Rolling apply Numba engine Binary window functions Computing rolling pairwise covariances and correlations Weighted window Expanding window Exponentially weighted window Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/window.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Scaling to... Scaling to large datasets # pandas provides data structures for in-memory analytics, which makes using pandas to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets that are a sizable fraction of memory become unwieldy, as some pandas operations need to make intermediate copies. This document provides a few recommendations for scaling your analysis to larger datasets. Itâs a complement to Enhancing performance , which focuses on speeding up analysis for datasets that fit in memory. Load less data # Suppose our raw dataset on disk has many columns. In [1]: import pandas as pd In [2]: import numpy as np In [3]: def make_timeseries ( start = \"2000-01-01\" , end = \"2000-12-31\" , freq = \"1D\" , seed = None ): ...: index = pd . date_range ( start = start , end = end , freq = freq , name = \"timestamp\" ) ...: n = len ( index ) ...: state = np . random . RandomState ( seed ) ...: columns = { ...: \"name\" : state . choice ([ \"Alice\" , \"Bob\" , \"Charlie\" ], size = n ), ...: \"id\" : state . poisson ( 1000 , size = n ), ...: \"x\" : state . rand ( n ) * 2 - 1 , ...: \"y\" : state . rand ( n ) * 2 - 1 , ...: } ...: df = pd . DataFrame ( columns , index = index , columns = sorted ( columns )) ...: if df . index [ - 1 ] == end : ...: df = df . iloc [: - 1 ] ...: return df ...: In [4]: timeseries = [ ...: make_timeseries ( freq = \"1min\" , seed = i ) . rename ( columns = lambda x : f \" { x } _ { i } \" ) ...: for i in range ( 10 ) ...: ] ...: In [5]: ts_wide = pd . concat ( timeseries , axis = 1 ) In [6]: ts_wide . head () Out[6]: id_0 name_0 x_0 ... name_9 x_9 y_9 timestamp ... 2000-01-01 00:00:00 977 Alice -0.821225 ... Charlie -0.957208 -0.757508 2000-01-01 00:01:00 1018 Bob -0.219182 ... Alice -0.414445 -0.100298 2000-01-01 00:02:00 927 Alice 0.660908 ... Charlie -0.325838 0.581859 2000-01-01 00:03:00 997 Bob -0.852458 ... Bob 0.992033 -0.686692 2000-01-01 00:04:00 965 Bob 0.717283 ... Charlie -0.924556 -0.184161 [5 rows x 40 columns] In [7]: ts_wide . to_parquet ( \"timeseries_wide.parquet\" ) To load the columns we want, we have two options. Option 1 loads in all the data and then filters to what we need. In [8]: columns = [ \"id_0\" , \"name_0\" , \"x_0\" , \"y_0\" ] In [9]: pd . read_parquet ( \"timeseries_wide.parquet\" )[ columns ] Out[9]: id_0 name_0 x_0 y_0 timestamp 2000-01-01 00:00:00 977 Alice -0.821225 0.906222 2000-01-01 00:01:00 1018 Bob -0.219182 0.350855 2000-01-01 00:02:00 927 Alice 0.660908 -0.798511 2000-01-01 00:03:00 997 Bob -0.852458 0.735260 2000-01-01 00:04:00 965 Bob 0.717283 0.393391 ... ... ... ... ... 2000-12-30 23:56:00 1037 Bob -0.814321 0.612836 2000-12-30 23:57:00 980 Bob 0.232195 -0.618828 2000-12-30 23:58:00 965 Alice -0.231131 0.026310 2000-12-30 23:59:00 984 Alice 0.942819 0.853128 2000-12-31 00:00:00 1003 Alice 0.201125 -0.136655 [525601 rows x 4 columns] Option 2 only loads the columns we request. In [10]: pd . read_parquet ( \"timeseries_wide.parquet\" , columns = columns ) Out[10]: id_0 name_0 x_0 y_0 timestamp 2000-01-01 00:00:00 977 Alice -0.821225 0.906222 2000-01-01 00:01:00 1018 Bob -0.219182 0.350855 2000-01-01 00:02:00 927 Alice 0.660908 -0.798511 2000-01-01 00:03:00 997 Bob -0.852458 0.735260 2000-01-01 00:04:00 965 Bob 0.717283 0.393391 ... ... ... ... ... 2000-12-30 23:56:00 1037 Bob -0.814321 0.612836 2000-12-30 23:57:00 980 Bob 0.232195 -0.618828 2000-12-30 23:58:00 965 Alice -0.231131 0.026310 2000-12-30 23:59:00 984 Alice 0.942819 0.853128 2000-12-31 00:00:00 1003 Alice 0.201125 -0.136655 [525601 rows x 4 columns] If we were to measure the memory usage of the two calls, weâd see that specifying columns uses about 1/10th the memory in this case. With pandas.read_csv() , you can specify usecols to limit the columns read into memory. Not all file formats that can be read by pandas provide an option to read a subset of columns. Use efficient datatypes # The default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values (commonly referred to as âlow-cardinalityâ data). By using more efficient data types, you can store larger datasets in memory. In [11]: ts = make_timeseries ( freq = \"30s\" , seed = 0 ) In [12]: ts . to_parquet ( \"timeseries.parquet\" ) In [13]: ts = pd . read_parquet ( \"timeseries.parquet\" ) In [14]: ts Out[14]: id name x y timestamp 2000-01-01 00:00:00 1041 Alice 0.889987 0.281011 2000-01-01 00:00:30 988 Bob -0.455299 0.488153 2000-01-01 00:01:00 1018 Alice 0.096061 0.580473 2000-01-01 00:01:30 992 Bob 0.142482 0.041665 2000-01-01 00:02:00 960 Bob -0.036235 0.802159 ... ... ... ... ... 2000-12-30 23:58:00 1022 Alice 0.266191 0.875579 2000-12-30 23:58:30 974 Alice -0.009826 0.413686 2000-12-30 23:59:00 1028 Charlie 0.307108 -0.656789 2000-12-30 23:59:30 1002 Alice 0.202602 0.541335 2000-12-31 00:00:00 987 Alice 0.200832 0.615972 [1051201 rows x 4 columns] Now, letâs inspect the data types and memory usage to see where we should focus our attention. In [15]: ts . dtypes Out[15]: id int64 name object x float64 y float64 dtype: object In [16]: ts . memory_usage ( deep = True ) # memory usage in bytes Out[16]: Index 8409608 id 8409608 name 65176434 x 8409608 y 8409608 dtype: int64 The name column is taking up much more memory than any other. It has just a few unique values, so itâs a good candidate for converting to a pandas.Categorical . With a pandas.Categorical , we store each unique name once and use space-efficient integers to know which specific name is used in each row. In [17]: ts2 = ts . copy () In [18]: ts2 [ \"name\" ] = ts2 [ \"name\" ] . astype ( \"category\" ) In [19]: ts2 . memory_usage ( deep = True ) Out[19]: Index 8409608 id 8409608 name 1051495 x 8409608 y 8409608 dtype: int64 We can go a bit further and downcast the numeric columns to their smallest types using pandas.to_numeric() . In [20]: ts2 [ \"id\" ] = pd . to_numeric ( ts2 [ \"id\" ], downcast = \"unsigned\" ) In [21]: ts2 [[ \"x\" , \"y\" ]] = ts2 [[ \"x\" , \"y\" ]] . apply ( pd . to_numeric , downcast = \"float\" ) In [22]: ts2 . dtypes Out[22]: id uint16 name category x float32 y float32 dtype: object In [23]: ts2 . memory_usage ( deep = True ) Out[23]: Index 8409608 id 2102402 name 1051495 x 4204804 y 4204804 dtype: int64 In [24]: reduction = ts2 . memory_usage ( deep = True ) . sum () / ts . memory_usage ( deep = True ) . sum () In [25]: print ( f \" { reduction : 0.2f } \" ) 0.20 In all, weâve reduced the in-memory footprint of this dataset to 1/5 of its original size. See Categorical data for more on pandas.Categorical and dtypes for an overview of all of pandasâ dtypes. Use chunking # Some workloads can be achieved with chunking by splitting a large problem into a bunch of small problems. For example, converting an individual CSV file into a Parquet file and repeating that for each file in a directory. As long as each chunk fits in memory, you can work with datasets that are much larger than memory. Note Chunking works well when the operation youâre performing requires zero or minimal coordination between chunks. For more complicated workflows, youâre better off using other libraries . Suppose we have an even larger âlogical datasetâ on disk thatâs a directory of parquet files. Each file in the directory represents a different year of the entire dataset. In [26]: import pathlib In [27]: N = 12 In [28]: starts = [ f \"20 { i : >02d } -01-01\" for i in range ( N )] In [29]: ends = [ f \"20 { i : >02d } -12-13\" for i in range ( N )] In [30]: pathlib . Path ( \"data/timeseries\" ) . mkdir ( exist_ok = True ) In [31]: for i , ( start , end ) in enumerate ( zip ( starts , ends )): ....: ts = make_timeseries ( start = start , end = end , freq = \"1min\" , seed = i ) ....: ts . to_parquet ( f \"data/timeseries/ts- { i : 0>2d } .parquet\" ) ....: data âââ timeseries âââ ts-00.parquet âââ ts-01.parquet âââ ts-02.parquet âââ ts-03.parquet âââ ts-04.parquet âââ ts-05.parquet âââ ts-06.parquet âââ ts-07.parquet âââ ts-08.parquet âââ ts-09.parquet âââ ts-10.parquet âââ ts-11.parquet Now weâll implement an out-of-core pandas.Series.value_counts() . The peak memory usage of this workflow is the single largest chunk, plus a small series storing the unique value counts up to this point. As long as each individual file fits in memory, this will work for arbitrary-sized datasets. In [32]: %%time ....: files = pathlib . Path ( \"data/timeseries/\" ) . glob ( \"ts*.parquet\" ) ....: counts = pd . Series ( dtype = int ) ....: for path in files : ....: df = pd . read_parquet ( path ) ....: counts = counts . add ( df [ \"name\" ] . value_counts (), fill_value = 0 ) ....: counts . astype ( int ) ....: CPU times: user 1.01 s, sys: 116 ms, total: 1.13 s Wall time: 1.05 s Out[32]: name Alice 1994645 Bob 1993692 Charlie 1994875 dtype: int64 Some readers, like pandas.read_csv() , offer parameters to control the chunksize when reading a single file. Manually chunking is an OK option for workflows that donât require too sophisticated of operations. Some operations, like pandas.DataFrame.groupby() , are much harder to do chunkwise. In these cases, you may be better switching to a different library that implements these out-of-core algorithms for you. Use Other Libraries # There are other libraries which provide similar APIs to pandas and work nicely with pandas DataFrame, and can give you the ability to scale your large dataset processing and analytics by parallel runtime, distributed memory, clustering, etc. You can find more information in the ecosystem page . previous Enhancing performance next Sparse data structures On this page Load less data Use efficient datatypes Use chunking Use Other Libraries Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/scale.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Nullable... Nullable Boolean data type # Note BooleanArray is currently experimental. Its API or implementation may change without warning. Indexing with NA values # pandas allows indexing with NA values in a boolean array, which are treated as False . In [1]: s = pd . Series ([ 1 , 2 , 3 ]) In [2]: mask = pd . array ([ True , False , pd . NA ], dtype = \"boolean\" ) In [3]: s [ mask ] Out[3]: 0 1 dtype: int64 If you would prefer to keep the NA values you can manually fill them with fillna(True) . In [4]: s [ mask . fillna ( True )] Out[4]: 0 1 2 3 dtype: int64 Kleene logical operations # arrays.BooleanArray implements Kleene Logic (sometimes called three-value logic) for logical operations like & (and), | (or) and ^ (exclusive-or). This table demonstrates the results for every combination. These operations are symmetrical, so flipping the left- and right-hand side makes no difference in the result. Expression Result True & True True True & False False True & NA NA False & False False False & NA False NA & NA NA True | True True True | False True True | NA True False | False False False | NA NA NA | NA NA True ^ True False True ^ False True True ^ NA NA False ^ False False False ^ NA NA NA ^ NA NA When an NA is present in an operation, the output value is NA only if the result cannot be determined solely based on the other input. For example, True | NA is True , because both True | True and True | False are True . In that case, we donât actually need to consider the value of the NA . On the other hand, True & NA is NA . The result depends on whether the NA really is True or False , since True & True is True , but True & False is False , so we canât determine the output. This differs from how np.nan behaves in logical operations. pandas treated np.nan is always false in the output . In or In [5]: pd . Series ([ True , False , np . nan ], dtype = \"object\" ) | True Out[5]: 0 True 1 True 2 False dtype: bool In [6]: pd . Series ([ True , False , np . nan ], dtype = \"boolean\" ) | True Out[6]: 0 True 1 True 2 True dtype: boolean In and In [7]: pd . Series ([ True , False , np . nan ], dtype = \"object\" ) & True Out[7]: 0 True 1 False 2 False dtype: bool In [8]: pd . Series ([ True , False , np . nan ], dtype = \"boolean\" ) & True Out[8]: 0 True 1 False 2 <NA> dtype: boolean previous Nullable integer data type next Chart visualization On this page Indexing with NA values Kleene logical operations Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/boolean.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.parallel_coordinates # pandas.plotting. parallel_coordinates ( frame , class_column , cols = None , ax = None , color = None , use_columns = False , xticks = None , colormap = None , axvlines = True , axvlines_kwds = None , sort_labels = False , ** kwargs ) [source] # Parallel coordinates plotting. Parameters : frame DataFrame class_column str Column name containing class names. cols list, optional A list of column names to use. ax matplotlib.axis, optional Matplotlib axis object. color list or tuple, optional Colors to use for the different classes. use_columns bool, optional If true, columns will be used as xticks. xticks list or tuple, optional A list of values to use for xticks. colormap str or matplotlib colormap, default None Colormap to use for line colors. axvlines bool, optional If true, vertical lines will be added at each xtick. axvlines_kwds keywords, optional Options to be passed to axvline method for vertical lines. sort_labels bool, default False Sort class_column labels, useful when assigning colors. **kwargs Options to pass to matplotlib plotting method. Returns : matplotlib.axes.Axes Examples >>> df = pd . read_csv ( ... 'https://raw.githubusercontent.com/pandas-dev/' ... 'pandas/main/pandas/tests/io/data/csv/iris.csv' ... ) >>> pd . plotting . parallel_coordinates ( ... df , 'Name' , color = ( '#556270' , '#4ECDC4' , '#C7F464' ) ... ) previous pandas.plotting.lag_plot next pandas.plotting.plot_params On this page parallel_coordinates() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide MultiIndex... MultiIndex / advanced indexing # This section covers indexing with a MultiIndex and other advanced indexing features . See the Indexing and Selecting Data for general indexing documentation. Warning Whether a copy or a reference is returned for a setting operation may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy . See the cookbook for some advanced strategies. Hierarchical indexing (MultiIndex) # Hierarchical / Multi-level indexing is very exciting as it opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d). In this section, we will show what exactly we mean by âhierarchicalâ indexing and how it integrates with all of the pandas indexing functionality described above and in prior sections. Later, when discussing group by and pivoting and reshaping data , weâll show non-trivial applications to illustrate how it aids in structuring data for analysis. See the cookbook for some advanced strategies. Creating a MultiIndex (hierarchical index) object # The MultiIndex object is the hierarchical analogue of the standard Index object which typically stores the axis labels in pandas objects. You can think of MultiIndex as an array of tuples where each tuple is unique. A MultiIndex can be created from a list of arrays (using MultiIndex.from_arrays() ), an array of tuples (using MultiIndex.from_tuples() ), a crossed set of iterables (using MultiIndex.from_product() ), or a DataFrame (using MultiIndex.from_frame() ). The Index constructor will attempt to return a MultiIndex when it is passed a list of tuples. The following examples demonstrate different ways to initialize MultiIndexes. In [1]: arrays = [ ...: [ \"bar\" , \"bar\" , \"baz\" , \"baz\" , \"foo\" , \"foo\" , \"qux\" , \"qux\" ], ...: [ \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" ], ...: ] ...: In [2]: tuples = list ( zip ( * arrays )) In [3]: tuples Out[3]: [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')] In [4]: index = pd . MultiIndex . from_tuples ( tuples , names = [ \"first\" , \"second\" ]) In [5]: index Out[5]: MultiIndex([('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], names=['first', 'second']) In [6]: s = pd . Series ( np . random . randn ( 8 ), index = index ) In [7]: s Out[7]: first second bar one 0.469112 two -0.282863 baz one -1.509059 two -1.135632 foo one 1.212112 two -0.173215 qux one 0.119209 two -1.044236 dtype: float64 When you want every pairing of the elements in two iterables, it can be easier to use the MultiIndex.from_product() method: In [8]: iterables = [[ \"bar\" , \"baz\" , \"foo\" , \"qux\" ], [ \"one\" , \"two\" ]] In [9]: pd . MultiIndex . from_product ( iterables , names = [ \"first\" , \"second\" ]) Out[9]: MultiIndex([('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], names=['first', 'second']) You can also construct a MultiIndex from a DataFrame directly, using the method MultiIndex.from_frame() . This is a complementary method to MultiIndex.to_frame() . In [10]: df = pd . DataFrame ( ....: [[ \"bar\" , \"one\" ], [ \"bar\" , \"two\" ], [ \"foo\" , \"one\" ], [ \"foo\" , \"two\" ]], ....: columns = [ \"first\" , \"second\" ], ....: ) ....: In [11]: pd . MultiIndex . from_frame ( df ) Out[11]: MultiIndex([('bar', 'one'), ('bar', 'two'), ('foo', 'one'), ('foo', 'two')], names=['first', 'second']) As a convenience, you can pass a list of arrays directly into Series or DataFrame to construct a MultiIndex automatically: In [12]: arrays = [ ....: np . array ([ \"bar\" , \"bar\" , \"baz\" , \"baz\" , \"foo\" , \"foo\" , \"qux\" , \"qux\" ]), ....: np . array ([ \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" ]), ....: ] ....: In [13]: s = pd . Series ( np . random . randn ( 8 ), index = arrays ) In [14]: s Out[14]: bar one -0.861849 two -2.104569 baz one -0.494929 two 1.071804 foo one 0.721555 two -0.706771 qux one -1.039575 two 0.271860 dtype: float64 In [15]: df = pd . DataFrame ( np . random . randn ( 8 , 4 ), index = arrays ) In [16]: df Out[16]: 0 1 2 3 bar one -0.424972 0.567020 0.276232 -1.087401 two -0.673690 0.113648 -1.478427 0.524988 baz one 0.404705 0.577046 -1.715002 -1.039268 two -0.370647 -1.157892 -1.344312 0.844885 foo one 1.075770 -0.109050 1.643563 -1.469388 two 0.357021 -0.674600 -1.776904 -0.968914 qux one -1.294524 0.413738 0.276662 -0.472035 two -0.013960 -0.362543 -0.006154 -0.923061 All of the MultiIndex constructors accept a names argument which stores string names for the levels themselves. If no names are provided, None will be assigned: In [17]: df . index . names Out[17]: FrozenList([None, None]) This index can back any axis of a pandas object, and the number of levels of the index is up to you: In [18]: df = pd . DataFrame ( np . random . randn ( 3 , 8 ), index = [ \"A\" , \"B\" , \"C\" ], columns = index ) In [19]: df Out[19]: first bar baz ... foo qux second one two one ... two one two A 0.895717 0.805244 -1.206412 ... 1.340309 -1.170299 -0.226169 B 0.410835 0.813850 0.132003 ... -1.187678 1.130127 -1.436737 C -1.413681 1.607920 1.024180 ... -2.211372 0.974466 -2.006747 [3 rows x 8 columns] In [20]: pd . DataFrame ( np . random . randn ( 6 , 6 ), index = index [: 6 ], columns = index [: 6 ]) Out[20]: first bar baz foo second one two one two one two first second bar one -0.410001 -0.078638 0.545952 -1.219217 -1.226825 0.769804 two -1.281247 -0.727707 -0.121306 -0.097883 0.695775 0.341734 baz one 0.959726 -1.110336 -0.619976 0.149748 -0.732339 0.687738 two 0.176444 0.403310 -0.154951 0.301624 -2.179861 -1.369849 foo one -0.954208 1.462696 -1.743161 -0.826591 -0.345352 1.314232 two 0.690579 0.995761 2.396780 0.014871 3.357427 -0.317441 Weâve âsparsifiedâ the higher levels of the indexes to make the console output a bit easier on the eyes. Note that how the index is displayed can be controlled using the multi_sparse option in pandas.set_options() : In [21]: with pd . option_context ( \"display.multi_sparse\" , False ): ....: df ....: Itâs worth keeping in mind that thereâs nothing preventing you from using tuples as atomic labels on an axis: In [22]: pd . Series ( np . random . randn ( 8 ), index = tuples ) Out[22]: (bar, one) -1.236269 (bar, two) 0.896171 (baz, one) -0.487602 (baz, two) -0.082240 (foo, one) -2.182937 (foo, two) 0.380396 (qux, one) 0.084844 (qux, two) 0.432390 dtype: float64 The reason that the MultiIndex matters is that it can allow you to do grouping, selection, and reshaping operations as we will describe below and in subsequent areas of the documentation. As you will see in later sections, you can find yourself working with hierarchically-indexed data without creating a MultiIndex explicitly yourself. However, when loading data from a file, you may wish to generate your own MultiIndex when preparing the data set. Reconstructing the level labels # The method get_level_values() will return a vector of the labels for each location at a particular level: In [23]: index . get_level_values ( 0 ) Out[23]: Index(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], dtype='object', name='first') In [24]: index . get_level_values ( \"second\" ) Out[24]: Index(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'], dtype='object', name='second') Basic indexing on axis with MultiIndex # One of the important features of hierarchical indexing is that you can select data by a âpartialâ label identifying a subgroup in the data. Partial selection âdropsâ levels of the hierarchical index in the result in a completely analogous way to selecting a column in a regular DataFrame: In [25]: df [ \"bar\" ] Out[25]: second one two A 0.895717 0.805244 B 0.410835 0.813850 C -1.413681 1.607920 In [26]: df [ \"bar\" , \"one\" ] Out[26]: A 0.895717 B 0.410835 C -1.413681 Name: (bar, one), dtype: float64 In [27]: df [ \"bar\" ][ \"one\" ] Out[27]: A 0.895717 B 0.410835 C -1.413681 Name: one, dtype: float64 In [28]: s [ \"qux\" ] Out[28]: one -1.039575 two 0.271860 dtype: float64 See Cross-section with hierarchical index for how to select on a deeper level. Defined levels # The MultiIndex keeps all the defined levels of an index, even if they are not actually used. When slicing an index, you may notice this. For example: In [29]: df . columns . levels # original MultiIndex Out[29]: FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']]) In [30]: df [[ \"foo\" , \"qux\" ]] . columns . levels # sliced Out[30]: FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']]) This is done to avoid a recomputation of the levels in order to make slicing highly performant. If you want to see only the used levels, you can use the get_level_values() method. In [31]: df [[ \"foo\" , \"qux\" ]] . columns . to_numpy () Out[31]: array([('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], dtype=object) # for a specific level In [32]: df [[ \"foo\" , \"qux\" ]] . columns . get_level_values ( 0 ) Out[32]: Index(['foo', 'foo', 'qux', 'qux'], dtype='object', name='first') To reconstruct the MultiIndex with only the used levels, the remove_unused_levels() method may be used. In [33]: new_mi = df [[ \"foo\" , \"qux\" ]] . columns . remove_unused_levels () In [34]: new_mi . levels Out[34]: FrozenList([['foo', 'qux'], ['one', 'two']]) Data alignment and using reindex # Operations between differently-indexed objects having MultiIndex on the axes will work as you expect; data alignment will work the same as an Index of tuples: In [35]: s + s [: - 2 ] Out[35]: bar one -1.723698 two -4.209138 baz one -0.989859 two 2.143608 foo one 1.443110 two -1.413542 qux one NaN two NaN dtype: float64 In [36]: s + s [:: 2 ] Out[36]: bar one -1.723698 two NaN baz one -0.989859 two NaN foo one 1.443110 two NaN qux one -2.079150 two NaN dtype: float64 The reindex() method of Series / DataFrames can be called with another MultiIndex , or even a list or array of tuples: In [37]: s . reindex ( index [: 3 ]) Out[37]: first second bar one -0.861849 two -2.104569 baz one -0.494929 dtype: float64 In [38]: s . reindex ([( \"foo\" , \"two\" ), ( \"bar\" , \"one\" ), ( \"qux\" , \"one\" ), ( \"baz\" , \"one\" )]) Out[38]: foo two -0.706771 bar one -0.861849 qux one -1.039575 baz one -0.494929 dtype: float64 Advanced indexing with hierarchical index # Syntactically integrating MultiIndex in advanced indexing with .loc is a bit challenging, but weâve made every effort to do so. In general, MultiIndex keys take the form of tuples. For example, the following works as you would expect: In [39]: df = df . T In [40]: df Out[40]: A B C first second bar one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 baz one -1.206412 0.132003 1.024180 two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 qux one -1.170299 1.130127 0.974466 two -0.226169 -1.436737 -2.006747 In [41]: df . loc [( \"bar\" , \"two\" )] Out[41]: A 0.805244 B 0.813850 C 1.607920 Name: (bar, two), dtype: float64 Note that df.loc['bar', 'two'] would also work in this example, but this shorthand notation can lead to ambiguity in general. If you also want to index a specific column with .loc , you must use a tuple like this: In [42]: df . loc [( \"bar\" , \"two\" ), \"A\" ] Out[42]: 0.8052440253863785 You donât have to specify all levels of the MultiIndex by passing only the first elements of the tuple. For example, you can use âpartialâ indexing to get all elements with bar in the first level as follows: In [43]: df . loc [ \"bar\" ] Out[43]: A B C second one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 This is a shortcut for the slightly more verbose notation df.loc[('bar',),] (equivalent to df.loc['bar',] in this example). âPartialâ slicing also works quite nicely. In [44]: df . loc [ \"baz\" : \"foo\" ] Out[44]: A B C first second baz one -1.206412 0.132003 1.024180 two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 You can slice with a ârangeâ of values, by providing a slice of tuples. In [45]: df . loc [( \"baz\" , \"two\" ):( \"qux\" , \"one\" )] Out[45]: A B C first second baz two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 qux one -1.170299 1.130127 0.974466 In [46]: df . loc [( \"baz\" , \"two\" ): \"foo\" ] Out[46]: A B C first second baz two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 Passing a list of labels or tuples works similar to reindexing: In [47]: df . loc [[( \"bar\" , \"two\" ), ( \"qux\" , \"one\" )]] Out[47]: A B C first second bar two 0.805244 0.813850 1.607920 qux one -1.170299 1.130127 0.974466 Note It is important to note that tuples and lists are not treated identically in pandas when it comes to indexing. Whereas a tuple is interpreted as one multi-level key, a list is used to specify several keys. Or in other words, tuples go horizontally (traversing levels), lists go vertically (scanning levels). Importantly, a list of tuples indexes several complete MultiIndex keys, whereas a tuple of lists refer to several values within a level: In [48]: s = pd . Series ( ....: [ 1 , 2 , 3 , 4 , 5 , 6 ], ....: index = pd . MultiIndex . from_product ([[ \"A\" , \"B\" ], [ \"c\" , \"d\" , \"e\" ]]), ....: ) ....: In [49]: s . loc [[( \"A\" , \"c\" ), ( \"B\" , \"d\" )]] # list of tuples Out[49]: A c 1 B d 5 dtype: int64 In [50]: s . loc [([ \"A\" , \"B\" ], [ \"c\" , \"d\" ])] # tuple of lists Out[50]: A c 1 d 2 B c 4 d 5 dtype: int64 Using slicers # You can slice a MultiIndex by providing multiple indexers. You can provide any of the selectors as if you are indexing by label, see Selection by Label , including slices, lists of labels, labels, and boolean indexers. You can use slice(None) to select all the contents of that level. You do not need to specify all the deeper levels, they will be implied as slice(None) . As usual, both sides of the slicers are included as this is label indexing. Warning You should specify all axes in the .loc specifier, meaning the indexer for the index and for the columns . There are some ambiguous cases where the passed indexer could be misinterpreted as indexing both axes, rather than into say the MultiIndex for the rows. You should do this: df . loc [( slice ( \"A1\" , \"A3\" ), ... ), :] # noqa: E999 You should not do this: df . loc [( slice ( \"A1\" , \"A3\" ), ... )] # noqa: E999 In [51]: def mklbl ( prefix , n ): ....: return [ \" %s%s \" % ( prefix , i ) for i in range ( n )] ....: In [52]: miindex = pd . MultiIndex . from_product ( ....: [ mklbl ( \"A\" , 4 ), mklbl ( \"B\" , 2 ), mklbl ( \"C\" , 4 ), mklbl ( \"D\" , 2 )] ....: ) ....: In [53]: micolumns = pd . MultiIndex . from_tuples ( ....: [( \"a\" , \"foo\" ), ( \"a\" , \"bar\" ), ( \"b\" , \"foo\" ), ( \"b\" , \"bah\" )], names = [ \"lvl0\" , \"lvl1\" ] ....: ) ....: In [54]: dfmi = ( ....: pd . DataFrame ( ....: np . arange ( len ( miindex ) * len ( micolumns )) . reshape ( ....: ( len ( miindex ), len ( micolumns )) ....: ), ....: index = miindex , ....: columns = micolumns , ....: ) ....: . sort_index () ....: . sort_index ( axis = 1 ) ....: ) ....: In [55]: dfmi Out[55]: lvl0 a b lvl1 bar foo bah foo A0 B0 C0 D0 1 0 3 2 D1 5 4 7 6 C1 D0 9 8 11 10 D1 13 12 15 14 C2 D0 17 16 19 18 ... ... ... ... ... A3 B1 C1 D1 237 236 239 238 C2 D0 241 240 243 242 D1 245 244 247 246 C3 D0 249 248 251 250 D1 253 252 255 254 [64 rows x 4 columns] Basic MultiIndex slicing using slices, lists, and labels. In [56]: dfmi . loc [( slice ( \"A1\" , \"A3\" ), slice ( None ), [ \"C1\" , \"C3\" ]), :] Out[56]: lvl0 a b lvl1 bar foo bah foo A1 B0 C1 D0 73 72 75 74 D1 77 76 79 78 C3 D0 89 88 91 90 D1 93 92 95 94 B1 C1 D0 105 104 107 106 ... ... ... ... ... A3 B0 C3 D1 221 220 223 222 B1 C1 D0 233 232 235 234 D1 237 236 239 238 C3 D0 249 248 251 250 D1 253 252 255 254 [24 rows x 4 columns] You can use pandas.IndexSlice to facilitate a more natural syntax using : , rather than using slice(None) . In [57]: idx = pd . IndexSlice In [58]: dfmi . loc [ idx [:, :, [ \"C1\" , \"C3\" ]], idx [:, \"foo\" ]] Out[58]: lvl0 a b lvl1 foo foo A0 B0 C1 D0 8 10 D1 12 14 C3 D0 24 26 D1 28 30 B1 C1 D0 40 42 ... ... ... A3 B0 C3 D1 220 222 B1 C1 D0 232 234 D1 236 238 C3 D0 248 250 D1 252 254 [32 rows x 2 columns] It is possible to perform quite complicated selections using this method on multiple axes at the same time. In [59]: dfmi . loc [ \"A1\" , ( slice ( None ), \"foo\" )] Out[59]: lvl0 a b lvl1 foo foo B0 C0 D0 64 66 D1 68 70 C1 D0 72 74 D1 76 78 C2 D0 80 82 ... ... ... B1 C1 D1 108 110 C2 D0 112 114 D1 116 118 C3 D0 120 122 D1 124 126 [16 rows x 2 columns] In [60]: dfmi . loc [ idx [:, :, [ \"C1\" , \"C3\" ]], idx [:, \"foo\" ]] Out[60]: lvl0 a b lvl1 foo foo A0 B0 C1 D0 8 10 D1 12 14 C3 D0 24 26 D1 28 30 B1 C1 D0 40 42 ... ... ... A3 B0 C3 D1 220 222 B1 C1 D0 232 234 D1 236 238 C3 D0 248 250 D1 252 254 [32 rows x 2 columns] Using a boolean indexer you can provide selection related to the values . In [61]: mask = dfmi [( \"a\" , \"foo\" )] > 200 In [62]: dfmi . loc [ idx [ mask , :, [ \"C1\" , \"C3\" ]], idx [:, \"foo\" ]] Out[62]: lvl0 a b lvl1 foo foo A3 B0 C1 D1 204 206 C3 D0 216 218 D1 220 222 B1 C1 D0 232 234 D1 236 238 C3 D0 248 250 D1 252 254 You can also specify the axis argument to .loc to interpret the passed slicers on a single axis. In [63]: dfmi . loc ( axis = 0 )[:, :, [ \"C1\" , \"C3\" ]] Out[63]: lvl0 a b lvl1 bar foo bah foo A0 B0 C1 D0 9 8 11 10 D1 13 12 15 14 C3 D0 25 24 27 26 D1 29 28 31 30 B1 C1 D0 41 40 43 42 ... ... ... ... ... A3 B0 C3 D1 221 220 223 222 B1 C1 D0 233 232 235 234 D1 237 236 239 238 C3 D0 249 248 251 250 D1 253 252 255 254 [32 rows x 4 columns] Furthermore, you can set the values using the following methods. In [64]: df2 = dfmi . copy () In [65]: df2 . loc ( axis = 0 )[:, :, [ \"C1\" , \"C3\" ]] = - 10 In [66]: df2 Out[66]: lvl0 a b lvl1 bar foo bah foo A0 B0 C0 D0 1 0 3 2 D1 5 4 7 6 C1 D0 -10 -10 -10 -10 D1 -10 -10 -10 -10 C2 D0 17 16 19 18 ... ... ... ... ... A3 B1 C1 D1 -10 -10 -10 -10 C2 D0 241 240 243 242 D1 245 244 247 246 C3 D0 -10 -10 -10 -10 D1 -10 -10 -10 -10 [64 rows x 4 columns] You can use a right-hand-side of an alignable object as well. In [67]: df2 = dfmi . copy () In [68]: df2 . loc [ idx [:, :, [ \"C1\" , \"C3\" ]], :] = df2 * 1000 In [69]: df2 Out[69]: lvl0 a b lvl1 bar foo bah foo A0 B0 C0 D0 1 0 3 2 D1 5 4 7 6 C1 D0 9000 8000 11000 10000 D1 13000 12000 15000 14000 C2 D0 17 16 19 18 ... ... ... ... ... A3 B1 C1 D1 237000 236000 239000 238000 C2 D0 241 240 243 242 D1 245 244 247 246 C3 D0 249000 248000 251000 250000 D1 253000 252000 255000 254000 [64 rows x 4 columns] Cross-section # The xs() method of DataFrame additionally takes a level argument to make selecting data at a particular level of a MultiIndex easier. In [70]: df Out[70]: A B C first second bar one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 baz one -1.206412 0.132003 1.024180 two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 qux one -1.170299 1.130127 0.974466 two -0.226169 -1.436737 -2.006747 In [71]: df . xs ( \"one\" , level = \"second\" ) Out[71]: A B C first bar 0.895717 0.410835 -1.413681 baz -1.206412 0.132003 1.024180 foo 1.431256 -0.076467 0.875906 qux -1.170299 1.130127 0.974466 # using the slicers In [72]: df . loc [( slice ( None ), \"one\" ), :] Out[72]: A B C first second bar one 0.895717 0.410835 -1.413681 baz one -1.206412 0.132003 1.024180 foo one 1.431256 -0.076467 0.875906 qux one -1.170299 1.130127 0.974466 You can also select on the columns with xs , by providing the axis argument. In [73]: df = df . T In [74]: df . xs ( \"one\" , level = \"second\" , axis = 1 ) Out[74]: first bar baz foo qux A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835 0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466 # using the slicers In [75]: df . loc [:, ( slice ( None ), \"one\" )] Out[75]: first bar baz foo qux second one one one one A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835 0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466 xs also allows selection with multiple keys. In [76]: df . xs (( \"one\" , \"bar\" ), level = ( \"second\" , \"first\" ), axis = 1 ) Out[76]: first bar second one A 0.895717 B 0.410835 C -1.413681 # using the slicers In [77]: df . loc [:, ( \"bar\" , \"one\" )] Out[77]: A 0.895717 B 0.410835 C -1.413681 Name: (bar, one), dtype: float64 You can pass drop_level=False to xs to retain the level that was selected. In [78]: df . xs ( \"one\" , level = \"second\" , axis = 1 , drop_level = False ) Out[78]: first bar baz foo qux second one one one one A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835 0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466 Compare the above with the result using drop_level=True (the default value). In [79]: df . xs ( \"one\" , level = \"second\" , axis = 1 , drop_level = True ) Out[79]: first bar baz foo qux A 0.895717 -1.206412 1.431256 -1.170299 B 0.410835 0.132003 -0.076467 1.130127 C -1.413681 1.024180 0.875906 0.974466 Advanced reindexing and alignment # Using the parameter level in the reindex() and align() methods of pandas objects is useful to broadcast values across a level. For instance: In [80]: midx = pd . MultiIndex ( ....: levels = [[ \"zero\" , \"one\" ], [ \"x\" , \"y\" ]], codes = [[ 1 , 1 , 0 , 0 ], [ 1 , 0 , 1 , 0 ]] ....: ) ....: In [81]: df = pd . DataFrame ( np . random . randn ( 4 , 2 ), index = midx ) In [82]: df Out[82]: 0 1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 In [83]: df2 = df . groupby ( level = 0 ) . mean () In [84]: df2 Out[84]: 0 1 one 1.060074 -0.109716 zero 1.271532 0.713416 In [85]: df2 . reindex ( df . index , level = 0 ) Out[85]: 0 1 one y 1.060074 -0.109716 x 1.060074 -0.109716 zero y 1.271532 0.713416 x 1.271532 0.713416 # aligning In [86]: df_aligned , df2_aligned = df . align ( df2 , level = 0 ) In [87]: df_aligned Out[87]: 0 1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 In [88]: df2_aligned Out[88]: 0 1 one y 1.060074 -0.109716 x 1.060074 -0.109716 zero y 1.271532 0.713416 x 1.271532 0.713416 Swapping levels with swaplevel # The swaplevel() method can switch the order of two levels: In [89]: df [: 5 ] Out[89]: 0 1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 In [90]: df [: 5 ] . swaplevel ( 0 , 1 , axis = 0 ) Out[90]: 0 1 y one 1.519970 -0.493662 x one 0.600178 0.274230 y zero 0.132885 -0.023688 x zero 2.410179 1.450520 Reordering levels with reorder_levels # The reorder_levels() method generalizes the swaplevel method, allowing you to permute the hierarchical index levels in one step: In [91]: df [: 5 ] . reorder_levels ([ 1 , 0 ], axis = 0 ) Out[91]: 0 1 y one 1.519970 -0.493662 x one 0.600178 0.274230 y zero 0.132885 -0.023688 x zero 2.410179 1.450520 Renaming names of an Index or MultiIndex # The rename() method is used to rename the labels of a MultiIndex , and is typically used to rename the columns of a DataFrame . The columns argument of rename allows a dictionary to be specified that includes only the columns you wish to rename. In [92]: df . rename ( columns = { 0 : \"col0\" , 1 : \"col1\" }) Out[92]: col0 col1 one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 This method can also be used to rename specific labels of the main index of the DataFrame . In [93]: df . rename ( index = { \"one\" : \"two\" , \"y\" : \"z\" }) Out[93]: 0 1 two z 1.519970 -0.493662 x 0.600178 0.274230 zero z 0.132885 -0.023688 x 2.410179 1.450520 The rename_axis() method is used to rename the name of a Index or MultiIndex . In particular, the names of the levels of a MultiIndex can be specified, which is useful if reset_index() is later used to move the values from the MultiIndex to a column. In [94]: df . rename_axis ( index = [ \"abc\" , \"def\" ]) Out[94]: 0 1 abc def one y 1.519970 -0.493662 x 0.600178 0.274230 zero y 0.132885 -0.023688 x 2.410179 1.450520 Note that the columns of a DataFrame are an index, so that using rename_axis with the columns argument will change the name of that index. In [95]: df . rename_axis ( columns = \"Cols\" ) . columns Out[95]: RangeIndex(start=0, stop=2, step=1, name='Cols') Both rename and rename_axis support specifying a dictionary, Series or a mapping function to map labels/names to new values. When working with an Index object directly, rather than via a DataFrame , Index.set_names() can be used to change the names. In [96]: mi = pd . MultiIndex . from_product ([[ 1 , 2 ], [ \"a\" , \"b\" ]], names = [ \"x\" , \"y\" ]) In [97]: mi . names Out[97]: FrozenList(['x', 'y']) In [98]: mi2 = mi . rename ( \"new name\" , level = 0 ) In [99]: mi2 Out[99]: MultiIndex([(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')], names=['new name', 'y']) You cannot set the names of the MultiIndex via a level. In [100]: mi . levels [ 0 ] . name = \"name via level\" --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) Cell In [ 100 ], line 1 ----> 1 mi . levels [ 0 ] . name = \"name via level\" File ~/work/pandas/pandas/pandas/core/indexes/base.py:1697, in Index.name (self, value) 1693 @name . setter 1694 def name ( self , value : Hashable ) -> None : 1695 if self . _no_setting_name : 1696 # Used in MultiIndex.levels to avoid silently ignoring name updates. -> 1697 raise RuntimeError ( 1698 \"Cannot set name on a level of a MultiIndex. Use \" 1699 \"'MultiIndex.set_names' instead.\" 1700 ) 1701 maybe_extract_name ( value , None , type ( self )) 1702 self . _name = value RuntimeError : Cannot set name on a level of a MultiIndex. Use 'MultiIndex.set_names' instead. Use Index.set_names() instead. Sorting a MultiIndex # For MultiIndex -ed objects to be indexed and sliced effectively, they need to be sorted. As with any index, you can use sort_index() . In [101]: import random In [102]: random . shuffle ( tuples ) In [103]: s = pd . Series ( np . random . randn ( 8 ), index = pd . MultiIndex . from_tuples ( tuples )) In [104]: s Out[104]: baz two 0.206053 qux one -0.251905 two -2.213588 bar two 1.063327 one 1.266143 foo one 0.299368 baz one -0.863838 foo two 0.408204 dtype: float64 In [105]: s . sort_index () Out[105]: bar one 1.266143 two 1.063327 baz one -0.863838 two 0.206053 foo one 0.299368 two 0.408204 qux one -0.251905 two -2.213588 dtype: float64 In [106]: s . sort_index ( level = 0 ) Out[106]: bar one 1.266143 two 1.063327 baz one -0.863838 two 0.206053 foo one 0.299368 two 0.408204 qux one -0.251905 two -2.213588 dtype: float64 In [107]: s . sort_index ( level = 1 ) Out[107]: bar one 1.266143 baz one -0.863838 foo one 0.299368 qux one -0.251905 bar two 1.063327 baz two 0.206053 foo two 0.408204 qux two -2.213588 dtype: float64 You may also pass a level name to sort_index if the MultiIndex levels are named. In [108]: s . index = s . index . set_names ([ \"L1\" , \"L2\" ]) In [109]: s . sort_index ( level = \"L1\" ) Out[109]: L1 L2 bar one 1.266143 two 1.063327 baz one -0.863838 two 0.206053 foo one 0.299368 two 0.408204 qux one -0.251905 two -2.213588 dtype: float64 In [110]: s . sort_index ( level = \"L2\" ) Out[110]: L1 L2 bar one 1.266143 baz one -0.863838 foo one 0.299368 qux one -0.251905 bar two 1.063327 baz two 0.206053 foo two 0.408204 qux two -2.213588 dtype: float64 On higher dimensional objects, you can sort any of the other axes by level if they have a MultiIndex : In [111]: df . T . sort_index ( level = 1 , axis = 1 ) Out[111]: one zero one zero x x y y 0 0.600178 2.410179 1.519970 0.132885 1 0.274230 1.450520 -0.493662 -0.023688 Indexing will work even if the data are not sorted, but will be rather inefficient (and show a PerformanceWarning ). It will also return a copy of the data rather than a view: In [112]: dfm = pd . DataFrame ( .....: { \"jim\" : [ 0 , 0 , 1 , 1 ], \"joe\" : [ \"x\" , \"x\" , \"z\" , \"y\" ], \"jolie\" : np . random . rand ( 4 )} .....: ) .....: In [113]: dfm = dfm . set_index ([ \"jim\" , \"joe\" ]) In [114]: dfm Out[114]: jolie jim joe 0 x 0.490671 x 0.120248 1 z 0.537020 y 0.110968 In [115]: dfm . loc [( 1 , 'z' )] Out[115]: jolie jim joe 1 z 0.53702 Furthermore, if you try to index something that is not fully lexsorted, this can raise: In [116]: dfm . loc [( 0 , 'y' ):( 1 , 'z' )] --------------------------------------------------------------------------- UnsortedIndexError Traceback (most recent call last) Cell In [ 116 ], line 1 ----> 1 dfm . loc [( 0 , 'y' ):( 1 , 'z' )] File ~/work/pandas/pandas/pandas/core/indexing.py:1192, in _LocationIndexer.__getitem__ (self, key) 1190 maybe_callable = com . apply_if_callable ( key , self . obj ) 1191 maybe_callable = self . _check_deprecated_callable_usage ( key , maybe_callable ) -> 1192 return self . _getitem_axis ( maybe_callable , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1412, in _LocIndexer._getitem_axis (self, key, axis) 1410 if isinstance ( key , slice ): 1411 self . _validate_key ( key , axis ) -> 1412 return self . _get_slice_axis ( key , axis = axis ) 1413 elif com . is_bool_indexer ( key ): 1414 return self . _getbool_axis ( key , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1444, in _LocIndexer._get_slice_axis (self, slice_obj, axis) 1441 return obj . copy ( deep = False ) 1443 labels = obj . _get_axis ( axis ) -> 1444 indexer = labels . slice_indexer ( slice_obj . start , slice_obj . stop , slice_obj . step ) 1446 if isinstance ( indexer , slice ): 1447 return self . obj . _slice ( indexer , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6708, in Index.slice_indexer (self, start, end, step) 6664 def slice_indexer ( 6665 self , 6666 start : Hashable | None = None , 6667 end : Hashable | None = None , 6668 step : int | None = None , 6669 ) -> slice : 6670 \"\"\" 6671 Compute the slice indexer for input labels and step. 6672 (...) 6706 slice(1, 3, None) 6707 \"\"\" -> 6708 start_slice , end_slice = self . slice_locs ( start , end , step = step ) 6710 # return a slice 6711 if not is_scalar ( start_slice ): File ~/work/pandas/pandas/pandas/core/indexes/multi.py:2923, in MultiIndex.slice_locs (self, start, end, step) 2871 \"\"\" 2872 For an ordered MultiIndex, compute the slice locations for input 2873 labels. (...) 2919 sequence of such. 2920 \"\"\" 2921 # This function adds nothing to its parent implementation (the magic 2922 # happens in get_slice_bound method), but it adds meaningful doc. -> 2923 return super () . slice_locs ( start , end , step ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6934, in Index.slice_locs (self, start, end, step) 6932 start_slice = None 6933 if start is not None : -> 6934 start_slice = self . get_slice_bound ( start , \"left\" ) 6935 if start_slice is None : 6936 start_slice = 0 File ~/work/pandas/pandas/pandas/core/indexes/multi.py:2867, in MultiIndex.get_slice_bound (self, label, side) 2865 if not isinstance ( label , tuple ): 2866 label = ( label ,) -> 2867 return self . _partial_tup_index ( label , side = side ) File ~/work/pandas/pandas/pandas/core/indexes/multi.py:2927, in MultiIndex._partial_tup_index (self, tup, side) 2925 def _partial_tup_index ( self , tup : tuple , side : Literal [ \"left\" , \"right\" ] = \"left\" ): 2926 if len ( tup ) > self . _lexsort_depth : -> 2927 raise UnsortedIndexError ( 2928 f \"Key length ( { len ( tup ) } ) was greater than MultiIndex lexsort depth \" 2929 f \"( { self . _lexsort_depth } )\" 2930 ) 2932 n = len ( tup ) 2933 start , end = 0 , len ( self ) UnsortedIndexError : 'Key length (2) was greater than MultiIndex lexsort depth (1)' The is_monotonic_increasing() method on a MultiIndex shows if the index is sorted: In [117]: dfm . index . is_monotonic_increasing Out[117]: False In [118]: dfm = dfm . sort_index () In [119]: dfm Out[119]: jolie jim joe 0 x 0.490671 x 0.120248 1 y 0.110968 z 0.537020 In [120]: dfm . index . is_monotonic_increasing Out[120]: True And now selection works as expected. In [121]: dfm . loc [( 0 , \"y\" ):( 1 , \"z\" )] Out[121]: jolie jim joe 1 y 0.110968 z 0.537020 Take methods # Similar to NumPy ndarrays, pandas Index , Series , and DataFrame also provides the take() method that retrieves elements along a given axis at the given indices. The given indices must be either a list or an ndarray of integer index positions. take will also accept negative integers as relative positions to the end of the object. In [122]: index = pd . Index ( np . random . randint ( 0 , 1000 , 10 )) In [123]: index Out[123]: Index([214, 502, 712, 567, 786, 175, 993, 133, 758, 329], dtype='int64') In [124]: positions = [ 0 , 9 , 3 ] In [125]: index [ positions ] Out[125]: Index([214, 329, 567], dtype='int64') In [126]: index . take ( positions ) Out[126]: Index([214, 329, 567], dtype='int64') In [127]: ser = pd . Series ( np . random . randn ( 10 )) In [128]: ser . iloc [ positions ] Out[128]: 0 -0.179666 9 1.824375 3 0.392149 dtype: float64 In [129]: ser . take ( positions ) Out[129]: 0 -0.179666 9 1.824375 3 0.392149 dtype: float64 For DataFrames, the given indices should be a 1d list or ndarray that specifies row or column positions. In [130]: frm = pd . DataFrame ( np . random . randn ( 5 , 3 )) In [131]: frm . take ([ 1 , 4 , 3 ]) Out[131]: 0 1 2 1 -1.237881 0.106854 -1.276829 4 0.629675 -1.425966 1.857704 3 0.979542 -1.633678 0.615855 In [132]: frm . take ([ 0 , 2 ], axis = 1 ) Out[132]: 0 2 0 0.595974 0.601544 1 -1.237881 -1.276829 2 -0.767101 1.499591 3 0.979542 0.615855 4 0.629675 1.857704 It is important to note that the take method on pandas objects are not intended to work on boolean indices and may return unexpected results. In [133]: arr = np . random . randn ( 10 ) In [134]: arr . take ([ False , False , True , True ]) Out[134]: array([-1.1935, -1.1935, 0.6775, 0.6775]) In [135]: arr [[ 0 , 1 ]] Out[135]: array([-1.1935, 0.6775]) In [136]: ser = pd . Series ( np . random . randn ( 10 )) In [137]: ser . take ([ False , False , True , True ]) Out[137]: 0 0.233141 0 0.233141 1 -0.223540 1 -0.223540 dtype: float64 In [138]: ser . iloc [[ 0 , 1 ]] Out[138]: 0 0.233141 1 -0.223540 dtype: float64 Finally, as a small note on performance, because the take method handles a narrower range of inputs, it can offer performance that is a good deal faster than fancy indexing. In [139]: arr = np . random . randn ( 10000 , 5 ) In [140]: indexer = np . arange ( 10000 ) In [141]: random . shuffle ( indexer ) In [142]: % timeit arr[indexer] .....: % timeit arr.take(indexer, axis=0) .....: 249 us +- 3.23 us per loop (mean +- std. dev. of 7 runs, 1,000 loops each) 74.4 us +- 1.42 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each) In [143]: ser = pd . Series ( arr [:, 0 ]) In [144]: % timeit ser.iloc[indexer] .....: % timeit ser.take(indexer) .....: 150 us +- 3.01 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each) 139 us +- 20.3 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each) Index types # We have discussed MultiIndex in the previous sections pretty extensively. Documentation about DatetimeIndex and PeriodIndex are shown here , and documentation about TimedeltaIndex is found here . In the following sub-sections we will highlight some other index types. CategoricalIndex # CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements. In [145]: from pandas.api.types import CategoricalDtype In [146]: df = pd . DataFrame ({ \"A\" : np . arange ( 6 ), \"B\" : list ( \"aabbca\" )}) In [147]: df [ \"B\" ] = df [ \"B\" ] . astype ( CategoricalDtype ( list ( \"cab\" ))) In [148]: df Out[148]: A B 0 0 a 1 1 a 2 2 b 3 3 b 4 4 c 5 5 a In [149]: df . dtypes Out[149]: A int64 B category dtype: object In [150]: df [ \"B\" ] . cat . categories Out[150]: Index(['c', 'a', 'b'], dtype='object') Setting the index will create a CategoricalIndex . In [151]: df2 = df . set_index ( \"B\" ) In [152]: df2 . index Out[152]: CategoricalIndex(['a', 'a', 'b', 'b', 'c', 'a'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B') Indexing with __getitem__/.iloc/.loc works similarly to an Index with duplicates. The indexers must be in the category or the operation will raise a KeyError . In [153]: df2 . loc [ \"a\" ] Out[153]: A B a 0 a 1 a 5 The CategoricalIndex is preserved after indexing: In [154]: df2 . loc [ \"a\" ] . index Out[154]: CategoricalIndex(['a', 'a', 'a'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B') Sorting the index will sort by the order of the categories (recall that we created the index with CategoricalDtype(list('cab')) , so the sorted order is cab ). In [155]: df2 . sort_index () Out[155]: A B c 4 a 0 a 1 a 5 b 2 b 3 Groupby operations on the index will preserve the index nature as well. In [156]: df2 . groupby ( level = 0 , observed = True ) . sum () Out[156]: A B c 4 a 6 b 5 In [157]: df2 . groupby ( level = 0 , observed = True ) . sum () . index Out[157]: CategoricalIndex(['c', 'a', 'b'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B') Reindexing operations will return a resulting index based on the type of the passed indexer. Passing a list will return a plain-old Index ; indexing with a Categorical will return a CategoricalIndex , indexed according to the categories of the passed Categorical dtype. This allows one to arbitrarily index these even with values not in the categories, similarly to how you can reindex any pandas index. In [158]: df3 = pd . DataFrame ( .....: { \"A\" : np . arange ( 3 ), \"B\" : pd . Series ( list ( \"abc\" )) . astype ( \"category\" )} .....: ) .....: In [159]: df3 = df3 . set_index ( \"B\" ) In [160]: df3 Out[160]: A B a 0 b 1 c 2 In [161]: df3 . reindex ([ \"a\" , \"e\" ]) Out[161]: A B a 0.0 e NaN In [162]: df3 . reindex ([ \"a\" , \"e\" ]) . index Out[162]: Index(['a', 'e'], dtype='object', name='B') In [163]: df3 . reindex ( pd . Categorical ([ \"a\" , \"e\" ], categories = list ( \"abe\" ))) Out[163]: A B a 0.0 e NaN In [164]: df3 . reindex ( pd . Categorical ([ \"a\" , \"e\" ], categories = list ( \"abe\" ))) . index Out[164]: CategoricalIndex(['a', 'e'], categories=['a', 'b', 'e'], ordered=False, dtype='category', name='B') Warning Reshaping and Comparison operations on a CategoricalIndex must have the same categories or a TypeError will be raised. In [165]: df4 = pd . DataFrame ({ \"A\" : np . arange ( 2 ), \"B\" : list ( \"ba\" )}) In [166]: df4 [ \"B\" ] = df4 [ \"B\" ] . astype ( CategoricalDtype ( list ( \"ab\" ))) In [167]: df4 = df4 . set_index ( \"B\" ) In [168]: df4 . index Out[168]: CategoricalIndex(['b', 'a'], categories=['a', 'b'], ordered=False, dtype='category', name='B') In [169]: df5 = pd . DataFrame ({ \"A\" : np . arange ( 2 ), \"B\" : list ( \"bc\" )}) In [170]: df5 [ \"B\" ] = df5 [ \"B\" ] . astype ( CategoricalDtype ( list ( \"bc\" ))) In [171]: df5 = df5 . set_index ( \"B\" ) In [172]: df5 . index Out[172]: CategoricalIndex(['b', 'c'], categories=['b', 'c'], ordered=False, dtype='category', name='B') In [173]: pd . concat ([ df4 , df5 ]) Out[173]: A B b 0 a 1 b 0 c 1 RangeIndex # RangeIndex is a sub-class of Index that provides the default index for all DataFrame and Series objects. RangeIndex is an optimized version of Index that can represent a monotonic ordered set. These are analogous to Python range types . A RangeIndex will always have an int64 dtype. In [174]: idx = pd . RangeIndex ( 5 ) In [175]: idx Out[175]: RangeIndex(start=0, stop=5, step=1) RangeIndex is the default index for all DataFrame and Series objects: In [176]: ser = pd . Series ([ 1 , 2 , 3 ]) In [177]: ser . index Out[177]: RangeIndex(start=0, stop=3, step=1) In [178]: df = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]]) In [179]: df . index Out[179]: RangeIndex(start=0, stop=2, step=1) In [180]: df . columns Out[180]: RangeIndex(start=0, stop=2, step=1) A RangeIndex will behave similarly to a Index with an int64 dtype and operations on a RangeIndex , whose result cannot be represented by a RangeIndex , but should have an integer dtype, will be converted to an Index with int64 . For example: In [181]: idx [[ 0 , 2 ]] Out[181]: Index([0, 2], dtype='int64') IntervalIndex # IntervalIndex together with its own dtype, IntervalDtype as well as the Interval scalar type, allow first-class support in pandas for interval notation. The IntervalIndex allows some unique indexing and is also used as a return type for the categories in cut() and qcut() . Indexing with an IntervalIndex # An IntervalIndex can be used in Series and in DataFrame as the index. In [182]: df = pd . DataFrame ( .....: { \"A\" : [ 1 , 2 , 3 , 4 ]}, index = pd . IntervalIndex . from_breaks ([ 0 , 1 , 2 , 3 , 4 ]) .....: ) .....: In [183]: df Out[183]: A (0, 1] 1 (1, 2] 2 (2, 3] 3 (3, 4] 4 Label based indexing via .loc along the edges of an interval works as you would expect, selecting that particular interval. In [184]: df . loc [ 2 ] Out[184]: A 2 Name: (1, 2], dtype: int64 In [185]: df . loc [[ 2 , 3 ]] Out[185]: A (1, 2] 2 (2, 3] 3 If you select a label contained within an interval, this will also select the interval. In [186]: df . loc [ 2.5 ] Out[186]: A 3 Name: (2, 3], dtype: int64 In [187]: df . loc [[ 2.5 , 3.5 ]] Out[187]: A (2, 3] 3 (3, 4] 4 Selecting using an Interval will only return exact matches. In [188]: df . loc [ pd . Interval ( 1 , 2 )] Out[188]: A 2 Name: (1, 2], dtype: int64 Trying to select an Interval that is not exactly contained in the IntervalIndex will raise a KeyError . In [189]: df . loc [ pd . Interval ( 0.5 , 2.5 )] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) Cell In [ 189 ], line 1 ----> 1 df . loc [ pd . Interval ( 0.5 , 2.5 )] File ~/work/pandas/pandas/pandas/core/indexing.py:1192, in _LocationIndexer.__getitem__ (self, key) 1190 maybe_callable = com . apply_if_callable ( key , self . obj ) 1191 maybe_callable = self . _check_deprecated_callable_usage ( key , maybe_callable ) -> 1192 return self . _getitem_axis ( maybe_callable , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1432, in _LocIndexer._getitem_axis (self, key, axis) 1430 # fall thru to straight lookup 1431 self . _validate_key ( key , axis ) -> 1432 return self . _get_label ( key , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1382, in _LocIndexer._get_label (self, label, axis) 1380 def _get_label ( self , label , axis : AxisInt ): 1381 # GH#5567 this will fail if the label is not present in the axis. -> 1382 return self . obj . xs ( label , axis = axis ) File ~/work/pandas/pandas/pandas/core/generic.py:4323, in NDFrame.xs (self, key, axis, level, drop_level) 4321 new_index = index [ loc ] 4322 else : -> 4323 loc = index . get_loc ( key ) 4325 if isinstance ( loc , np . ndarray ): 4326 if loc . dtype == np . bool_ : File ~/work/pandas/pandas/pandas/core/indexes/interval.py:679, in IntervalIndex.get_loc (self, key) 677 matches = mask . sum () 678 if matches == 0 : --> 679 raise KeyError ( key ) 680 if matches == 1 : 681 return mask . argmax () KeyError : Interval(0.5, 2.5, closed='right') Selecting all Intervals that overlap a given Interval can be performed using the overlaps() method to create a boolean indexer. In [190]: idxr = df . index . overlaps ( pd . Interval ( 0.5 , 2.5 )) In [191]: idxr Out[191]: array([ True, True, True, False]) In [192]: df [ idxr ] Out[192]: A (0, 1] 1 (1, 2] 2 (2, 3] 3 Binning data with cut and qcut # cut() and qcut() both return a Categorical object, and the bins they create are stored as an IntervalIndex in its .categories attribute. In [193]: c = pd . cut ( range ( 4 ), bins = 2 ) In [194]: c Out[194]: [(-0.003, 1.5], (-0.003, 1.5], (1.5, 3.0], (1.5, 3.0]] Categories (2, interval[float64, right]): [(-0.003, 1.5] < (1.5, 3.0]] In [195]: c . categories Out[195]: IntervalIndex([(-0.003, 1.5], (1.5, 3.0]], dtype='interval[float64, right]') cut() also accepts an IntervalIndex for its bins argument, which enables a useful pandas idiom. First, We call cut() with some data and bins set to a fixed number, to generate the bins. Then, we pass the values of .categories as the bins argument in subsequent calls to cut() , supplying new data which will be binned into the same bins. In [196]: pd . cut ([ 0 , 3 , 5 , 1 ], bins = c . categories ) Out[196]: [(-0.003, 1.5], (1.5, 3.0], NaN, (-0.003, 1.5]] Categories (2, interval[float64, right]): [(-0.003, 1.5] < (1.5, 3.0]] Any value which falls outside all bins will be assigned a NaN value. Generating ranges of intervals # If we need intervals on a regular frequency, we can use the interval_range() function to create an IntervalIndex using various combinations of start , end , and periods . The default frequency for interval_range is a 1 for numeric intervals, and calendar day for datetime-like intervals: In [197]: pd . interval_range ( start = 0 , end = 5 ) Out[197]: IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]], dtype='interval[int64, right]') In [198]: pd . interval_range ( start = pd . Timestamp ( \"2017-01-01\" ), periods = 4 ) Out[198]: IntervalIndex([(2017-01-01 00:00:00, 2017-01-02 00:00:00], (2017-01-02 00:00:00, 2017-01-03 00:00:00], (2017-01-03 00:00:00, 2017-01-04 00:00:00], (2017-01-04 00:00:00, 2017-01-05 00:00:00]], dtype='interval[datetime64[ns], right]') In [199]: pd . interval_range ( end = pd . Timedelta ( \"3 days\" ), periods = 3 ) Out[199]: IntervalIndex([(0 days 00:00:00, 1 days 00:00:00], (1 days 00:00:00, 2 days 00:00:00], (2 days 00:00:00, 3 days 00:00:00]], dtype='interval[timedelta64[ns], right]') The freq parameter can used to specify non-default frequencies, and can utilize a variety of frequency aliases with datetime-like intervals: In [200]: pd . interval_range ( start = 0 , periods = 5 , freq = 1.5 ) Out[200]: IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0], (6.0, 7.5]], dtype='interval[float64, right]') In [201]: pd . interval_range ( start = pd . Timestamp ( \"2017-01-01\" ), periods = 4 , freq = \"W\" ) Out[201]: IntervalIndex([(2017-01-01 00:00:00, 2017-01-08 00:00:00], (2017-01-08 00:00:00, 2017-01-15 00:00:00], (2017-01-15 00:00:00, 2017-01-22 00:00:00], (2017-01-22 00:00:00, 2017-01-29 00:00:00]], dtype='interval[datetime64[ns], right]') In [202]: pd . interval_range ( start = pd . Timedelta ( \"0 days\" ), periods = 3 , freq = \"9h\" ) Out[202]: IntervalIndex([(0 days 00:00:00, 0 days 09:00:00], (0 days 09:00:00, 0 days 18:00:00], (0 days 18:00:00, 1 days 03:00:00]], dtype='interval[timedelta64[ns], right]') Additionally, the closed parameter can be used to specify which side(s) the intervals are closed on. Intervals are closed on the right side by default. In [203]: pd . interval_range ( start = 0 , end = 4 , closed = \"both\" ) Out[203]: IntervalIndex([[0, 1], [1, 2], [2, 3], [3, 4]], dtype='interval[int64, both]') In [204]: pd . interval_range ( start = 0 , end = 4 , closed = \"neither\" ) Out[204]: IntervalIndex([(0, 1), (1, 2), (2, 3), (3, 4)], dtype='interval[int64, neither]') Specifying start , end , and periods will generate a range of evenly spaced intervals from start to end inclusively, with periods number of elements in the resulting IntervalIndex : In [205]: pd . interval_range ( start = 0 , end = 6 , periods = 4 ) Out[205]: IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]], dtype='interval[float64, right]') In [206]: pd . interval_range ( pd . Timestamp ( \"2018-01-01\" ), pd . Timestamp ( \"2018-02-28\" ), periods = 3 ) Out[206]: IntervalIndex([(2018-01-01 00:00:00, 2018-01-20 08:00:00], (2018-01-20 08:00:00, 2018-02-08 16:00:00], (2018-02-08 16:00:00, 2018-02-28 00:00:00]], dtype='interval[datetime64[ns], right]') Miscellaneous indexing FAQ # Integer indexing # Label-based indexing with integer axis labels is a thorny topic. It has been discussed heavily on mailing lists and among various members of the scientific Python community. In pandas, our general viewpoint is that labels matter more than integer locations. Therefore, with an integer axis index only label-based indexing is possible with the standard tools like .loc . The following code will generate exceptions: In [207]: s = pd . Series ( range ( 5 )) In [208]: s [ - 1 ] --------------------------------------------------------------------------- ValueError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexes/range.py:413, in RangeIndex.get_loc (self, key) 412 try : --> 413 return self . _range . index ( new_key ) 414 except ValueError as err : ValueError : -1 is not in range The above exception was the direct cause of the following exception : KeyError Traceback (most recent call last) Cell In [ 208 ], line 1 ----> 1 s [ - 1 ] File ~/work/pandas/pandas/pandas/core/series.py:1133, in Series.__getitem__ (self, key) 1130 return self . _values [ key ] 1132 elif key_is_scalar : -> 1133 return self . _get_value ( key ) 1135 # Convert generator to list before going through hashable part 1136 # (We will iterate through the generator there to check for slices) 1137 if is_iterator ( key ): File ~/work/pandas/pandas/pandas/core/series.py:1249, in Series._get_value (self, label, takeable) 1246 return self . _values [ label ] 1248 # Similar to Index.get_value, but we do not fall back to positional -> 1249 loc = self . index . get_loc ( label ) 1251 if is_integer ( loc ): 1252 return self . _values [ loc ] File ~/work/pandas/pandas/pandas/core/indexes/range.py:415, in RangeIndex.get_loc (self, key) 413 return self . _range . index ( new_key ) 414 except ValueError as err : --> 415 raise KeyError ( key ) from err 416 if isinstance ( key , Hashable ): 417 raise KeyError ( key ) KeyError : -1 In [209]: df = pd . DataFrame ( np . random . randn ( 5 , 4 )) In [210]: df Out[210]: 0 1 2 3 0 -0.435772 -1.188928 -0.808286 -0.284634 1 -1.815703 1.347213 -0.243487 0.514704 2 1.162969 -0.287725 -0.179734 0.993962 3 -0.212673 0.909872 -0.733333 -0.349893 4 0.456434 -0.306735 0.553396 0.166221 In [211]: df . loc [ - 2 :] Out[211]: 0 1 2 3 0 -0.435772 -1.188928 -0.808286 -0.284634 1 -1.815703 1.347213 -0.243487 0.514704 2 1.162969 -0.287725 -0.179734 0.993962 3 -0.212673 0.909872 -0.733333 -0.349893 4 0.456434 -0.306735 0.553396 0.166221 This deliberate decision was made to prevent ambiguities and subtle bugs (many users reported finding bugs when the API change was made to stop âfalling backâ on position-based indexing). Non-monotonic indexes require exact matches # If the index of a Series or DataFrame is monotonically increasing or decreasing, then the bounds of a label-based slice can be outside the range of the index, much like slice indexing a normal Python list . Monotonicity of an index can be tested with the is_monotonic_increasing() and is_monotonic_decreasing() attributes. In [212]: df = pd . DataFrame ( index = [ 2 , 3 , 3 , 4 , 5 ], columns = [ \"data\" ], data = list ( range ( 5 ))) In [213]: df . index . is_monotonic_increasing Out[213]: True # no rows 0 or 1, but still returns rows 2, 3 (both of them), and 4: In [214]: df . loc [ 0 : 4 , :] Out[214]: data 2 0 3 1 3 2 4 3 # slice is are outside the index, so empty DataFrame is returned In [215]: df . loc [ 13 : 15 , :] Out[215]: Empty DataFrame Columns: [data] Index: [] On the other hand, if the index is not monotonic, then both slice bounds must be unique members of the index. In [216]: df = pd . DataFrame ( index = [ 2 , 3 , 1 , 4 , 3 , 5 ], columns = [ \"data\" ], data = list ( range ( 6 ))) In [217]: df . index . is_monotonic_increasing Out[217]: False # OK because 2 and 4 are in the index In [218]: df . loc [ 2 : 4 , :] Out[218]: data 2 0 3 1 1 2 4 3 # 0 is not in the index In [219]: df . loc [ 0 : 4 , :] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexes/base.py:3812, in Index.get_loc (self, key) 3811 try : -> 3812 return self . _engine . get_loc ( casted_key ) 3813 except KeyError as err : File ~/work/pandas/pandas/pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc () File ~/work/pandas/pandas/pandas/_libs/index.pyx:191, in pandas._libs.index.IndexEngine.get_loc () File ~/work/pandas/pandas/pandas/_libs/index.pyx:234, in pandas._libs.index.IndexEngine._get_loc_duplicates () File ~/work/pandas/pandas/pandas/_libs/index.pyx:242, in pandas._libs.index.IndexEngine._maybe_get_bool_indexer () File ~/work/pandas/pandas/pandas/_libs/index.pyx:134, in pandas._libs.index._unpack_bool_indexer () KeyError : 0 The above exception was the direct cause of the following exception : KeyError Traceback (most recent call last) Cell In [ 219 ], line 1 ----> 1 df . loc [ 0 : 4 , :] File ~/work/pandas/pandas/pandas/core/indexing.py:1185, in _LocationIndexer.__getitem__ (self, key) 1183 if self . _is_scalar_access ( key ): 1184 return self . obj . _get_value ( * key , takeable = self . _takeable ) -> 1185 return self . _getitem_tuple ( key ) 1186 else : 1187 # we by definition only have the 0th axis 1188 axis = self . axis or 0 File ~/work/pandas/pandas/pandas/core/indexing.py:1378, in _LocIndexer._getitem_tuple (self, tup) 1375 if self . _multi_take_opportunity ( tup ): 1376 return self . _multi_take ( tup ) -> 1378 return self . _getitem_tuple_same_dim ( tup ) File ~/work/pandas/pandas/pandas/core/indexing.py:1021, in _LocationIndexer._getitem_tuple_same_dim (self, tup) 1018 if com . is_null_slice ( key ): 1019 continue -> 1021 retval = getattr ( retval , self . name ) . _getitem_axis ( key , axis = i ) 1022 # We should never have retval.ndim < self.ndim, as that should 1023 # be handled by the _getitem_lowerdim call above. 1024 assert retval . ndim == self . ndim File ~/work/pandas/pandas/pandas/core/indexing.py:1412, in _LocIndexer._getitem_axis (self, key, axis) 1410 if isinstance ( key , slice ): 1411 self . _validate_key ( key , axis ) -> 1412 return self . _get_slice_axis ( key , axis = axis ) 1413 elif com . is_bool_indexer ( key ): 1414 return self . _getbool_axis ( key , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1444, in _LocIndexer._get_slice_axis (self, slice_obj, axis) 1441 return obj . copy ( deep = False ) 1443 labels = obj . _get_axis ( axis ) -> 1444 indexer = labels . slice_indexer ( slice_obj . start , slice_obj . stop , slice_obj . step ) 1446 if isinstance ( indexer , slice ): 1447 return self . obj . _slice ( indexer , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6708, in Index.slice_indexer (self, start, end, step) 6664 def slice_indexer ( 6665 self , 6666 start : Hashable | None = None , 6667 end : Hashable | None = None , 6668 step : int | None = None , 6669 ) -> slice : 6670 \"\"\" 6671 Compute the slice indexer for input labels and step. 6672 (...) 6706 slice(1, 3, None) 6707 \"\"\" -> 6708 start_slice , end_slice = self . slice_locs ( start , end , step = step ) 6710 # return a slice 6711 if not is_scalar ( start_slice ): File ~/work/pandas/pandas/pandas/core/indexes/base.py:6934, in Index.slice_locs (self, start, end, step) 6932 start_slice = None 6933 if start is not None : -> 6934 start_slice = self . get_slice_bound ( start , \"left\" ) 6935 if start_slice is None : 6936 start_slice = 0 File ~/work/pandas/pandas/pandas/core/indexes/base.py:6859, in Index.get_slice_bound (self, label, side) 6856 return self . _searchsorted_monotonic ( label , side ) 6857 except ValueError : 6858 # raise the original KeyError -> 6859 raise err 6861 if isinstance ( slc , np . ndarray ): 6862 # get_loc may return a boolean array, which 6863 # is OK as long as they are representable by a slice. 6864 assert is_bool_dtype ( slc . dtype ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6853, in Index.get_slice_bound (self, label, side) 6851 # we need to look up the label 6852 try : -> 6853 slc = self . get_loc ( label ) 6854 except KeyError as err : 6855 try : File ~/work/pandas/pandas/pandas/core/indexes/base.py:3819, in Index.get_loc (self, key) 3814 if isinstance ( casted_key , slice ) or ( 3815 isinstance ( casted_key , abc . Iterable ) 3816 and any ( isinstance ( x , slice ) for x in casted_key ) 3817 ): 3818 raise InvalidIndexError ( key ) -> 3819 raise KeyError ( key ) from err 3820 except TypeError : 3821 # If we have a listlike key, _check_indexing_error will raise 3822 # InvalidIndexError. Otherwise we fall through and re-raise 3823 # the TypeError. 3824 self . _check_indexing_error ( key ) KeyError : 0 # 3 is not a unique label In [220]: df . loc [ 2 : 3 , :] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) Cell In [ 220 ], line 1 ----> 1 df . loc [ 2 : 3 , :] File ~/work/pandas/pandas/pandas/core/indexing.py:1185, in _LocationIndexer.__getitem__ (self, key) 1183 if self . _is_scalar_access ( key ): 1184 return self . obj . _get_value ( * key , takeable = self . _takeable ) -> 1185 return self . _getitem_tuple ( key ) 1186 else : 1187 # we by definition only have the 0th axis 1188 axis = self . axis or 0 File ~/work/pandas/pandas/pandas/core/indexing.py:1378, in _LocIndexer._getitem_tuple (self, tup) 1375 if self . _multi_take_opportunity ( tup ): 1376 return self . _multi_take ( tup ) -> 1378 return self . _getitem_tuple_same_dim ( tup ) File ~/work/pandas/pandas/pandas/core/indexing.py:1021, in _LocationIndexer._getitem_tuple_same_dim (self, tup) 1018 if com . is_null_slice ( key ): 1019 continue -> 1021 retval = getattr ( retval , self . name ) . _getitem_axis ( key , axis = i ) 1022 # We should never have retval.ndim < self.ndim, as that should 1023 # be handled by the _getitem_lowerdim call above. 1024 assert retval . ndim == self . ndim File ~/work/pandas/pandas/pandas/core/indexing.py:1412, in _LocIndexer._getitem_axis (self, key, axis) 1410 if isinstance ( key , slice ): 1411 self . _validate_key ( key , axis ) -> 1412 return self . _get_slice_axis ( key , axis = axis ) 1413 elif com . is_bool_indexer ( key ): 1414 return self . _getbool_axis ( key , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexing.py:1444, in _LocIndexer._get_slice_axis (self, slice_obj, axis) 1441 return obj . copy ( deep = False ) 1443 labels = obj . _get_axis ( axis ) -> 1444 indexer = labels . slice_indexer ( slice_obj . start , slice_obj . stop , slice_obj . step ) 1446 if isinstance ( indexer , slice ): 1447 return self . obj . _slice ( indexer , axis = axis ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6708, in Index.slice_indexer (self, start, end, step) 6664 def slice_indexer ( 6665 self , 6666 start : Hashable | None = None , 6667 end : Hashable | None = None , 6668 step : int | None = None , 6669 ) -> slice : 6670 \"\"\" 6671 Compute the slice indexer for input labels and step. 6672 (...) 6706 slice(1, 3, None) 6707 \"\"\" -> 6708 start_slice , end_slice = self . slice_locs ( start , end , step = step ) 6710 # return a slice 6711 if not is_scalar ( start_slice ): File ~/work/pandas/pandas/pandas/core/indexes/base.py:6940, in Index.slice_locs (self, start, end, step) 6938 end_slice = None 6939 if end is not None : -> 6940 end_slice = self . get_slice_bound ( end , \"right\" ) 6941 if end_slice is None : 6942 end_slice = len ( self ) File ~/work/pandas/pandas/pandas/core/indexes/base.py:6867, in Index.get_slice_bound (self, label, side) 6865 slc = lib . maybe_booleans_to_slice ( slc . view ( \"u1\" )) 6866 if isinstance ( slc , np . ndarray ): -> 6867 raise KeyError ( 6868 f \"Cannot get { side } slice bound for non-unique \" 6869 f \"label: { repr ( original_label ) } \" 6870 ) 6872 if isinstance ( slc , slice ): 6873 if side == \"left\" : KeyError : 'Cannot get right slice bound for non-unique label: 3' Index.is_monotonic_increasing and Index.is_monotonic_decreasing only check that an index is weakly monotonic. To check for strict monotonicity, you can combine one of those with the is_unique() attribute. In [221]: weakly_monotonic = pd . Index ([ \"a\" , \"b\" , \"c\" , \"c\" ]) In [222]: weakly_monotonic Out[222]: Index(['a', 'b', 'c', 'c'], dtype='object') In [223]: weakly_monotonic . is_monotonic_increasing Out[223]: True In [224]: weakly_monotonic . is_monotonic_increasing & weakly_monotonic . is_unique Out[224]: False Endpoints are inclusive # Compared with standard Python sequence slicing in which the slice endpoint is not inclusive, label-based slicing in pandas is inclusive . The primary reason for this is that it is often not possible to easily determine the âsuccessorâ or next element after a particular label in an index. For example, consider the following Series : In [225]: s = pd . Series ( np . random . randn ( 6 ), index = list ( \"abcdef\" )) In [226]: s Out[226]: a -0.101684 b -0.734907 c -0.130121 d -0.476046 e 0.759104 f 0.213379 dtype: float64 Suppose we wished to slice from c to e , using integers this would be accomplished as such: In [227]: s [ 2 : 5 ] Out[227]: c -0.130121 d -0.476046 e 0.759104 dtype: float64 However, if you only had c and e , determining the next element in the index can be somewhat complicated. For example, the following does not work: In [228]: s . loc [ 'c' : 'e' + 1 ] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In [ 228 ], line 1 ----> 1 s . loc [ 'c' : 'e' + 1 ] TypeError : can only concatenate str (not \"int\") to str A very common use case is to limit a time series to start and end at two specific dates. To enable this, we made the design choice to make label-based slicing include both endpoints: In [229]: s . loc [ \"c\" : \"e\" ] Out[229]: c -0.130121 d -0.476046 e 0.759104 dtype: float64 This is most definitely a âpracticality beats purityâ sort of thing, but it is something to watch out for if you expect label-based slicing to behave exactly in the way that standard Python integer slicing works. Indexing potentially changes underlying Series dtype # The different indexing operation can potentially change the dtype of a Series . In [230]: series1 = pd . Series ([ 1 , 2 , 3 ]) In [231]: series1 . dtype Out[231]: dtype('int64') In [232]: res = series1 . reindex ([ 0 , 4 ]) In [233]: res . dtype Out[233]: dtype('float64') In [234]: res Out[234]: 0 1.0 4 NaN dtype: float64 In [235]: series2 = pd . Series ([ True ]) In [236]: series2 . dtype Out[236]: dtype('bool') In [237]: res = series2 . reindex_like ( series1 ) In [238]: res . dtype Out[238]: dtype('O') In [239]: res Out[239]: 0 True 1 NaN 2 NaN dtype: object This is because the (re)indexing operations above silently inserts NaNs and the dtype changes accordingly. This can cause some issues when using numpy ufuncs such as numpy.logical_and . See the GH 2388 for a more detailed discussion. previous Indexing and selecting data next Copy-on-Write (CoW) On this page Hierarchical indexing (MultiIndex) Creating a MultiIndex (hierarchical index) object Reconstructing the level labels Basic indexing on axis with MultiIndex Defined levels Data alignment and using reindex Advanced indexing with hierarchical index Using slicers Cross-section Advanced reindexing and alignment Swapping levels with swaplevel Reordering levels with reorder_levels Renaming names of an Index or MultiIndex Sorting a MultiIndex Take methods Index types CategoricalIndex RangeIndex IntervalIndex Indexing with an IntervalIndex Binning data with cut and qcut Generating ranges of intervals Miscellaneous indexing FAQ Integer indexing Non-monotonic indexes require exact matches Endpoints are inclusive Indexing potentially changes underlying Series dtype Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/advanced.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Extensions pandas.api.e... pandas.api.extensions.register_dataframe_accessor # pandas.api.extensions. register_dataframe_accessor ( name ) [source] # Register a custom accessor on DataFrame objects. Parameters : name str Name under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute. Returns : callable A class decorator. See also register_dataframe_accessor Register a custom accessor on DataFrame objects. register_series_accessor Register a custom accessor on Series objects. register_index_accessor Register a custom accessor on Index objects. Notes When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be def __init__ ( self , pandas_object ): # noqa: E999 ... For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype. >>> pd . Series ([ 'a' , 'b' ]) . dt Traceback (most recent call last): ... AttributeError : Can only use .dt accessor with datetimelike values Examples In your library code: import pandas as pd @pd . api . extensions . register_dataframe_accessor ( \"geo\" ) class GeoAccessor : def __init__ ( self , pandas_obj ): self . _obj = pandas_obj @property def center ( self ): # return the geographic center point of this DataFrame lat = self . _obj . latitude lon = self . _obj . longitude return ( float ( lon . mean ()), float ( lat . mean ())) def plot ( self ): # plot this array's data on a map, e.g., using Cartopy pass Back in an interactive IPython session: In [1]: ds = pd . DataFrame ({ \"longitude\" : np . linspace ( 0 , 10 ), ...: \"latitude\" : np . linspace ( 0 , 20 )}) In [2]: ds . geo . center Out[2]: (5.0, 10.0) In [3]: ds . geo . plot () # plots data on a map previous pandas.api.extensions.register_extension_dtype next pandas.api.extensions.register_series_accessor On this page register_dataframe_accessor() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_dataframe_accessor.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Options and settings Options and settings # Overview # pandas has an options API configure and customize global behavior related to DataFrame display, data behavior and more. Options have a full âdotted-styleâ, case-insensitive name (e.g. display.max_rows ). You can get/set options directly as attributes of the top-level options attribute: In [1]: import pandas as pd In [2]: pd . options . display . max_rows Out[2]: 15 In [3]: pd . options . display . max_rows = 999 In [4]: pd . options . display . max_rows Out[4]: 999 The API is composed of 5 relevant functions, available directly from the pandas namespace: get_option() / set_option() - get/set the value of a single option. reset_option() - reset one or more options to their default value. describe_option() - print the descriptions of one or more options. option_context() - execute a codeblock with a set of options that revert to prior settings after execution. Note Developers can check out pandas/core/config_init.py for more information. All of the functions above accept a regexp pattern ( re.search style) as an argument, to match an unambiguous substring: In [5]: pd . get_option ( \"display.chop_threshold\" ) In [6]: pd . set_option ( \"display.chop_threshold\" , 2 ) In [7]: pd . get_option ( \"display.chop_threshold\" ) Out[7]: 2 In [8]: pd . set_option ( \"chop\" , 4 ) In [9]: pd . get_option ( \"display.chop_threshold\" ) Out[9]: 4 The following will not work because it matches multiple option names, e.g. display.max_colwidth , display.max_rows , display.max_columns : In [10]: pd . get_option ( \"max\" ) --------------------------------------------------------------------------- OptionError Traceback (most recent call last) Cell In [ 10 ], line 1 ----> 1 pd . get_option ( \"max\" ) File ~/work/pandas/pandas/pandas/_config/config.py:274, in CallableDynamicDoc.__call__ (self, *args, **kwds) 273 def __call__ ( self , * args , ** kwds ) -> T : --> 274 return self . __func__ ( * args , ** kwds ) File ~/work/pandas/pandas/pandas/_config/config.py:146, in _get_option (pat, silent) 145 def _get_option ( pat : str , silent : bool = False ) -> Any : --> 146 key = _get_single_key ( pat , silent ) 148 # walk the nested dict 149 root , k = _get_root ( key ) File ~/work/pandas/pandas/pandas/_config/config.py:134, in _get_single_key (pat, silent) 132 raise OptionError ( f \"No such keys(s): { repr ( pat ) } \" ) 133 if len ( keys ) > 1 : --> 134 raise OptionError ( \"Pattern matched multiple keys\" ) 135 key = keys [ 0 ] 137 if not silent : OptionError : Pattern matched multiple keys Warning Using this form of shorthand may cause your code to break if new options with similar names are added in future versions. Available options # You can get a list of available options and their descriptions with describe_option() . When called with no argument describe_option() will print out the descriptions for all available options. In [11]: pd . describe_option () compute.use_bottleneck : bool Use the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True] compute.use_numba : bool Use the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False] compute.use_numexpr : bool Use the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True] display.chop_threshold : float or None if set to a float value, all float values smaller than the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None] display.colheader_justify : 'left'/'right' Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right] display.date_dayfirst : boolean When True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False] display.date_yearfirst : boolean When True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False] display.encoding : str/unicode Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf8] display.expand_frame_repr : boolean Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, `max_columns` is still respected, but the output will wrap-around across multiple \"pages\" if its width exceeds `display.width`. [default: True] [currently: True] display.float_format : callable The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None] display.html.border : int A ``border=value`` attribute is inserted in the ``<table>`` tag for the DataFrame HTML repr. [default: 1] [currently: 1] display.html.table_schema : boolean Whether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False] display.html.use_mathjax : boolean When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True] display.large_repr : 'truncate'/'info' For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table, or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate] display.max_categories : int This sets the maximum number of categories pandas should output when printing out a `Categorical` or a Series of dtype \"category\". [default: 8] [currently: 8] display.max_columns : int If max_cols is exceeded, switch to truncate view. Depending on `large_repr`, objects are either centrally truncated or printed as a summary view. 'None' value means unlimited. In case python/IPython is running in a terminal and `large_repr` equals 'truncate' this can be set to 0 or None and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection and defaults to 20. [default: 0] [currently: 0] display.max_colwidth : int or None The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \"...\" placeholder is embedded in the output. A 'None' value means unlimited. [default: 50] [currently: 50] display.max_dir_items : int The number of items that will be added to `dir(...)`. 'None' value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added. This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100] display.max_info_columns : int max_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100] display.max_info_rows : int df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785] display.max_rows : int If max_rows is exceeded, switch to truncate view. Depending on `large_repr`, objects are either centrally truncated or printed as a summary view. 'None' value means unlimited. In case python/IPython is running in a terminal and `large_repr` equals 'truncate' this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60] display.max_seq_items : int or None When pretty-printing a long sequence, no more then `max_seq_items` will be printed. If items are omitted, they will be denoted by the addition of \"...\" to the resulting string. If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100] display.memory_usage : bool, string or None This specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,'deep' [default: True] [currently: True] display.min_rows : int The numbers of rows to show in a truncated view (when `max_rows` is exceeded). Ignored when `max_rows` is set to None or 0. When set to None, follows the value of `max_rows`. [default: 10] [currently: 10] display.multi_sparse : boolean \"sparsify\" MultiIndex display (don't display repeated elements in outer levels within groups) [default: True] [currently: True] display.notebook_repr_html : boolean When True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True] display.pprint_nest_depth : int Controls the number of nested levels to process when pretty-printing [default: 3] [currently: 3] display.precision : int Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to ``precision`` in :meth:`numpy.set_printoptions`. [default: 6] [currently: 6] display.show_dimensions : boolean or 'truncate' Whether to print out dimensions at the end of DataFrame repr. If 'truncate' is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate] display.unicode.ambiguous_as_wide : boolean Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.unicode.east_asian_width : boolean Whether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.width : int Width of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80] future.infer_string Whether to infer sequence of str objects as pyarrow string dtype, which will be the default in pandas 3.0 (at which point this option will be deprecated). [default: False] [currently: False] future.no_silent_downcasting Whether to opt-in to the future behavior which will *not* silently downcast results from Series and DataFrame `where`, `mask`, and `clip` methods. Silent downcasting will be removed in pandas 3.0 (at which point this option will be deprecated). [default: False] [currently: False] io.excel.ods.reader : string The default Excel reader engine for 'ods' files. Available options: auto, odf, calamine. [default: auto] [currently: auto] io.excel.ods.writer : string The default Excel writer engine for 'ods' files. Available options: auto, odf. [default: auto] [currently: auto] io.excel.xls.reader : string The default Excel reader engine for 'xls' files. Available options: auto, xlrd, calamine. [default: auto] [currently: auto] io.excel.xlsb.reader : string The default Excel reader engine for 'xlsb' files. Available options: auto, pyxlsb, calamine. [default: auto] [currently: auto] io.excel.xlsm.reader : string The default Excel reader engine for 'xlsm' files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsm.writer : string The default Excel writer engine for 'xlsm' files. Available options: auto, openpyxl. [default: auto] [currently: auto] io.excel.xlsx.reader : string The default Excel reader engine for 'xlsx' files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsx.writer : string The default Excel writer engine for 'xlsx' files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto] io.hdf.default_format : format default format writing format, if None, then put will default to 'fixed' and append will default to 'table' [default: None] [currently: None] io.hdf.dropna_table : boolean drop ALL nan rows when appending to a table [default: False] [currently: False] io.parquet.engine : string The default parquet reader/writer engine. Available options: 'auto', 'pyarrow', 'fastparquet', the default is 'auto' [default: auto] [currently: auto] io.sql.engine : string The default sql reader/writer engine. Available options: 'auto', 'sqlalchemy', the default is 'auto' [default: auto] [currently: auto] mode.chained_assignment : string Raise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn] mode.copy_on_write : bool Use new copy-view behaviour using Copy-on-Write. Defaults to False, unless overridden by the 'PANDAS_COPY_ON_WRITE' environment variable (if set to \"1\" for True, needs to be set before pandas is imported). [default: False] [currently: False] mode.data_manager : string Internal data manager type; can be \"block\" or \"array\". Defaults to \"block\", unless overridden by the 'PANDAS_DATA_MANAGER' environment variable (needs to be set before pandas is imported). [default: block] [currently: block] (Deprecated, use `` instead.) mode.sim_interactive : boolean Whether to simulate interactive mode for purposes of testing [default: False] [currently: False] mode.string_storage : string The default storage for StringDtype. [default: auto] [currently: auto] mode.use_inf_as_na : boolean True means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). This option is deprecated in pandas 2.1.0 and will be removed in 3.0. [default: False] [currently: False] (Deprecated, use `` instead.) plotting.backend : str The plotting backend to use. The default value is \"matplotlib\", the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib] plotting.matplotlib.register_converters : bool or 'auto'. Whether to register converters with matplotlib's units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto] styler.format.decimal : str The character representation for the decimal separator for floats and complex. [default: .] [currently: .] styler.format.escape : str, optional Whether to escape certain characters according to the given context; html or latex. [default: None] [currently: None] styler.format.formatter : str, callable, dict, optional A formatter object to be used as default within ``Styler.format``. [default: None] [currently: None] styler.format.na_rep : str, optional The string representation for values identified as missing. [default: None] [currently: None] styler.format.precision : int The precision for floats and complex numbers. [default: 6] [currently: 6] styler.format.thousands : str, optional The character representation for thousands separator for floats, int and complex. [default: None] [currently: None] styler.html.mathjax : bool If False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True] styler.latex.environment : str The environment to replace ``\\begin{table}``. If \"longtable\" is used results in a specific longtable environment format. [default: None] [currently: None] styler.latex.hrules : bool Whether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False] styler.latex.multicol_align : {\"r\", \"c\", \"l\", \"naive-l\", \"naive-r\"} The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. \"\\|r\" will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r] styler.latex.multirow_align : {\"c\", \"t\", \"b\"} The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c] styler.render.encoding : str The encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8] styler.render.max_columns : int, optional The maximum number of columns that will be rendered. May still be reduced to satisfy ``max_elements``, which takes precedence. [default: None] [currently: None] styler.render.max_elements : int The maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144] styler.render.max_rows : int, optional The maximum number of rows that will be rendered. May still be reduced to satisfy ``max_elements``, which takes precedence. [default: None] [currently: None] styler.render.repr : str Determine which output to use in Jupyter Notebook in {\"html\", \"latex\"}. [default: html] [currently: html] styler.sparse.columns : bool Whether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True] styler.sparse.index : bool Whether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True] Getting and setting options # As described above, get_option() and set_option() are available from the pandas namespace. To change an option, call set_option('option regex', new_value) . In [12]: pd . get_option ( \"mode.sim_interactive\" ) Out[12]: False In [13]: pd . set_option ( \"mode.sim_interactive\" , True ) In [14]: pd . get_option ( \"mode.sim_interactive\" ) Out[14]: True Note The option 'mode.sim_interactive' is mostly used for debugging purposes. You can use reset_option() to revert to a settingâs default value In [15]: pd . get_option ( \"display.max_rows\" ) Out[15]: 60 In [16]: pd . set_option ( \"display.max_rows\" , 999 ) In [17]: pd . get_option ( \"display.max_rows\" ) Out[17]: 999 In [18]: pd . reset_option ( \"display.max_rows\" ) In [19]: pd . get_option ( \"display.max_rows\" ) Out[19]: 60 Itâs also possible to reset multiple options at once (using a regex): In [20]: pd . reset_option ( \"^display\" ) option_context() context manager has been exposed through the top-level API, allowing you to execute code with given option values. Option values are restored automatically when you exit the with block: In [21]: with pd . option_context ( \"display.max_rows\" , 10 , \"display.max_columns\" , 5 ): ....: print ( pd . get_option ( \"display.max_rows\" )) ....: print ( pd . get_option ( \"display.max_columns\" )) ....: 10 5 In [22]: print ( pd . get_option ( \"display.max_rows\" )) 60 In [23]: print ( pd . get_option ( \"display.max_columns\" )) 0 Setting startup options in Python/IPython environment # Using startup scripts for the Python/IPython environment to import pandas and set options makes working with pandas more efficient. To do this, create a .py or .ipy script in the startup directory of the desired profile. An example where the startup folder is in a default IPython profile can be found at: $IPYTHONDIR/profile_default/startup More information can be found in the IPython documentation . An example startup script for pandas is displayed below: import pandas as pd pd . set_option ( \"display.max_rows\" , 999 ) pd . set_option ( \"display.precision\" , 5 ) Frequently used options # The following is a demonstrates the more frequently used display options. display.max_rows and display.max_columns sets the maximum number of rows and columns displayed when a frame is pretty-printed. Truncated lines are replaced by an ellipsis. In [24]: df = pd . DataFrame ( np . random . randn ( 7 , 2 )) In [25]: pd . set_option ( \"display.max_rows\" , 7 ) In [26]: df Out[26]: 0 1 0 0.469112 -0.282863 1 -1.509059 -1.135632 2 1.212112 -0.173215 3 0.119209 -1.044236 4 -0.861849 -2.104569 5 -0.494929 1.071804 6 0.721555 -0.706771 In [27]: pd . set_option ( \"display.max_rows\" , 5 ) In [28]: df Out[28]: 0 1 0 0.469112 -0.282863 1 -1.509059 -1.135632 .. ... ... 5 -0.494929 1.071804 6 0.721555 -0.706771 [7 rows x 2 columns] In [29]: pd . reset_option ( \"display.max_rows\" ) Once the display.max_rows is exceeded, the display.min_rows options determines how many rows are shown in the truncated repr. In [30]: pd . set_option ( \"display.max_rows\" , 8 ) In [31]: pd . set_option ( \"display.min_rows\" , 4 ) # below max_rows -> all rows shown In [32]: df = pd . DataFrame ( np . random . randn ( 7 , 2 )) In [33]: df Out[33]: 0 1 0 -1.039575 0.271860 1 -0.424972 0.567020 2 0.276232 -1.087401 3 -0.673690 0.113648 4 -1.478427 0.524988 5 0.404705 0.577046 6 -1.715002 -1.039268 # above max_rows -> only min_rows (4) rows shown In [34]: df = pd . DataFrame ( np . random . randn ( 9 , 2 )) In [35]: df Out[35]: 0 1 0 -0.370647 -1.157892 1 -1.344312 0.844885 .. ... ... 7 0.276662 -0.472035 8 -0.013960 -0.362543 [9 rows x 2 columns] In [36]: pd . reset_option ( \"display.max_rows\" ) In [37]: pd . reset_option ( \"display.min_rows\" ) display.expand_frame_repr allows for the representation of a DataFrame to stretch across pages, wrapped over the all the columns. In [38]: df = pd . DataFrame ( np . random . randn ( 5 , 10 )) In [39]: pd . set_option ( \"expand_frame_repr\" , True ) In [40]: df Out[40]: 0 1 2 ... 7 8 9 0 -0.006154 -0.923061 0.895717 ... 1.340309 -1.170299 -0.226169 1 0.410835 0.813850 0.132003 ... -1.436737 -1.413681 1.607920 2 1.024180 0.569605 0.875906 ... -0.078638 0.545952 -1.219217 3 -1.226825 0.769804 -1.281247 ... 0.341734 0.959726 -1.110336 4 -0.619976 0.149748 -0.732339 ... 0.301624 -2.179861 -1.369849 [5 rows x 10 columns] In [41]: pd . set_option ( \"expand_frame_repr\" , False ) In [42]: df Out[42]: 0 1 2 3 4 5 6 7 8 9 0 -0.006154 -0.923061 0.895717 0.805244 -1.206412 2.565646 1.431256 1.340309 -1.170299 -0.226169 1 0.410835 0.813850 0.132003 -0.827317 -0.076467 -1.187678 1.130127 -1.436737 -1.413681 1.607920 2 1.024180 0.569605 0.875906 -2.211372 0.974466 -2.006747 -0.410001 -0.078638 0.545952 -1.219217 3 -1.226825 0.769804 -1.281247 -0.727707 -0.121306 -0.097883 0.695775 0.341734 0.959726 -1.110336 4 -0.619976 0.149748 -0.732339 0.687738 0.176444 0.403310 -0.154951 0.301624 -2.179861 -1.369849 In [43]: pd . reset_option ( \"expand_frame_repr\" ) display.large_repr displays a DataFrame that exceed max_columns or max_rows as a truncated frame or summary. In [44]: df = pd . DataFrame ( np . random . randn ( 10 , 10 )) In [45]: pd . set_option ( \"display.max_rows\" , 5 ) In [46]: pd . set_option ( \"large_repr\" , \"truncate\" ) In [47]: df Out[47]: 0 1 2 ... 7 8 9 0 -0.954208 1.462696 -1.743161 ... 0.995761 2.396780 0.014871 1 3.357427 -0.317441 -1.236269 ... 0.380396 0.084844 0.432390 .. ... ... ... ... ... ... ... 8 -0.303421 -0.858447 0.306996 ... 0.476720 0.473424 -0.242861 9 -0.014805 -0.284319 0.650776 ... 1.613616 0.464000 0.227371 [10 rows x 10 columns] In [48]: pd . set_option ( \"large_repr\" , \"info\" ) In [49]: df Out[49]: <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 10 non-null float64 1 1 10 non-null float64 2 2 10 non-null float64 3 3 10 non-null float64 4 4 10 non-null float64 5 5 10 non-null float64 6 6 10 non-null float64 7 7 10 non-null float64 8 8 10 non-null float64 9 9 10 non-null float64 dtypes: float64(10) memory usage: 928.0 bytes In [50]: pd . reset_option ( \"large_repr\" ) In [51]: pd . reset_option ( \"display.max_rows\" ) display.max_colwidth sets the maximum width of columns. Cells of this length or longer will be truncated with an ellipsis. In [52]: df = pd . DataFrame ( ....: np . array ( ....: [ ....: [ \"foo\" , \"bar\" , \"bim\" , \"uncomfortably long string\" ], ....: [ \"horse\" , \"cow\" , \"banana\" , \"apple\" ], ....: ] ....: ) ....: ) ....: In [53]: pd . set_option ( \"max_colwidth\" , 40 ) In [54]: df Out[54]: 0 1 2 3 0 foo bar bim uncomfortably long string 1 horse cow banana apple In [55]: pd . set_option ( \"max_colwidth\" , 6 ) In [56]: df Out[56]: 0 1 2 3 0 foo bar bim un... 1 horse cow ba... apple In [57]: pd . reset_option ( \"max_colwidth\" ) display.max_info_columns sets a threshold for the number of columns displayed when calling info() . In [58]: df = pd . DataFrame ( np . random . randn ( 10 , 10 )) In [59]: pd . set_option ( \"max_info_columns\" , 11 ) In [60]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 10 non-null float64 1 1 10 non-null float64 2 2 10 non-null float64 3 3 10 non-null float64 4 4 10 non-null float64 5 5 10 non-null float64 6 6 10 non-null float64 7 7 10 non-null float64 8 8 10 non-null float64 9 9 10 non-null float64 dtypes: float64(10) memory usage: 928.0 bytes In [61]: pd . set_option ( \"max_info_columns\" , 5 ) In [62]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Columns: 10 entries, 0 to 9 dtypes: float64(10) memory usage: 928.0 bytes In [63]: pd . reset_option ( \"max_info_columns\" ) display.max_info_rows : info() will usually show null-counts for each column. For a large DataFrame , this can be quite slow. max_info_rows and max_info_cols limit this null check to the specified rows and columns respectively. The info() keyword argument show_counts=True will override this. In [64]: df = pd . DataFrame ( np . random . choice ([ 0 , 1 , np . nan ], size = ( 10 , 10 ))) In [65]: df Out[65]: 0 1 2 3 4 5 6 7 8 9 0 0.0 NaN 1.0 NaN NaN 0.0 NaN 0.0 NaN 1.0 1 1.0 NaN 1.0 1.0 1.0 1.0 NaN 0.0 0.0 NaN 2 0.0 NaN 1.0 0.0 0.0 NaN NaN NaN NaN 0.0 3 NaN NaN NaN 0.0 1.0 1.0 NaN 1.0 NaN 1.0 4 0.0 NaN NaN NaN 0.0 NaN NaN NaN 1.0 0.0 5 0.0 1.0 1.0 1.0 1.0 0.0 NaN NaN 1.0 0.0 6 1.0 1.0 1.0 NaN 1.0 NaN 1.0 0.0 NaN NaN 7 0.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 NaN 8 NaN NaN NaN 0.0 NaN NaN NaN NaN 1.0 NaN 9 0.0 NaN 0.0 NaN NaN 0.0 NaN 1.0 1.0 0.0 In [66]: pd . set_option ( \"max_info_rows\" , 11 ) In [67]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 8 non-null float64 1 1 3 non-null float64 2 2 7 non-null float64 3 3 6 non-null float64 4 4 7 non-null float64 5 5 6 non-null float64 6 6 2 non-null float64 7 7 6 non-null float64 8 8 6 non-null float64 9 9 6 non-null float64 dtypes: float64(10) memory usage: 928.0 bytes In [68]: pd . set_option ( \"max_info_rows\" , 5 ) In [69]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 10 columns): # Column Dtype --- ------ ----- 0 0 float64 1 1 float64 2 2 float64 3 3 float64 4 4 float64 5 5 float64 6 6 float64 7 7 float64 8 8 float64 9 9 float64 dtypes: float64(10) memory usage: 928.0 bytes In [70]: pd . reset_option ( \"max_info_rows\" ) display.precision sets the output display precision in terms of decimal places. In [71]: df = pd . DataFrame ( np . random . randn ( 5 , 5 )) In [72]: pd . set_option ( \"display.precision\" , 7 ) In [73]: df Out[73]: 0 1 2 3 4 0 -1.1506406 -0.7983341 -0.5576966 0.3813531 1.3371217 1 -1.5310949 1.3314582 -0.5713290 -0.0266708 -1.0856630 2 -1.1147378 -0.0582158 -0.4867681 1.6851483 0.1125723 3 -1.4953086 0.8984347 -0.1482168 -1.5960698 0.1596530 4 0.2621358 0.0362196 0.1847350 -0.2550694 -0.2710197 In [74]: pd . set_option ( \"display.precision\" , 4 ) In [75]: df Out[75]: 0 1 2 3 4 0 -1.1506 -0.7983 -0.5577 0.3814 1.3371 1 -1.5311 1.3315 -0.5713 -0.0267 -1.0857 2 -1.1147 -0.0582 -0.4868 1.6851 0.1126 3 -1.4953 0.8984 -0.1482 -1.5961 0.1597 4 0.2621 0.0362 0.1847 -0.2551 -0.2710 display.chop_threshold sets the rounding threshold to zero when displaying a Series or DataFrame . This setting does not change the precision at which the number is stored. In [76]: df = pd . DataFrame ( np . random . randn ( 6 , 6 )) In [77]: pd . set_option ( \"chop_threshold\" , 0 ) In [78]: df Out[78]: 0 1 2 3 4 5 0 1.2884 0.2946 -1.1658 0.8470 -0.6856 0.6091 1 -0.3040 0.6256 -0.0593 0.2497 1.1039 -1.0875 2 1.9980 -0.2445 0.1362 0.8863 -1.3507 -0.8863 3 -1.0133 1.9209 -0.3882 -2.3144 0.6655 0.4026 4 0.3996 -1.7660 0.8504 0.3881 0.9923 0.7441 5 -0.7398 -1.0549 -0.1796 0.6396 1.5850 1.9067 In [79]: pd . set_option ( \"chop_threshold\" , 0.5 ) In [80]: df Out[80]: 0 1 2 3 4 5 0 1.2884 0.0000 -1.1658 0.8470 -0.6856 0.6091 1 0.0000 0.6256 0.0000 0.0000 1.1039 -1.0875 2 1.9980 0.0000 0.0000 0.8863 -1.3507 -0.8863 3 -1.0133 1.9209 0.0000 -2.3144 0.6655 0.0000 4 0.0000 -1.7660 0.8504 0.0000 0.9923 0.7441 5 -0.7398 -1.0549 0.0000 0.6396 1.5850 1.9067 In [81]: pd . reset_option ( \"chop_threshold\" ) display.colheader_justify controls the justification of the headers. The options are 'right' , and 'left' . In [82]: df = pd . DataFrame ( ....: np . array ([ np . random . randn ( 6 ), np . random . randint ( 1 , 9 , 6 ) * 0.1 , np . zeros ( 6 )]) . T , ....: columns = [ \"A\" , \"B\" , \"C\" ], ....: dtype = \"float\" , ....: ) ....: In [83]: pd . set_option ( \"colheader_justify\" , \"right\" ) In [84]: df Out[84]: A B C 0 0.1040 0.1 0.0 1 0.1741 0.5 0.0 2 -0.4395 0.4 0.0 3 -0.7413 0.8 0.0 4 -0.0797 0.4 0.0 5 -0.9229 0.3 0.0 In [85]: pd . set_option ( \"colheader_justify\" , \"left\" ) In [86]: df Out[86]: A B C 0 0.1040 0.1 0.0 1 0.1741 0.5 0.0 2 -0.4395 0.4 0.0 3 -0.7413 0.8 0.0 4 -0.0797 0.4 0.0 5 -0.9229 0.3 0.0 In [87]: pd . reset_option ( \"colheader_justify\" ) Number formatting # pandas also allows you to set how numbers are displayed in the console. This option is not set through the set_options API. Use the set_eng_float_format function to alter the floating-point formatting of pandas objects to produce a particular format. In [88]: import numpy as np In [89]: pd . set_eng_float_format ( accuracy = 3 , use_eng_prefix = True ) In [90]: s = pd . Series ( np . random . randn ( 5 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) In [91]: s / 1.0e3 Out[91]: a 303.638u b -721.084u c -622.696u d 648.250u e -1.945m dtype: float64 In [92]: s / 1.0e6 Out[92]: a 303.638n b -721.084n c -622.696n d 648.250n e -1.945u dtype: float64 Use round() to specifically control rounding of an individual DataFrame Unicode formatting # Warning Enabling this option will affect the performance for printing of DataFrame and Series (about 2 times slower). Use only when it is actually required. Some East Asian countries use Unicode characters whose width corresponds to two Latin characters. If a DataFrame or Series contains these characters, the default output mode may not align them properly. In [93]: df = pd . DataFrame ({ \"å½ç±\" : [ \"UK\" , \"æ¥æ¬\" ], \"åå\" : [ \"Alice\" , \"ãã®ã¶\" ]}) In [94]: df Out[94]: å½ç± åå 0 UK Alice 1 æ¥æ¬ ãã®ã¶ Enabling display.unicode.east_asian_width allows pandas to check each characterâs âEast Asian Widthâ property. These characters can be aligned properly by setting this option to True . However, this will result in longer render times than the standard len function. In [95]: pd . set_option ( \"display.unicode.east_asian_width\" , True ) In [96]: df Out[96]: å½ç± åå 0 UK Alice 1 æ¥æ¬ ãã®ã¶ In addition, Unicode characters whose width is âambiguousâ can either be 1 or 2 characters wide depending on the terminal setting or encoding. The option display.unicode.ambiguous_as_wide can be used to handle the ambiguity. By default, an âambiguousâ characterâs width, such as âÂ¡â (inverted exclamation) in the example below, is taken to be 1. In [97]: df = pd . DataFrame ({ \"a\" : [ \"xxx\" , \"Â¡Â¡\" ], \"b\" : [ \"yyy\" , \"Â¡Â¡\" ]}) In [98]: df Out[98]: a b 0 xxx yyy 1 Â¡Â¡ Â¡Â¡ Enabling display.unicode.ambiguous_as_wide makes pandas interpret these charactersâ widths to be 2. (Note that this option will only be effective when display.unicode.east_asian_width is enabled.) However, setting this option incorrectly for your terminal will cause these characters to be aligned incorrectly: In [99]: pd . set_option ( \"display.unicode.ambiguous_as_wide\" , True ) In [100]: df Out[100]: a b 0 xxx yyy 1 Â¡Â¡ Â¡Â¡ Table schema display # DataFrame and Series will publish a Table Schema representation by default. This can be enabled globally with the display.html.table_schema option: In [101]: pd . set_option ( \"display.html.table_schema\" , True ) Only 'display.max_rows' are serialized and published. previous Time deltas next Enhancing performance On this page Overview Available options Getting and setting options Setting startup options in Python/IPython environment Frequently used options Number formatting Unicode formatting Table schema display Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/options.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.radviz # pandas.plotting. radviz ( frame , class_column , ax = None , color = None , colormap = None , ** kwds ) [source] # Plot a multidimensional dataset in 2D. Each Series in the DataFrame is represented as a evenly distributed slice on a circle. Each data point is rendered in the circle according to the value on each Series. Highly correlated Series in the DataFrame are placed closer on the unit circle. RadViz allow to project a N-dimensional data set into a 2D space where the influence of each dimension can be interpreted as a balance between the influence of all dimensions. More info available at the original article describing RadViz. Parameters : frame DataFrame Object holding the data. class_column str Column name containing the name of the data point category. ax matplotlib.axes.Axes , optional A plot instance to which to add the information. color list[str] or tuple[str], optional Assign a color to each category. Example: [âblueâ, âgreenâ]. colormap str or matplotlib.colors.Colormap , default None Colormap to select colors from. If string, load colormap with that name from matplotlib. **kwds Options to pass to matplotlib scatter plotting method. Returns : matplotlib.axes.Axes See also pandas.plotting.andrews_curves Plot clustering visualization. Examples >>> df = pd . DataFrame ( ... { ... 'SepalLength' : [ 6.5 , 7.7 , 5.1 , 5.8 , 7.6 , 5.0 , 5.4 , 4.6 , 6.7 , 4.6 ], ... 'SepalWidth' : [ 3.0 , 3.8 , 3.8 , 2.7 , 3.0 , 2.3 , 3.0 , 3.2 , 3.3 , 3.6 ], ... 'PetalLength' : [ 5.5 , 6.7 , 1.9 , 5.1 , 6.6 , 3.3 , 4.5 , 1.4 , 5.7 , 1.0 ], ... 'PetalWidth' : [ 1.8 , 2.2 , 0.4 , 1.9 , 2.1 , 1.0 , 1.5 , 0.2 , 2.1 , 0.2 ], ... 'Category' : [ ... 'virginica' , ... 'virginica' , ... 'setosa' , ... 'virginica' , ... 'virginica' , ... 'versicolor' , ... 'versicolor' , ... 'setosa' , ... 'virginica' , ... 'setosa' ... ] ... } ... ) >>> pd . plotting . radviz ( df , 'Category' ) previous pandas.plotting.plot_params next pandas.plotting.register_matplotlib_converters On this page radviz() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Time series... Time series / date functionality # pandas contains extensive capabilities and features for working with time series data for all domains. Using the NumPy datetime64 and timedelta64 dtypes, pandas has consolidated a large number of features from other Python libraries like scikits.timeseries as well as created a tremendous amount of new functionality for manipulating time series data. For example, pandas supports: Parsing time series information from various sources and formats In [1]: import datetime In [2]: dti = pd . to_datetime ( ...: [ \"1/1/2018\" , np . datetime64 ( \"2018-01-01\" ), datetime . datetime ( 2018 , 1 , 1 )] ...: ) ...: In [3]: dti Out[3]: DatetimeIndex(['2018-01-01', '2018-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None) Generate sequences of fixed-frequency dates and time spans In [4]: dti = pd . date_range ( \"2018-01-01\" , periods = 3 , freq = \"h\" ) In [5]: dti Out[5]: DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00', '2018-01-01 02:00:00'], dtype='datetime64[ns]', freq='h') Manipulating and converting date times with timezone information In [6]: dti = dti . tz_localize ( \"UTC\" ) In [7]: dti Out[7]: DatetimeIndex(['2018-01-01 00:00:00+00:00', '2018-01-01 01:00:00+00:00', '2018-01-01 02:00:00+00:00'], dtype='datetime64[ns, UTC]', freq='h') In [8]: dti . tz_convert ( \"US/Pacific\" ) Out[8]: DatetimeIndex(['2017-12-31 16:00:00-08:00', '2017-12-31 17:00:00-08:00', '2017-12-31 18:00:00-08:00'], dtype='datetime64[ns, US/Pacific]', freq='h') Resampling or converting a time series to a particular frequency In [9]: idx = pd . date_range ( \"2018-01-01\" , periods = 5 , freq = \"h\" ) In [10]: ts = pd . Series ( range ( len ( idx )), index = idx ) In [11]: ts Out[11]: 2018-01-01 00:00:00 0 2018-01-01 01:00:00 1 2018-01-01 02:00:00 2 2018-01-01 03:00:00 3 2018-01-01 04:00:00 4 Freq: h, dtype: int64 In [12]: ts . resample ( \"2h\" ) . mean () Out[12]: 2018-01-01 00:00:00 0.5 2018-01-01 02:00:00 2.5 2018-01-01 04:00:00 4.0 Freq: 2h, dtype: float64 Performing date and time arithmetic with absolute or relative time increments In [13]: friday = pd . Timestamp ( \"2018-01-05\" ) In [14]: friday . day_name () Out[14]: 'Friday' # Add 1 day In [15]: saturday = friday + pd . Timedelta ( \"1 day\" ) In [16]: saturday . day_name () Out[16]: 'Saturday' # Add 1 business day (Friday --> Monday) In [17]: monday = friday + pd . offsets . BDay () In [18]: monday . day_name () Out[18]: 'Monday' pandas provides a relatively compact and self-contained set of tools for performing the above tasks and more. Overview # pandas captures 4 general time related concepts: Date times: A specific date and time with timezone support. Similar to datetime.datetime from the standard library. Time deltas: An absolute time duration. Similar to datetime.timedelta from the standard library. Time spans: A span of time defined by a point in time and its associated frequency. Date offsets: A relative time duration that respects calendar arithmetic. Similar to dateutil.relativedelta.relativedelta from the dateutil package. Concept Scalar Class Array Class pandas Data Type Primary Creation Method Date times Timestamp DatetimeIndex datetime64[ns] or datetime64[ns, tz] to_datetime or date_range Time deltas Timedelta TimedeltaIndex timedelta64[ns] to_timedelta or timedelta_range Time spans Period PeriodIndex period[freq] Period or period_range Date offsets DateOffset None None DateOffset For time series data, itâs conventional to represent the time component in the index of a Series or DataFrame so manipulations can be performed with respect to the time element. In [19]: pd . Series ( range ( 3 ), index = pd . date_range ( \"2000\" , freq = \"D\" , periods = 3 )) Out[19]: 2000-01-01 0 2000-01-02 1 2000-01-03 2 Freq: D, dtype: int64 However, Series and DataFrame can directly also support the time component as data itself. In [20]: pd . Series ( pd . date_range ( \"2000\" , freq = \"D\" , periods = 3 )) Out[20]: 0 2000-01-01 1 2000-01-02 2 2000-01-03 dtype: datetime64[ns] Series and DataFrame have extended data type support and functionality for datetime , timedelta and Period data when passed into those constructors. DateOffset data however will be stored as object data. In [21]: pd . Series ( pd . period_range ( \"1/1/2011\" , freq = \"M\" , periods = 3 )) Out[21]: 0 2011-01 1 2011-02 2 2011-03 dtype: period[M] In [22]: pd . Series ([ pd . DateOffset ( 1 ), pd . DateOffset ( 2 )]) Out[22]: 0 <DateOffset> 1 <2 * DateOffsets> dtype: object In [23]: pd . Series ( pd . date_range ( \"1/1/2011\" , freq = \"ME\" , periods = 3 )) Out[23]: 0 2011-01-31 1 2011-02-28 2 2011-03-31 dtype: datetime64[ns] Lastly, pandas represents null date times, time deltas, and time spans as NaT which is useful for representing missing or null date like values and behaves similar as np.nan does for float data. In [24]: pd . Timestamp ( pd . NaT ) Out[24]: NaT In [25]: pd . Timedelta ( pd . NaT ) Out[25]: NaT In [26]: pd . Period ( pd . NaT ) Out[26]: NaT # Equality acts as np.nan would In [27]: pd . NaT == pd . NaT Out[27]: False Timestamps vs. time spans # Timestamped data is the most basic type of time series data that associates values with points in time. For pandas objects it means using the points in time. In [28]: import datetime In [29]: pd . Timestamp ( datetime . datetime ( 2012 , 5 , 1 )) Out[29]: Timestamp('2012-05-01 00:00:00') In [30]: pd . Timestamp ( \"2012-05-01\" ) Out[30]: Timestamp('2012-05-01 00:00:00') In [31]: pd . Timestamp ( 2012 , 5 , 1 ) Out[31]: Timestamp('2012-05-01 00:00:00') However, in many cases it is more natural to associate things like change variables with a time span instead. The span represented by Period can be specified explicitly, or inferred from datetime string format. For example: In [32]: pd . Period ( \"2011-01\" ) Out[32]: Period('2011-01', 'M') In [33]: pd . Period ( \"2012-05\" , freq = \"D\" ) Out[33]: Period('2012-05-01', 'D') Timestamp and Period can serve as an index. Lists of Timestamp and Period are automatically coerced to DatetimeIndex and PeriodIndex respectively. In [34]: dates = [ ....: pd . Timestamp ( \"2012-05-01\" ), ....: pd . Timestamp ( \"2012-05-02\" ), ....: pd . Timestamp ( \"2012-05-03\" ), ....: ] ....: In [35]: ts = pd . Series ( np . random . randn ( 3 ), dates ) In [36]: type ( ts . index ) Out[36]: pandas.core.indexes.datetimes.DatetimeIndex In [37]: ts . index Out[37]: DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None) In [38]: ts Out[38]: 2012-05-01 0.469112 2012-05-02 -0.282863 2012-05-03 -1.509059 dtype: float64 In [39]: periods = [ pd . Period ( \"2012-01\" ), pd . Period ( \"2012-02\" ), pd . Period ( \"2012-03\" )] In [40]: ts = pd . Series ( np . random . randn ( 3 ), periods ) In [41]: type ( ts . index ) Out[41]: pandas.core.indexes.period.PeriodIndex In [42]: ts . index Out[42]: PeriodIndex(['2012-01', '2012-02', '2012-03'], dtype='period[M]') In [43]: ts Out[43]: 2012-01 -1.135632 2012-02 1.212112 2012-03 -0.173215 Freq: M, dtype: float64 pandas allows you to capture both representations and convert between them. Under the hood, pandas represents timestamps using instances of Timestamp and sequences of timestamps using instances of DatetimeIndex . For regular time spans, pandas uses Period objects for scalar values and PeriodIndex for sequences of spans. Better support for irregular intervals with arbitrary start and end points are forth-coming in future releases. Converting to timestamps # To convert a Series or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the to_datetime function. When passed a Series , this returns a Series (with the same index), while a list-like is converted to a DatetimeIndex : In [44]: pd . to_datetime ( pd . Series ([ \"Jul 31, 2009\" , \"Jan 10, 2010\" , None ])) Out[44]: 0 2009-07-31 1 2010-01-10 2 NaT dtype: datetime64[ns] In [45]: pd . to_datetime ([ \"2005/11/23\" , \"2010/12/31\" ]) Out[45]: DatetimeIndex(['2005-11-23', '2010-12-31'], dtype='datetime64[ns]', freq=None) If you use dates which start with the day first (i.e. European style), you can pass the dayfirst flag: In [46]: pd . to_datetime ([ \"04-01-2012 10:00\" ], dayfirst = True ) Out[46]: DatetimeIndex(['2012-01-04 10:00:00'], dtype='datetime64[ns]', freq=None) In [47]: pd . to_datetime ([ \"04-14-2012 10:00\" ], dayfirst = True ) Out[47]: DatetimeIndex(['2012-04-14 10:00:00'], dtype='datetime64[ns]', freq=None) Warning You see in the above example that dayfirst isnât strict. If a date canât be parsed with the day being first it will be parsed as if dayfirst were False and a warning will also be raised. If you pass a single string to to_datetime , it returns a single Timestamp . Timestamp can also accept string input, but it doesnât accept string parsing options like dayfirst or format , so use to_datetime if these are required. In [48]: pd . to_datetime ( \"2010/11/12\" ) Out[48]: Timestamp('2010-11-12 00:00:00') In [49]: pd . Timestamp ( \"2010/11/12\" ) Out[49]: Timestamp('2010-11-12 00:00:00') You can also use the DatetimeIndex constructor directly: In [50]: pd . DatetimeIndex ([ \"2018-01-01\" , \"2018-01-03\" , \"2018-01-05\" ]) Out[50]: DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-05'], dtype='datetime64[ns]', freq=None) The string âinferâ can be passed in order to set the frequency of the index as the inferred frequency upon creation: In [51]: pd . DatetimeIndex ([ \"2018-01-01\" , \"2018-01-03\" , \"2018-01-05\" ], freq = \"infer\" ) Out[51]: DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-05'], dtype='datetime64[ns]', freq='2D') Providing a format argument # In addition to the required datetime string, a format argument can be passed to ensure specific parsing. This could also potentially speed up the conversion considerably. In [52]: pd . to_datetime ( \"2010/11/12\" , format = \"%Y/%m/ %d \" ) Out[52]: Timestamp('2010-11-12 00:00:00') In [53]: pd . to_datetime ( \"12-11-2010 00:00\" , format = \" %d -%m-%Y %H:%M\" ) Out[53]: Timestamp('2010-11-12 00:00:00') For more information on the choices available when specifying the format option, see the Python datetime documentation . Assembling datetime from multiple DataFrame columns # You can also pass a DataFrame of integer or string columns to assemble into a Series of Timestamps . In [54]: df = pd . DataFrame ( ....: { \"year\" : [ 2015 , 2016 ], \"month\" : [ 2 , 3 ], \"day\" : [ 4 , 5 ], \"hour\" : [ 2 , 3 ]} ....: ) ....: In [55]: pd . to_datetime ( df ) Out[55]: 0 2015-02-04 02:00:00 1 2016-03-05 03:00:00 dtype: datetime64[ns] You can pass only the columns that you need to assemble. In [56]: pd . to_datetime ( df [[ \"year\" , \"month\" , \"day\" ]]) Out[56]: 0 2015-02-04 1 2016-03-05 dtype: datetime64[ns] pd.to_datetime looks for standard designations of the datetime component in the column names, including: required: year , month , day optional: hour , minute , second , millisecond , microsecond , nanosecond Invalid data # The default behavior, errors='raise' , is to raise when unparsable: In [57]: pd . to_datetime ([ '2009/07/31' , 'asd' ], errors = 'raise' ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [ 57 ], line 1 ----> 1 pd . to_datetime ([ '2009/07/31' , 'asd' ], errors = 'raise' ) File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:1104, in to_datetime (arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache) 1102 result = _convert_and_box_cache ( argc , cache_array ) 1103 else : -> 1104 result = convert_listlike ( argc , format ) 1105 else : 1106 result = convert_listlike ( np . array ([ arg ]), format )[ 0 ] File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:435, in _convert_listlike_datetimes (arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact) 433 # `format` could be inferred, or user didn't ask for mixed-format parsing. 434 if format is not None and format != \"mixed\" : --> 435 return _array_strptime_with_fallback ( arg , name , utc , format , exact , errors ) 437 result , tz_parsed = objects_to_datetime64 ( 438 arg , 439 dayfirst = dayfirst , ( ... ) 443 allow_object = True , 444 ) 446 if tz_parsed is not None : 447 # We can take a shortcut since the datetime64 numpy array 448 # is in UTC File ~/work/pandas/pandas/pandas/core/tools/datetimes.py:469, in _array_strptime_with_fallback (arg, name, utc, fmt, exact, errors) 458 def _array_strptime_with_fallback ( 459 arg , 460 name , ( ... ) 464 errors : str , 465 ) -> Index : 466 \"\"\" 467 Call array_strptime, with fallback behavior depending on 'errors'. 468 \"\"\" --> 469 result , tz_out = array_strptime ( arg , fmt , exact = exact , errors = errors , utc = utc ) 470 if tz_out is not None : 471 unit = np . datetime_data ( result . dtype )[ 0 ] File ~/work/pandas/pandas/pandas/_libs/tslibs/strptime.pyx:501, in pandas._libs.tslibs.strptime.array_strptime () File ~/work/pandas/pandas/pandas/_libs/tslibs/strptime.pyx:451, in pandas._libs.tslibs.strptime.array_strptime () File ~/work/pandas/pandas/pandas/_libs/tslibs/strptime.pyx:583, in pandas._libs.tslibs.strptime._parse_with_format () ValueError : time data \"asd\" doesn't match format \"%Y/%m/%d\", at position 1. You might want to try: - passing ` format ` if your strings have a consistent format ; - passing ` format = 'ISO8601' ` if your strings are all ISO8601 but not necessarily in exactly the same format ; - passing ` format = 'mixed' ` , and the format will be inferred for each element individually . You might want to use ` dayfirst ` alongside this . Pass errors='coerce' to convert unparsable data to NaT (not a time): In [58]: pd . to_datetime ([ \"2009/07/31\" , \"asd\" ], errors = \"coerce\" ) Out[58]: DatetimeIndex(['2009-07-31', 'NaT'], dtype='datetime64[ns]', freq=None) Epoch timestamps # pandas supports converting integer or float epoch times to Timestamp and DatetimeIndex . The default unit is nanoseconds, since that is how Timestamp objects are stored internally. However, epochs are often stored in another unit which can be specified. These are computed from the starting point specified by the origin parameter. In [59]: pd . to_datetime ( ....: [ 1349720105 , 1349806505 , 1349892905 , 1349979305 , 1350065705 ], unit = \"s\" ....: ) ....: Out[59]: DatetimeIndex(['2012-10-08 18:15:05', '2012-10-09 18:15:05', '2012-10-10 18:15:05', '2012-10-11 18:15:05', '2012-10-12 18:15:05'], dtype='datetime64[ns]', freq=None) In [60]: pd . to_datetime ( ....: [ 1349720105100 , 1349720105200 , 1349720105300 , 1349720105400 , 1349720105500 ], ....: unit = \"ms\" , ....: ) ....: Out[60]: DatetimeIndex(['2012-10-08 18:15:05.100000', '2012-10-08 18:15:05.200000', '2012-10-08 18:15:05.300000', '2012-10-08 18:15:05.400000', '2012-10-08 18:15:05.500000'], dtype='datetime64[ns]', freq=None) Note The unit parameter does not use the same strings as the format parameter that was discussed above ). The available units are listed on the documentation for pandas.to_datetime() . Constructing a Timestamp or DatetimeIndex with an epoch timestamp with the tz argument specified will raise a ValueError. If you have epochs in wall time in another timezone, you can read the epochs as timezone-naive timestamps and then localize to the appropriate timezone: In [61]: pd . Timestamp ( 1262347200000000000 ) . tz_localize ( \"US/Pacific\" ) Out[61]: Timestamp('2010-01-01 12:00:00-0800', tz='US/Pacific') In [62]: pd . DatetimeIndex ([ 1262347200000000000 ]) . tz_localize ( \"US/Pacific\" ) Out[62]: DatetimeIndex(['2010-01-01 12:00:00-08:00'], dtype='datetime64[ns, US/Pacific]', freq=None) Note Epoch times will be rounded to the nearest nanosecond. Warning Conversion of float epoch times can lead to inaccurate and unexpected results. Python floats have about 15 digits precision in decimal. Rounding during conversion from float to high precision Timestamp is unavoidable. The only way to achieve exact precision is to use a fixed-width types (e.g. an int64). In [63]: pd . to_datetime ([ 1490195805.433 , 1490195805.433502912 ], unit = \"s\" ) Out[63]: DatetimeIndex(['2017-03-22 15:16:45.433000088', '2017-03-22 15:16:45.433502913'], dtype='datetime64[ns]', freq=None) In [64]: pd . to_datetime ( 1490195805433502912 , unit = \"ns\" ) Out[64]: Timestamp('2017-03-22 15:16:45.433502912') See also Using the origin parameter From timestamps to epoch # To invert the operation from above, namely, to convert from a Timestamp to a âunixâ epoch: In [65]: stamps = pd . date_range ( \"2012-10-08 18:15:05\" , periods = 4 , freq = \"D\" ) In [66]: stamps Out[66]: DatetimeIndex(['2012-10-08 18:15:05', '2012-10-09 18:15:05', '2012-10-10 18:15:05', '2012-10-11 18:15:05'], dtype='datetime64[ns]', freq='D') We subtract the epoch (midnight at January 1, 1970 UTC) and then floor divide by the âunitâ (1 second). In [67]: ( stamps - pd . Timestamp ( \"1970-01-01\" )) // pd . Timedelta ( \"1s\" ) Out[67]: Index([1349720105, 1349806505, 1349892905, 1349979305], dtype='int64') Using the origin parameter # Using the origin parameter, one can specify an alternative starting point for creation of a DatetimeIndex . For example, to use 1960-01-01 as the starting date: In [68]: pd . to_datetime ([ 1 , 2 , 3 ], unit = \"D\" , origin = pd . Timestamp ( \"1960-01-01\" )) Out[68]: DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], dtype='datetime64[ns]', freq=None) The default is set at origin='unix' , which defaults to 1970-01-01 00:00:00 . Commonly called âunix epochâ or POSIX time. In [69]: pd . to_datetime ([ 1 , 2 , 3 ], unit = \"D\" ) Out[69]: DatetimeIndex(['1970-01-02', '1970-01-03', '1970-01-04'], dtype='datetime64[ns]', freq=None) Generating ranges of timestamps # To generate an index with timestamps, you can use either the DatetimeIndex or Index constructor and pass in a list of datetime objects: In [70]: dates = [ ....: datetime . datetime ( 2012 , 5 , 1 ), ....: datetime . datetime ( 2012 , 5 , 2 ), ....: datetime . datetime ( 2012 , 5 , 3 ), ....: ] ....: # Note the frequency information In [71]: index = pd . DatetimeIndex ( dates ) In [72]: index Out[72]: DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None) # Automatically converted to DatetimeIndex In [73]: index = pd . Index ( dates ) In [74]: index Out[74]: DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None) In practice this becomes very cumbersome because we often need a very long index with a large number of timestamps. If we need timestamps on a regular frequency, we can use the date_range() and bdate_range() functions to create a DatetimeIndex . The default frequency for date_range is a calendar day while the default for bdate_range is a business day : In [75]: start = datetime . datetime ( 2011 , 1 , 1 ) In [76]: end = datetime . datetime ( 2012 , 1 , 1 ) In [77]: index = pd . date_range ( start , end ) In [78]: index Out[78]: DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-08', '2011-01-09', '2011-01-10', ... '2011-12-23', '2011-12-24', '2011-12-25', '2011-12-26', '2011-12-27', '2011-12-28', '2011-12-29', '2011-12-30', '2011-12-31', '2012-01-01'], dtype='datetime64[ns]', length=366, freq='D') In [79]: index = pd . bdate_range ( start , end ) In [80]: index Out[80]: DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13', '2011-01-14', ... '2011-12-19', '2011-12-20', '2011-12-21', '2011-12-22', '2011-12-23', '2011-12-26', '2011-12-27', '2011-12-28', '2011-12-29', '2011-12-30'], dtype='datetime64[ns]', length=260, freq='B') Convenience functions like date_range and bdate_range can utilize a variety of frequency aliases : In [81]: pd . date_range ( start , periods = 1000 , freq = \"ME\" ) Out[81]: DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-30', '2011-05-31', '2011-06-30', '2011-07-31', '2011-08-31', '2011-09-30', '2011-10-31', ... '2093-07-31', '2093-08-31', '2093-09-30', '2093-10-31', '2093-11-30', '2093-12-31', '2094-01-31', '2094-02-28', '2094-03-31', '2094-04-30'], dtype='datetime64[ns]', length=1000, freq='ME') In [82]: pd . bdate_range ( start , periods = 250 , freq = \"BQS\" ) Out[82]: DatetimeIndex(['2011-01-03', '2011-04-01', '2011-07-01', '2011-10-03', '2012-01-02', '2012-04-02', '2012-07-02', '2012-10-01', '2013-01-01', '2013-04-01', ... '2071-01-01', '2071-04-01', '2071-07-01', '2071-10-01', '2072-01-01', '2072-04-01', '2072-07-01', '2072-10-03', '2073-01-02', '2073-04-03'], dtype='datetime64[ns]', length=250, freq='BQS-JAN') date_range and bdate_range make it easy to generate a range of dates using various combinations of parameters like start , end , periods , and freq . The start and end dates are strictly inclusive, so dates outside of those specified will not be generated: In [83]: pd . date_range ( start , end , freq = \"BME\" ) Out[83]: DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29', '2011-05-31', '2011-06-30', '2011-07-29', '2011-08-31', '2011-09-30', '2011-10-31', '2011-11-30', '2011-12-30'], dtype='datetime64[ns]', freq='BME') In [84]: pd . date_range ( start , end , freq = \"W\" ) Out[84]: DatetimeIndex(['2011-01-02', '2011-01-09', '2011-01-16', '2011-01-23', '2011-01-30', '2011-02-06', '2011-02-13', '2011-02-20', '2011-02-27', '2011-03-06', '2011-03-13', '2011-03-20', '2011-03-27', '2011-04-03', '2011-04-10', '2011-04-17', '2011-04-24', '2011-05-01', '2011-05-08', '2011-05-15', '2011-05-22', '2011-05-29', '2011-06-05', '2011-06-12', '2011-06-19', '2011-06-26', '2011-07-03', '2011-07-10', '2011-07-17', '2011-07-24', '2011-07-31', '2011-08-07', '2011-08-14', '2011-08-21', '2011-08-28', '2011-09-04', '2011-09-11', '2011-09-18', '2011-09-25', '2011-10-02', '2011-10-09', '2011-10-16', '2011-10-23', '2011-10-30', '2011-11-06', '2011-11-13', '2011-11-20', '2011-11-27', '2011-12-04', '2011-12-11', '2011-12-18', '2011-12-25', '2012-01-01'], dtype='datetime64[ns]', freq='W-SUN') In [85]: pd . bdate_range ( end = end , periods = 20 ) Out[85]: DatetimeIndex(['2011-12-05', '2011-12-06', '2011-12-07', '2011-12-08', '2011-12-09', '2011-12-12', '2011-12-13', '2011-12-14', '2011-12-15', '2011-12-16', '2011-12-19', '2011-12-20', '2011-12-21', '2011-12-22', '2011-12-23', '2011-12-26', '2011-12-27', '2011-12-28', '2011-12-29', '2011-12-30'], dtype='datetime64[ns]', freq='B') In [86]: pd . bdate_range ( start = start , periods = 20 ) Out[86]: DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13', '2011-01-14', '2011-01-17', '2011-01-18', '2011-01-19', '2011-01-20', '2011-01-21', '2011-01-24', '2011-01-25', '2011-01-26', '2011-01-27', '2011-01-28'], dtype='datetime64[ns]', freq='B') Specifying start , end , and periods will generate a range of evenly spaced dates from start to end inclusively, with periods number of elements in the resulting DatetimeIndex : In [87]: pd . date_range ( \"2018-01-01\" , \"2018-01-05\" , periods = 5 ) Out[87]: DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04', '2018-01-05'], dtype='datetime64[ns]', freq=None) In [88]: pd . date_range ( \"2018-01-01\" , \"2018-01-05\" , periods = 10 ) Out[88]: DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 10:40:00', '2018-01-01 21:20:00', '2018-01-02 08:00:00', '2018-01-02 18:40:00', '2018-01-03 05:20:00', '2018-01-03 16:00:00', '2018-01-04 02:40:00', '2018-01-04 13:20:00', '2018-01-05 00:00:00'], dtype='datetime64[ns]', freq=None) Custom frequency ranges # bdate_range can also generate a range of custom frequency dates by using the weekmask and holidays parameters. These parameters will only be used if a custom frequency string is passed. In [89]: weekmask = \"Mon Wed Fri\" In [90]: holidays = [ datetime . datetime ( 2011 , 1 , 5 ), datetime . datetime ( 2011 , 3 , 14 )] In [91]: pd . bdate_range ( start , end , freq = \"C\" , weekmask = weekmask , holidays = holidays ) Out[91]: DatetimeIndex(['2011-01-03', '2011-01-07', '2011-01-10', '2011-01-12', '2011-01-14', '2011-01-17', '2011-01-19', '2011-01-21', '2011-01-24', '2011-01-26', ... '2011-12-09', '2011-12-12', '2011-12-14', '2011-12-16', '2011-12-19', '2011-12-21', '2011-12-23', '2011-12-26', '2011-12-28', '2011-12-30'], dtype='datetime64[ns]', length=154, freq='C') In [92]: pd . bdate_range ( start , end , freq = \"CBMS\" , weekmask = weekmask ) Out[92]: DatetimeIndex(['2011-01-03', '2011-02-02', '2011-03-02', '2011-04-01', '2011-05-02', '2011-06-01', '2011-07-01', '2011-08-01', '2011-09-02', '2011-10-03', '2011-11-02', '2011-12-02'], dtype='datetime64[ns]', freq='CBMS') See also Custom business days Timestamp limitations # The limits of timestamp representation depend on the chosen resolution. For nanosecond resolution, the time span that can be represented using a 64-bit integer is limited to approximately 584 years: In [93]: pd . Timestamp . min Out[93]: Timestamp('1677-09-21 00:12:43.145224193') In [94]: pd . Timestamp . max Out[94]: Timestamp('2262-04-11 23:47:16.854775807') When choosing second-resolution, the available range grows to +/- 2.9e11 years . Different resolutions can be converted to each other through as_unit . See also Representing out-of-bounds spans Indexing # One of the main uses for DatetimeIndex is as an index for pandas objects. The DatetimeIndex class contains many time series related optimizations: A large range of dates for various offsets are pre-computed and cached under the hood in order to make generating subsequent date ranges very fast (just have to grab a slice). Fast shifting using the shift method on pandas objects. Unioning of overlapping DatetimeIndex objects with the same frequency is very fast (important for fast data alignment). Quick access to date fields via properties such as year , month , etc. Regularization functions like snap and very fast asof logic. DatetimeIndex objects have all the basic functionality of regular Index objects, and a smorgasbord of advanced time series specific methods for easy frequency processing. See also Reindexing methods Note While pandas does not force you to have a sorted date index, some of these methods may have unexpected or incorrect behavior if the dates are unsorted. DatetimeIndex can be used like a regular index and offers all of its intelligent functionality like selection, slicing, etc. In [95]: rng = pd . date_range ( start , end , freq = \"BME\" ) In [96]: ts = pd . Series ( np . random . randn ( len ( rng )), index = rng ) In [97]: ts . index Out[97]: DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29', '2011-05-31', '2011-06-30', '2011-07-29', '2011-08-31', '2011-09-30', '2011-10-31', '2011-11-30', '2011-12-30'], dtype='datetime64[ns]', freq='BME') In [98]: ts [: 5 ] . index Out[98]: DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29', '2011-05-31'], dtype='datetime64[ns]', freq='BME') In [99]: ts [:: 2 ] . index Out[99]: DatetimeIndex(['2011-01-31', '2011-03-31', '2011-05-31', '2011-07-29', '2011-09-30', '2011-11-30'], dtype='datetime64[ns]', freq='2BME') Partial string indexing # Dates and strings that parse to timestamps can be passed as indexing parameters: In [100]: ts [ \"1/31/2011\" ] Out[100]: 0.11920871129693428 In [101]: ts [ datetime . datetime ( 2011 , 12 , 25 ):] Out[101]: 2011-12-30 0.56702 Freq: BME, dtype: float64 In [102]: ts [ \"10/31/2011\" : \"12/31/2011\" ] Out[102]: 2011-10-31 0.271860 2011-11-30 -0.424972 2011-12-30 0.567020 Freq: BME, dtype: float64 To provide convenience for accessing longer time series, you can also pass in the year or year and month as strings: In [103]: ts [ \"2011\" ] Out[103]: 2011-01-31 0.119209 2011-02-28 -1.044236 2011-03-31 -0.861849 2011-04-29 -2.104569 2011-05-31 -0.494929 2011-06-30 1.071804 2011-07-29 0.721555 2011-08-31 -0.706771 2011-09-30 -1.039575 2011-10-31 0.271860 2011-11-30 -0.424972 2011-12-30 0.567020 Freq: BME, dtype: float64 In [104]: ts [ \"2011-6\" ] Out[104]: 2011-06-30 1.071804 Freq: BME, dtype: float64 This type of slicing will work on a DataFrame with a DatetimeIndex as well. Since the partial string selection is a form of label slicing, the endpoints will be included. This would include matching times on an included date: Warning Indexing DataFrame rows with a single string with getitem (e.g. frame[dtstring] ) is deprecated starting with pandas 1.2.0 (given the ambiguity whether it is indexing the rows or selecting a column) and will be removed in a future version. The equivalent with .loc (e.g. frame.loc[dtstring] ) is still supported. In [105]: dft = pd . DataFrame ( .....: np . random . randn ( 100000 , 1 ), .....: columns = [ \"A\" ], .....: index = pd . date_range ( \"20130101\" , periods = 100000 , freq = \"min\" ), .....: ) .....: In [106]: dft Out[106]: A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-03-11 10:35:00 -0.747967 2013-03-11 10:36:00 -0.034523 2013-03-11 10:37:00 -0.201754 2013-03-11 10:38:00 -1.509067 2013-03-11 10:39:00 -1.693043 [100000 rows x 1 columns] In [107]: dft . loc [ \"2013\" ] Out[107]: A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-03-11 10:35:00 -0.747967 2013-03-11 10:36:00 -0.034523 2013-03-11 10:37:00 -0.201754 2013-03-11 10:38:00 -1.509067 2013-03-11 10:39:00 -1.693043 [100000 rows x 1 columns] This starts on the very first time in the month, and includes the last date and time for the month: In [108]: dft [ \"2013-1\" : \"2013-2\" ] Out[108]: A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-28 23:55:00 0.850929 2013-02-28 23:56:00 0.976712 2013-02-28 23:57:00 -2.693884 2013-02-28 23:58:00 -1.575535 2013-02-28 23:59:00 -1.573517 [84960 rows x 1 columns] This specifies a stop time that includes all of the times on the last day : In [109]: dft [ \"2013-1\" : \"2013-2-28\" ] Out[109]: A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-28 23:55:00 0.850929 2013-02-28 23:56:00 0.976712 2013-02-28 23:57:00 -2.693884 2013-02-28 23:58:00 -1.575535 2013-02-28 23:59:00 -1.573517 [84960 rows x 1 columns] This specifies an exact stop time (and is not the same as the above): In [110]: dft [ \"2013-1\" : \"2013-2-28 00:00:00\" ] Out[110]: A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-27 23:56:00 1.197749 2013-02-27 23:57:00 0.720521 2013-02-27 23:58:00 -0.072718 2013-02-27 23:59:00 -0.681192 2013-02-28 00:00:00 -0.557501 [83521 rows x 1 columns] We are stopping on the included end-point as it is part of the index: In [111]: dft [ \"2013-1-15\" : \"2013-1-15 12:30:00\" ] Out[111]: A 2013-01-15 00:00:00 -0.984810 2013-01-15 00:01:00 0.941451 2013-01-15 00:02:00 1.559365 2013-01-15 00:03:00 1.034374 2013-01-15 00:04:00 -1.480656 ... ... 2013-01-15 12:26:00 0.371454 2013-01-15 12:27:00 -0.930806 2013-01-15 12:28:00 -0.069177 2013-01-15 12:29:00 0.066510 2013-01-15 12:30:00 -0.003945 [751 rows x 1 columns] DatetimeIndex partial string indexing also works on a DataFrame with a MultiIndex : In [112]: dft2 = pd . DataFrame ( .....: np . random . randn ( 20 , 1 ), .....: columns = [ \"A\" ], .....: index = pd . MultiIndex . from_product ( .....: [ pd . date_range ( \"20130101\" , periods = 10 , freq = \"12h\" ), [ \"a\" , \"b\" ]] .....: ), .....: ) .....: In [113]: dft2 Out[113]: A 2013-01-01 00:00:00 a -0.298694 b 0.823553 2013-01-01 12:00:00 a 0.943285 b -1.479399 2013-01-02 00:00:00 a -1.643342 ... ... 2013-01-04 12:00:00 b 0.069036 2013-01-05 00:00:00 a 0.122297 b 1.422060 2013-01-05 12:00:00 a 0.370079 b 1.016331 [20 rows x 1 columns] In [114]: dft2 . loc [ \"2013-01-05\" ] Out[114]: A 2013-01-05 00:00:00 a 0.122297 b 1.422060 2013-01-05 12:00:00 a 0.370079 b 1.016331 In [115]: idx = pd . IndexSlice In [116]: dft2 = dft2 . swaplevel ( 0 , 1 ) . sort_index () In [117]: dft2 . loc [ idx [:, \"2013-01-05\" ], :] Out[117]: A a 2013-01-05 00:00:00 0.122297 2013-01-05 12:00:00 0.370079 b 2013-01-05 00:00:00 1.422060 2013-01-05 12:00:00 1.016331 Slicing with string indexing also honors UTC offset. In [118]: df = pd . DataFrame ([ 0 ], index = pd . DatetimeIndex ([ \"2019-01-01\" ], tz = \"US/Pacific\" )) In [119]: df Out[119]: 0 2019-01-01 00:00:00-08:00 0 In [120]: df [ \"2019-01-01 12:00:00+04:00\" : \"2019-01-01 13:00:00+04:00\" ] Out[120]: 0 2019-01-01 00:00:00-08:00 0 Slice vs. exact match # The same string used as an indexing parameter can be treated either as a slice or as an exact match depending on the resolution of the index. If the string is less accurate than the index, it will be treated as a slice, otherwise as an exact match. Consider a Series object with a minute resolution index: In [121]: series_minute = pd . Series ( .....: [ 1 , 2 , 3 ], .....: pd . DatetimeIndex ( .....: [ \"2011-12-31 23:59:00\" , \"2012-01-01 00:00:00\" , \"2012-01-01 00:02:00\" ] .....: ), .....: ) .....: In [122]: series_minute . index . resolution Out[122]: 'minute' A timestamp string less accurate than a minute gives a Series object. In [123]: series_minute [ \"2011-12-31 23\" ] Out[123]: 2011-12-31 23:59:00 1 dtype: int64 A timestamp string with minute resolution (or more accurate), gives a scalar instead, i.e. it is not casted to a slice. In [124]: series_minute [ \"2011-12-31 23:59\" ] Out[124]: 1 In [125]: series_minute [ \"2011-12-31 23:59:00\" ] Out[125]: 1 If index resolution is second, then the minute-accurate timestamp gives a Series . In [126]: series_second = pd . Series ( .....: [ 1 , 2 , 3 ], .....: pd . DatetimeIndex ( .....: [ \"2011-12-31 23:59:59\" , \"2012-01-01 00:00:00\" , \"2012-01-01 00:00:01\" ] .....: ), .....: ) .....: In [127]: series_second . index . resolution Out[127]: 'second' In [128]: series_second [ \"2011-12-31 23:59\" ] Out[128]: 2011-12-31 23:59:59 1 dtype: int64 If the timestamp string is treated as a slice, it can be used to index DataFrame with .loc[] as well. In [129]: dft_minute = pd . DataFrame ( .....: { \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}, index = series_minute . index .....: ) .....: In [130]: dft_minute . loc [ \"2011-12-31 23\" ] Out[130]: a b 2011-12-31 23:59:00 1 4 Warning However, if the string is treated as an exact match, the selection in DataFrame âs [] will be column-wise and not row-wise, see Indexing Basics . For example dft_minute['2011-12-31 23:59'] will raise KeyError as '2012-12-31 23:59' has the same resolution as the index and there is no column with such name: To always have unambiguous selection, whether the row is treated as a slice or a single selection, use .loc . In [131]: dft_minute . loc [ \"2011-12-31 23:59\" ] Out[131]: a 1 b 4 Name: 2011-12-31 23:59:00, dtype: int64 Note also that DatetimeIndex resolution cannot be less precise than day. In [132]: series_monthly = pd . Series ( .....: [ 1 , 2 , 3 ], pd . DatetimeIndex ([ \"2011-12\" , \"2012-01\" , \"2012-02\" ]) .....: ) .....: In [133]: series_monthly . index . resolution Out[133]: 'day' In [134]: series_monthly [ \"2011-12\" ] # returns Series Out[134]: 2011-12-01 1 dtype: int64 Exact indexing # As discussed in previous section, indexing a DatetimeIndex with a partial string depends on the âaccuracyâ of the period, in other words how specific the interval is in relation to the resolution of the index. In contrast, indexing with Timestamp or datetime objects is exact, because the objects have exact meaning. These also follow the semantics of including both endpoints . These Timestamp and datetime objects have exact hours, minutes, and seconds , even though they were not explicitly specified (they are 0 ). In [135]: dft [ datetime . datetime ( 2013 , 1 , 1 ): datetime . datetime ( 2013 , 2 , 28 )] Out[135]: A 2013-01-01 00:00:00 0.276232 2013-01-01 00:01:00 -1.087401 2013-01-01 00:02:00 -0.673690 2013-01-01 00:03:00 0.113648 2013-01-01 00:04:00 -1.478427 ... ... 2013-02-27 23:56:00 1.197749 2013-02-27 23:57:00 0.720521 2013-02-27 23:58:00 -0.072718 2013-02-27 23:59:00 -0.681192 2013-02-28 00:00:00 -0.557501 [83521 rows x 1 columns] With no defaults. In [136]: dft [ .....: datetime . datetime ( 2013 , 1 , 1 , 10 , 12 , 0 ): datetime . datetime ( .....: 2013 , 2 , 28 , 10 , 12 , 0 .....: ) .....: ] .....: Out[136]: A 2013-01-01 10:12:00 0.565375 2013-01-01 10:13:00 0.068184 2013-01-01 10:14:00 0.788871 2013-01-01 10:15:00 -0.280343 2013-01-01 10:16:00 0.931536 ... ... 2013-02-28 10:08:00 0.148098 2013-02-28 10:09:00 -0.388138 2013-02-28 10:10:00 0.139348 2013-02-28 10:11:00 0.085288 2013-02-28 10:12:00 0.950146 [83521 rows x 1 columns] Truncating & fancy indexing # A truncate() convenience function is provided that is similar to slicing. Note that truncate assumes a 0 value for any unspecified date component in a DatetimeIndex in contrast to slicing which returns any partially matching dates: In [137]: rng2 = pd . date_range ( \"2011-01-01\" , \"2012-01-01\" , freq = \"W\" ) In [138]: ts2 = pd . Series ( np . random . randn ( len ( rng2 )), index = rng2 ) In [139]: ts2 . truncate ( before = \"2011-11\" , after = \"2011-12\" ) Out[139]: 2011-11-06 0.437823 2011-11-13 -0.293083 2011-11-20 -0.059881 2011-11-27 1.252450 Freq: W-SUN, dtype: float64 In [140]: ts2 [ \"2011-11\" : \"2011-12\" ] Out[140]: 2011-11-06 0.437823 2011-11-13 -0.293083 2011-11-20 -0.059881 2011-11-27 1.252450 2011-12-04 0.046611 2011-12-11 0.059478 2011-12-18 -0.286539 2011-12-25 0.841669 Freq: W-SUN, dtype: float64 Even complicated fancy indexing that breaks the DatetimeIndex frequency regularity will result in a DatetimeIndex , although frequency is lost: In [141]: ts2 . iloc [[ 0 , 2 , 6 ]] . index Out[141]: DatetimeIndex(['2011-01-02', '2011-01-16', '2011-02-13'], dtype='datetime64[ns]', freq=None) Time/date components # There are several time/date properties that one can access from Timestamp or a collection of timestamps like a DatetimeIndex . Property Description year The year of the datetime month The month of the datetime day The days of the datetime hour The hour of the datetime minute The minutes of the datetime second The seconds of the datetime microsecond The microseconds of the datetime nanosecond The nanoseconds of the datetime date Returns datetime.date (does not contain timezone information) time Returns datetime.time (does not contain timezone information) timetz Returns datetime.time as local time with timezone information dayofyear The ordinal day of year day_of_year The ordinal day of year weekofyear The week ordinal of the year week The week ordinal of the year dayofweek The number of the day of the week with Monday=0, Sunday=6 day_of_week The number of the day of the week with Monday=0, Sunday=6 weekday The number of the day of the week with Monday=0, Sunday=6 quarter Quarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc. days_in_month The number of days in the month of the datetime is_month_start Logical indicating if first day of month (defined by frequency) is_month_end Logical indicating if last day of month (defined by frequency) is_quarter_start Logical indicating if first day of quarter (defined by frequency) is_quarter_end Logical indicating if last day of quarter (defined by frequency) is_year_start Logical indicating if first day of year (defined by frequency) is_year_end Logical indicating if last day of year (defined by frequency) is_leap_year Logical indicating if the date belongs to a leap year Furthermore, if you have a Series with datetimelike values, then you can access these properties via the .dt accessor, as detailed in the section on .dt accessors . You may obtain the year, week and day components of the ISO year from the ISO 8601 standard: In [142]: idx = pd . date_range ( start = \"2019-12-29\" , freq = \"D\" , periods = 4 ) In [143]: idx . isocalendar () Out[143]: year week day 2019-12-29 2019 52 7 2019-12-30 2020 1 1 2019-12-31 2020 1 2 2020-01-01 2020 1 3 In [144]: idx . to_series () . dt . isocalendar () Out[144]: year week day 2019-12-29 2019 52 7 2019-12-30 2020 1 1 2019-12-31 2020 1 2 2020-01-01 2020 1 3 DateOffset objects # In the preceding examples, frequency strings (e.g. 'D' ) were used to specify a frequency that defined: how the date times in DatetimeIndex were spaced when using date_range() the frequency of a Period or PeriodIndex These frequency strings map to a DateOffset object and its subclasses. A DateOffset is similar to a Timedelta that represents a duration of time but follows specific calendar duration rules. For example, a Timedelta day will always increment datetimes by 24 hours, while a DateOffset day will increment datetimes to the same time the next day whether a day represents 23, 24 or 25 hours due to daylight savings time. However, all DateOffset subclasses that are an hour or smaller ( Hour , Minute , Second , Milli , Micro , Nano ) behave like Timedelta and respect absolute time. The basic DateOffset acts similar to dateutil.relativedelta ( relativedelta documentation ) that shifts a date time by the corresponding calendar duration specified. The arithmetic operator ( + ) can be used to perform the shift. # This particular day contains a day light savings time transition In [145]: ts = pd . Timestamp ( \"2016-10-30 00:00:00\" , tz = \"Europe/Helsinki\" ) # Respects absolute time In [146]: ts + pd . Timedelta ( days = 1 ) Out[146]: Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki') # Respects calendar time In [147]: ts + pd . DateOffset ( days = 1 ) Out[147]: Timestamp('2016-10-31 00:00:00+0200', tz='Europe/Helsinki') In [148]: friday = pd . Timestamp ( \"2018-01-05\" ) In [149]: friday . day_name () Out[149]: 'Friday' # Add 2 business days (Friday --> Tuesday) In [150]: two_business_days = 2 * pd . offsets . BDay () In [151]: friday + two_business_days Out[151]: Timestamp('2018-01-09 00:00:00') In [152]: ( friday + two_business_days ) . day_name () Out[152]: 'Tuesday' Most DateOffsets have associated frequencies strings, or offset aliases, that can be passed into freq keyword arguments. The available date offsets and associated frequency strings can be found below: Date Offset Frequency String Description DateOffset None Generic offset class, defaults to absolute 24 hours BDay or BusinessDay 'B' business day (weekday) CDay or CustomBusinessDay 'C' custom business day Week 'W' one week, optionally anchored on a day of the week WeekOfMonth 'WOM' the x-th day of the y-th week of each month LastWeekOfMonth 'LWOM' the x-th day of the last week of each month MonthEnd 'ME' calendar month end MonthBegin 'MS' calendar month begin BMonthEnd or BusinessMonthEnd 'BME' business month end BMonthBegin or BusinessMonthBegin 'BMS' business month begin CBMonthEnd or CustomBusinessMonthEnd 'CBME' custom business month end CBMonthBegin or CustomBusinessMonthBegin 'CBMS' custom business month begin SemiMonthEnd 'SME' 15th (or other day_of_month) and calendar month end SemiMonthBegin 'SMS' 15th (or other day_of_month) and calendar month begin QuarterEnd 'QE' calendar quarter end QuarterBegin 'QS' calendar quarter begin BQuarterEnd 'BQE business quarter end BQuarterBegin 'BQS' business quarter begin FY5253Quarter 'REQ' retail (aka 52-53 week) quarter YearEnd 'YE' calendar year end YearBegin 'YS' or 'BYS' calendar year begin BYearEnd 'BYE' business year end BYearBegin 'BYS' business year begin FY5253 'RE' retail (aka 52-53 week) year Easter None Easter holiday BusinessHour 'bh' business hour CustomBusinessHour 'cbh' custom business hour Day 'D' one absolute day Hour 'h' one hour Minute 'min' one minute Second 's' one second Milli 'ms' one millisecond Micro 'us' one microsecond Nano 'ns' one nanosecond DateOffsets additionally have rollforward() and rollback() methods for moving a date forward or backward respectively to a valid offset date relative to the offset. For example, business offsets will roll dates that land on the weekends (Saturday and Sunday) forward to Monday since business offsets operate on the weekdays. In [153]: ts = pd . Timestamp ( \"2018-01-06 00:00:00\" ) In [154]: ts . day_name () Out[154]: 'Saturday' # BusinessHour's valid offset dates are Monday through Friday In [155]: offset = pd . offsets . BusinessHour ( start = \"09:00\" ) # Bring the date to the closest offset date (Monday) In [156]: offset . rollforward ( ts ) Out[156]: Timestamp('2018-01-08 09:00:00') # Date is brought to the closest offset date first and then the hour is added In [157]: ts + offset Out[157]: Timestamp('2018-01-08 10:00:00') These operations preserve time (hour, minute, etc) information by default. To reset time to midnight, use normalize() before or after applying the operation (depending on whether you want the time information included in the operation). In [158]: ts = pd . Timestamp ( \"2014-01-01 09:00\" ) In [159]: day = pd . offsets . Day () In [160]: day + ts Out[160]: Timestamp('2014-01-02 09:00:00') In [161]: ( day + ts ) . normalize () Out[161]: Timestamp('2014-01-02 00:00:00') In [162]: ts = pd . Timestamp ( \"2014-01-01 22:00\" ) In [163]: hour = pd . offsets . Hour () In [164]: hour + ts Out[164]: Timestamp('2014-01-01 23:00:00') In [165]: ( hour + ts ) . normalize () Out[165]: Timestamp('2014-01-01 00:00:00') In [166]: ( hour + pd . Timestamp ( \"2014-01-01 23:30\" )) . normalize () Out[166]: Timestamp('2014-01-02 00:00:00') Parametric offsets # Some of the offsets can be âparameterizedâ when created to result in different behaviors. For example, the Week offset for generating weekly data accepts a weekday parameter which results in the generated dates always lying on a particular day of the week: In [167]: d = datetime . datetime ( 2008 , 8 , 18 , 9 , 0 ) In [168]: d Out[168]: datetime.datetime(2008, 8, 18, 9, 0) In [169]: d + pd . offsets . Week () Out[169]: Timestamp('2008-08-25 09:00:00') In [170]: d + pd . offsets . Week ( weekday = 4 ) Out[170]: Timestamp('2008-08-22 09:00:00') In [171]: ( d + pd . offsets . Week ( weekday = 4 )) . weekday () Out[171]: 4 In [172]: d - pd . offsets . Week () Out[172]: Timestamp('2008-08-11 09:00:00') The normalize option will be effective for addition and subtraction. In [173]: d + pd . offsets . Week ( normalize = True ) Out[173]: Timestamp('2008-08-25 00:00:00') In [174]: d - pd . offsets . Week ( normalize = True ) Out[174]: Timestamp('2008-08-11 00:00:00') Another example is parameterizing YearEnd with the specific ending month: In [175]: d + pd . offsets . YearEnd () Out[175]: Timestamp('2008-12-31 09:00:00') In [176]: d + pd . offsets . YearEnd ( month = 6 ) Out[176]: Timestamp('2009-06-30 09:00:00') Using offsets with Series / DatetimeIndex # Offsets can be used with either a Series or DatetimeIndex to apply the offset to each element. In [177]: rng = pd . date_range ( \"2012-01-01\" , \"2012-01-03\" ) In [178]: s = pd . Series ( rng ) In [179]: rng Out[179]: DatetimeIndex(['2012-01-01', '2012-01-02', '2012-01-03'], dtype='datetime64[ns]', freq='D') In [180]: rng + pd . DateOffset ( months = 2 ) Out[180]: DatetimeIndex(['2012-03-01', '2012-03-02', '2012-03-03'], dtype='datetime64[ns]', freq=None) In [181]: s + pd . DateOffset ( months = 2 ) Out[181]: 0 2012-03-01 1 2012-03-02 2 2012-03-03 dtype: datetime64[ns] In [182]: s - pd . DateOffset ( months = 2 ) Out[182]: 0 2011-11-01 1 2011-11-02 2 2011-11-03 dtype: datetime64[ns] If the offset class maps directly to a Timedelta ( Day , Hour , Minute , Second , Micro , Milli , Nano ) it can be used exactly like a Timedelta - see the Timedelta section for more examples. In [183]: s - pd . offsets . Day ( 2 ) Out[183]: 0 2011-12-30 1 2011-12-31 2 2012-01-01 dtype: datetime64[ns] In [184]: td = s - pd . Series ( pd . date_range ( \"2011-12-29\" , \"2011-12-31\" )) In [185]: td Out[185]: 0 3 days 1 3 days 2 3 days dtype: timedelta64[ns] In [186]: td + pd . offsets . Minute ( 15 ) Out[186]: 0 3 days 00:15:00 1 3 days 00:15:00 2 3 days 00:15:00 dtype: timedelta64[ns] Note that some offsets (such as BQuarterEnd ) do not have a vectorized implementation. They can still be used but may calculate significantly slower and will show a PerformanceWarning In [187]: rng + pd . offsets . BQuarterEnd () Out[187]: DatetimeIndex(['2012-03-30', '2012-03-30', '2012-03-30'], dtype='datetime64[ns]', freq=None) Custom business days # The CDay or CustomBusinessDay class provides a parametric BusinessDay class which can be used to create customized business day calendars which account for local holidays and local weekend conventions. As an interesting example, letâs look at Egypt where a Friday-Saturday weekend is observed. In [188]: weekmask_egypt = \"Sun Mon Tue Wed Thu\" # They also observe International Workers' Day so let's # add that for a couple of years In [189]: holidays = [ .....: \"2012-05-01\" , .....: datetime . datetime ( 2013 , 5 , 1 ), .....: np . datetime64 ( \"2014-05-01\" ), .....: ] .....: In [190]: bday_egypt = pd . offsets . CustomBusinessDay ( .....: holidays = holidays , .....: weekmask = weekmask_egypt , .....: ) .....: In [191]: dt = datetime . datetime ( 2013 , 4 , 30 ) In [192]: dt + 2 * bday_egypt Out[192]: Timestamp('2013-05-05 00:00:00') Letâs map to the weekday names: In [193]: dts = pd . date_range ( dt , periods = 5 , freq = bday_egypt ) In [194]: pd . Series ( dts . weekday , dts ) . map ( pd . Series ( \"Mon Tue Wed Thu Fri Sat Sun\" . split ())) Out[194]: 2013-04-30 Tue 2013-05-02 Thu 2013-05-05 Sun 2013-05-06 Mon 2013-05-07 Tue Freq: C, dtype: object Holiday calendars can be used to provide the list of holidays. See the holiday calendar section for more information. In [195]: from pandas.tseries.holiday import USFederalHolidayCalendar In [196]: bday_us = pd . offsets . CustomBusinessDay ( calendar = USFederalHolidayCalendar ()) # Friday before MLK Day In [197]: dt = datetime . datetime ( 2014 , 1 , 17 ) # Tuesday after MLK Day (Monday is skipped because it's a holiday) In [198]: dt + bday_us Out[198]: Timestamp('2014-01-21 00:00:00') Monthly offsets that respect a certain holiday calendar can be defined in the usual way. In [199]: bmth_us = pd . offsets . CustomBusinessMonthBegin ( calendar = USFederalHolidayCalendar ()) # Skip new years In [200]: dt = datetime . datetime ( 2013 , 12 , 17 ) In [201]: dt + bmth_us Out[201]: Timestamp('2014-01-02 00:00:00') # Define date index with custom offset In [202]: pd . date_range ( start = \"20100101\" , end = \"20120101\" , freq = bmth_us ) Out[202]: DatetimeIndex(['2010-01-04', '2010-02-01', '2010-03-01', '2010-04-01', '2010-05-03', '2010-06-01', '2010-07-01', '2010-08-02', '2010-09-01', '2010-10-01', '2010-11-01', '2010-12-01', '2011-01-03', '2011-02-01', '2011-03-01', '2011-04-01', '2011-05-02', '2011-06-01', '2011-07-01', '2011-08-01', '2011-09-01', '2011-10-03', '2011-11-01', '2011-12-01'], dtype='datetime64[ns]', freq='CBMS') Note The frequency string âCâ is used to indicate that a CustomBusinessDay DateOffset is used, it is important to note that since CustomBusinessDay is a parameterised type, instances of CustomBusinessDay may differ and this is not detectable from the âCâ frequency string. The user therefore needs to ensure that the âCâ frequency string is used consistently within the userâs application. Business hour # The BusinessHour class provides a business hour representation on BusinessDay , allowing to use specific start and end times. By default, BusinessHour uses 9:00 - 17:00 as business hours. Adding BusinessHour will increment Timestamp by hourly frequency. If target Timestamp is out of business hours, move to the next business hour then increment it. If the result exceeds the business hours end, the remaining hours are added to the next business day. In [203]: bh = pd . offsets . BusinessHour () In [204]: bh Out[204]: <BusinessHour: bh=09:00-17:00> # 2014-08-01 is Friday In [205]: pd . Timestamp ( \"2014-08-01 10:00\" ) . weekday () Out[205]: 4 In [206]: pd . Timestamp ( \"2014-08-01 10:00\" ) + bh Out[206]: Timestamp('2014-08-01 11:00:00') # Below example is the same as: pd.Timestamp('2014-08-01 09:00') + bh In [207]: pd . Timestamp ( \"2014-08-01 08:00\" ) + bh Out[207]: Timestamp('2014-08-01 10:00:00') # If the results is on the end time, move to the next business day In [208]: pd . Timestamp ( \"2014-08-01 16:00\" ) + bh Out[208]: Timestamp('2014-08-04 09:00:00') # Remainings are added to the next day In [209]: pd . Timestamp ( \"2014-08-01 16:30\" ) + bh Out[209]: Timestamp('2014-08-04 09:30:00') # Adding 2 business hours In [210]: pd . Timestamp ( \"2014-08-01 10:00\" ) + pd . offsets . BusinessHour ( 2 ) Out[210]: Timestamp('2014-08-01 12:00:00') # Subtracting 3 business hours In [211]: pd . Timestamp ( \"2014-08-01 10:00\" ) + pd . offsets . BusinessHour ( - 3 ) Out[211]: Timestamp('2014-07-31 15:00:00') You can also specify start and end time by keywords. The argument must be a str with an hour:minute representation or a datetime.time instance. Specifying seconds, microseconds and nanoseconds as business hour results in ValueError . In [212]: bh = pd . offsets . BusinessHour ( start = \"11:00\" , end = datetime . time ( 20 , 0 )) In [213]: bh Out[213]: <BusinessHour: bh=11:00-20:00> In [214]: pd . Timestamp ( \"2014-08-01 13:00\" ) + bh Out[214]: Timestamp('2014-08-01 14:00:00') In [215]: pd . Timestamp ( \"2014-08-01 09:00\" ) + bh Out[215]: Timestamp('2014-08-01 12:00:00') In [216]: pd . Timestamp ( \"2014-08-01 18:00\" ) + bh Out[216]: Timestamp('2014-08-01 19:00:00') Passing start time later than end represents midnight business hour. In this case, business hour exceeds midnight and overlap to the next day. Valid business hours are distinguished by whether it started from valid BusinessDay . In [217]: bh = pd . offsets . BusinessHour ( start = \"17:00\" , end = \"09:00\" ) In [218]: bh Out[218]: <BusinessHour: bh=17:00-09:00> In [219]: pd . Timestamp ( \"2014-08-01 17:00\" ) + bh Out[219]: Timestamp('2014-08-01 18:00:00') In [220]: pd . Timestamp ( \"2014-08-01 23:00\" ) + bh Out[220]: Timestamp('2014-08-02 00:00:00') # Although 2014-08-02 is Saturday, # it is valid because it starts from 08-01 (Friday). In [221]: pd . Timestamp ( \"2014-08-02 04:00\" ) + bh Out[221]: Timestamp('2014-08-02 05:00:00') # Although 2014-08-04 is Monday, # it is out of business hours because it starts from 08-03 (Sunday). In [222]: pd . Timestamp ( \"2014-08-04 04:00\" ) + bh Out[222]: Timestamp('2014-08-04 18:00:00') Applying BusinessHour.rollforward and rollback to out of business hours results in the next business hour start or previous dayâs end. Different from other offsets, BusinessHour.rollforward may output different results from apply by definition. This is because one dayâs business hour end is equal to next dayâs business hour start. For example, under the default business hours (9:00 - 17:00), there is no gap (0 minutes) between 2014-08-01 17:00 and 2014-08-04 09:00 . # This adjusts a Timestamp to business hour edge In [223]: pd . offsets . BusinessHour () . rollback ( pd . Timestamp ( \"2014-08-02 15:00\" )) Out[223]: Timestamp('2014-08-01 17:00:00') In [224]: pd . offsets . BusinessHour () . rollforward ( pd . Timestamp ( \"2014-08-02 15:00\" )) Out[224]: Timestamp('2014-08-04 09:00:00') # It is the same as BusinessHour() + pd.Timestamp('2014-08-01 17:00'). # And it is the same as BusinessHour() + pd.Timestamp('2014-08-04 09:00') In [225]: pd . offsets . BusinessHour () + pd . Timestamp ( \"2014-08-02 15:00\" ) Out[225]: Timestamp('2014-08-04 10:00:00') # BusinessDay results (for reference) In [226]: pd . offsets . BusinessHour () . rollforward ( pd . Timestamp ( \"2014-08-02\" )) Out[226]: Timestamp('2014-08-04 09:00:00') # It is the same as BusinessDay() + pd.Timestamp('2014-08-01') # The result is the same as rollworward because BusinessDay never overlap. In [227]: pd . offsets . BusinessHour () + pd . Timestamp ( \"2014-08-02\" ) Out[227]: Timestamp('2014-08-04 10:00:00') BusinessHour regards Saturday and Sunday as holidays. To use arbitrary holidays, you can use CustomBusinessHour offset, as explained in the following subsection. Custom business hour # The CustomBusinessHour is a mixture of BusinessHour and CustomBusinessDay which allows you to specify arbitrary holidays. CustomBusinessHour works as the same as BusinessHour except that it skips specified custom holidays. In [228]: from pandas.tseries.holiday import USFederalHolidayCalendar In [229]: bhour_us = pd . offsets . CustomBusinessHour ( calendar = USFederalHolidayCalendar ()) # Friday before MLK Day In [230]: dt = datetime . datetime ( 2014 , 1 , 17 , 15 ) In [231]: dt + bhour_us Out[231]: Timestamp('2014-01-17 16:00:00') # Tuesday after MLK Day (Monday is skipped because it's a holiday) In [232]: dt + bhour_us * 2 Out[232]: Timestamp('2014-01-21 09:00:00') You can use keyword arguments supported by either BusinessHour and CustomBusinessDay . In [233]: bhour_mon = pd . offsets . CustomBusinessHour ( start = \"10:00\" , weekmask = \"Tue Wed Thu Fri\" ) # Monday is skipped because it's a holiday, business hour starts from 10:00 In [234]: dt + bhour_mon * 2 Out[234]: Timestamp('2014-01-21 10:00:00') Offset aliases # A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as offset aliases . Alias Description B business day frequency C custom business day frequency D calendar day frequency W weekly frequency ME month end frequency SME semi-month end frequency (15th and end of month) BME business month end frequency CBME custom business month end frequency MS month start frequency SMS semi-month start frequency (1st and 15th) BMS business month start frequency CBMS custom business month start frequency QE quarter end frequency BQE business quarter end frequency QS quarter start frequency BQS business quarter start frequency YE year end frequency BYE business year end frequency YS year start frequency BYS business year start frequency h hourly frequency bh business hour frequency cbh custom business hour frequency min minutely frequency s secondly frequency ms milliseconds us microseconds ns nanoseconds Deprecated since version 2.2.0: Aliases H , BH , CBH , T , S , L , U , and N are deprecated in favour of the aliases h , bh , cbh , min , s , ms , us , and ns . Note When using the offset aliases above, it should be noted that functions such as date_range() , bdate_range() , will only return timestamps that are in the interval defined by start_date and end_date . If the start_date does not correspond to the frequency, the returned timestamps will start at the next valid timestamp, same for end_date , the returned timestamps will stop at the previous valid timestamp. For example, for the offset MS , if the start_date is not the first of the month, the returned timestamps will start with the first day of the next month. If end_date is not the first day of a month, the last returned timestamp will be the first day of the corresponding month. In [235]: dates_lst_1 = pd . date_range ( \"2020-01-06\" , \"2020-04-03\" , freq = \"MS\" ) In [236]: dates_lst_1 Out[236]: DatetimeIndex(['2020-02-01', '2020-03-01', '2020-04-01'], dtype='datetime64[ns]', freq='MS') In [237]: dates_lst_2 = pd . date_range ( \"2020-01-01\" , \"2020-04-01\" , freq = \"MS\" ) In [238]: dates_lst_2 Out[238]: DatetimeIndex(['2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01'], dtype='datetime64[ns]', freq='MS') We can see in the above example date_range() and bdate_range() will only return the valid timestamps between the start_date and end_date . If these are not valid timestamps for the given frequency it will roll to the next value for start_date (respectively previous for the end_date ) Period aliases # A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as period aliases . Alias Description B business day frequency D calendar day frequency W weekly frequency M monthly frequency Q quarterly frequency Y yearly frequency h hourly frequency min minutely frequency s secondly frequency ms milliseconds us microseconds ns nanoseconds Deprecated since version 2.2.0: Aliases A , H , T , S , L , U , and N are deprecated in favour of the aliases Y , h , min , s , ms , us , and ns . Combining aliases # As we have seen previously, the alias and the offset instance are fungible in most functions: In [239]: pd . date_range ( start , periods = 5 , freq = \"B\" ) Out[239]: DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07'], dtype='datetime64[ns]', freq='B') In [240]: pd . date_range ( start , periods = 5 , freq = pd . offsets . BDay ()) Out[240]: DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07'], dtype='datetime64[ns]', freq='B') You can combine together day and intraday offsets: In [241]: pd . date_range ( start , periods = 10 , freq = \"2h20min\" ) Out[241]: DatetimeIndex(['2011-01-01 00:00:00', '2011-01-01 02:20:00', '2011-01-01 04:40:00', '2011-01-01 07:00:00', '2011-01-01 09:20:00', '2011-01-01 11:40:00', '2011-01-01 14:00:00', '2011-01-01 16:20:00', '2011-01-01 18:40:00', '2011-01-01 21:00:00'], dtype='datetime64[ns]', freq='140min') In [242]: pd . date_range ( start , periods = 10 , freq = \"1D10us\" ) Out[242]: DatetimeIndex([ '2011-01-01 00:00:00', '2011-01-02 00:00:00.000010', '2011-01-03 00:00:00.000020', '2011-01-04 00:00:00.000030', '2011-01-05 00:00:00.000040', '2011-01-06 00:00:00.000050', '2011-01-07 00:00:00.000060', '2011-01-08 00:00:00.000070', '2011-01-09 00:00:00.000080', '2011-01-10 00:00:00.000090'], dtype='datetime64[ns]', freq='86400000010us') Anchored offsets # For some frequencies you can specify an anchoring suffix: Alias Description W-SUN weekly frequency (Sundays). Same as âWâ W-MON weekly frequency (Mondays) W-TUE weekly frequency (Tuesdays) W-WED weekly frequency (Wednesdays) W-THU weekly frequency (Thursdays) W-FRI weekly frequency (Fridays) W-SAT weekly frequency (Saturdays) (B)Q(E)(S)-DEC quarterly frequency, year ends in December. Same as âQEâ (B)Q(E)(S)-JAN quarterly frequency, year ends in January (B)Q(E)(S)-FEB quarterly frequency, year ends in February (B)Q(E)(S)-MAR quarterly frequency, year ends in March (B)Q(E)(S)-APR quarterly frequency, year ends in April (B)Q(E)(S)-MAY quarterly frequency, year ends in May (B)Q(E)(S)-JUN quarterly frequency, year ends in June (B)Q(E)(S)-JUL quarterly frequency, year ends in July (B)Q(E)(S)-AUG quarterly frequency, year ends in August (B)Q(E)(S)-SEP quarterly frequency, year ends in September (B)Q(E)(S)-OCT quarterly frequency, year ends in October (B)Q(E)(S)-NOV quarterly frequency, year ends in November (B)Y(E)(S)-DEC annual frequency, anchored end of December. Same as âYEâ (B)Y(E)(S)-JAN annual frequency, anchored end of January (B)Y(E)(S)-FEB annual frequency, anchored end of February (B)Y(E)(S)-MAR annual frequency, anchored end of March (B)Y(E)(S)-APR annual frequency, anchored end of April (B)Y(E)(S)-MAY annual frequency, anchored end of May (B)Y(E)(S)-JUN annual frequency, anchored end of June (B)Y(E)(S)-JUL annual frequency, anchored end of July (B)Y(E)(S)-AUG annual frequency, anchored end of August (B)Y(E)(S)-SEP annual frequency, anchored end of September (B)Y(E)(S)-OCT annual frequency, anchored end of October (B)Y(E)(S)-NOV annual frequency, anchored end of November These can be used as arguments to date_range , bdate_range , constructors for DatetimeIndex , as well as various other timeseries-related functions in pandas. Anchored offset semantics # For those offsets that are anchored to the start or end of specific frequency ( MonthEnd , MonthBegin , WeekEnd , etc), the following rules apply to rolling forward and backwards. When n is not 0, if the given date is not on an anchor point, it snapped to the next(previous) anchor point, and moved |n|-1 additional steps forwards or backwards. In [243]: pd . Timestamp ( \"2014-01-02\" ) + pd . offsets . MonthBegin ( n = 1 ) Out[243]: Timestamp('2014-02-01 00:00:00') In [244]: pd . Timestamp ( \"2014-01-02\" ) + pd . offsets . MonthEnd ( n = 1 ) Out[244]: Timestamp('2014-01-31 00:00:00') In [245]: pd . Timestamp ( \"2014-01-02\" ) - pd . offsets . MonthBegin ( n = 1 ) Out[245]: Timestamp('2014-01-01 00:00:00') In [246]: pd . Timestamp ( \"2014-01-02\" ) - pd . offsets . MonthEnd ( n = 1 ) Out[246]: Timestamp('2013-12-31 00:00:00') In [247]: pd . Timestamp ( \"2014-01-02\" ) + pd . offsets . MonthBegin ( n = 4 ) Out[247]: Timestamp('2014-05-01 00:00:00') In [248]: pd . Timestamp ( \"2014-01-02\" ) - pd . offsets . MonthBegin ( n = 4 ) Out[248]: Timestamp('2013-10-01 00:00:00') If the given date is on an anchor point, it is moved |n| points forwards or backwards. In [249]: pd . Timestamp ( \"2014-01-01\" ) + pd . offsets . MonthBegin ( n = 1 ) Out[249]: Timestamp('2014-02-01 00:00:00') In [250]: pd . Timestamp ( \"2014-01-31\" ) + pd . offsets . MonthEnd ( n = 1 ) Out[250]: Timestamp('2014-02-28 00:00:00') In [251]: pd . Timestamp ( \"2014-01-01\" ) - pd . offsets . MonthBegin ( n = 1 ) Out[251]: Timestamp('2013-12-01 00:00:00') In [252]: pd . Timestamp ( \"2014-01-31\" ) - pd . offsets . MonthEnd ( n = 1 ) Out[252]: Timestamp('2013-12-31 00:00:00') In [253]: pd . Timestamp ( \"2014-01-01\" ) + pd . offsets . MonthBegin ( n = 4 ) Out[253]: Timestamp('2014-05-01 00:00:00') In [254]: pd . Timestamp ( \"2014-01-31\" ) - pd . offsets . MonthBegin ( n = 4 ) Out[254]: Timestamp('2013-10-01 00:00:00') For the case when n=0 , the date is not moved if on an anchor point, otherwise it is rolled forward to the next anchor point. In [255]: pd . Timestamp ( \"2014-01-02\" ) + pd . offsets . MonthBegin ( n = 0 ) Out[255]: Timestamp('2014-02-01 00:00:00') In [256]: pd . Timestamp ( \"2014-01-02\" ) + pd . offsets . MonthEnd ( n = 0 ) Out[256]: Timestamp('2014-01-31 00:00:00') In [257]: pd . Timestamp ( \"2014-01-01\" ) + pd . offsets . MonthBegin ( n = 0 ) Out[257]: Timestamp('2014-01-01 00:00:00') In [258]: pd . Timestamp ( \"2014-01-31\" ) + pd . offsets . MonthEnd ( n = 0 ) Out[258]: Timestamp('2014-01-31 00:00:00') Holidays / holiday calendars # Holidays and calendars provide a simple way to define holiday rules to be used with CustomBusinessDay or in other analysis that requires a predefined set of holidays. The AbstractHolidayCalendar class provides all the necessary methods to return a list of holidays and only rules need to be defined in a specific holiday calendar class. Furthermore, the start_date and end_date class attributes determine over what date range holidays are generated. These should be overwritten on the AbstractHolidayCalendar class to have the range apply to all calendar subclasses. USFederalHolidayCalendar is the only calendar that exists and primarily serves as an example for developing other calendars. For holidays that occur on fixed dates (e.g., US Memorial Day or July 4th) an observance rule determines when that holiday is observed if it falls on a weekend or some other non-observed day. Defined observance rules are: Rule Description nearest_workday move Saturday to Friday and Sunday to Monday sunday_to_monday move Sunday to following Monday next_monday_or_tuesday move Saturday to Monday and Sunday/Monday to Tuesday previous_friday move Saturday and Sunday to previous Fridayâ next_monday move Saturday and Sunday to following Monday An example of how holidays and holiday calendars are defined: In [259]: from pandas.tseries.holiday import ( .....: Holiday , .....: USMemorialDay , .....: AbstractHolidayCalendar , .....: nearest_workday , .....: MO , .....: ) .....: In [260]: class ExampleCalendar ( AbstractHolidayCalendar ): .....: rules = [ .....: USMemorialDay , .....: Holiday ( \"July 4th\" , month = 7 , day = 4 , observance = nearest_workday ), .....: Holiday ( .....: \"Columbus Day\" , .....: month = 10 , .....: day = 1 , .....: offset = pd . DateOffset ( weekday = MO ( 2 )), .....: ), .....: ] .....: In [261]: cal = ExampleCalendar () In [262]: cal . holidays ( datetime . datetime ( 2012 , 1 , 1 ), datetime . datetime ( 2012 , 12 , 31 )) Out[262]: DatetimeIndex(['2012-05-28', '2012-07-04', '2012-10-08'], dtype='datetime64[ns]', freq=None) hint : weekday=MO(2) is same as 2 * Week(weekday=2) Using this calendar, creating an index or doing offset arithmetic skips weekends and holidays (i.e., Memorial Day/July 4th). For example, the below defines a custom business day offset using the ExampleCalendar . Like any other offset, it can be used to create a DatetimeIndex or added to datetime or Timestamp objects. In [263]: pd . date_range ( .....: start = \"7/1/2012\" , end = \"7/10/2012\" , freq = pd . offsets . CDay ( calendar = cal ) .....: ) . to_pydatetime () .....: Out[263]: array([datetime.datetime(2012, 7, 2, 0, 0), datetime.datetime(2012, 7, 3, 0, 0), datetime.datetime(2012, 7, 5, 0, 0), datetime.datetime(2012, 7, 6, 0, 0), datetime.datetime(2012, 7, 9, 0, 0), datetime.datetime(2012, 7, 10, 0, 0)], dtype=object) In [264]: offset = pd . offsets . CustomBusinessDay ( calendar = cal ) In [265]: datetime . datetime ( 2012 , 5 , 25 ) + offset Out[265]: Timestamp('2012-05-29 00:00:00') In [266]: datetime . datetime ( 2012 , 7 , 3 ) + offset Out[266]: Timestamp('2012-07-05 00:00:00') In [267]: datetime . datetime ( 2012 , 7 , 3 ) + 2 * offset Out[267]: Timestamp('2012-07-06 00:00:00') In [268]: datetime . datetime ( 2012 , 7 , 6 ) + offset Out[268]: Timestamp('2012-07-09 00:00:00') Ranges are defined by the start_date and end_date class attributes of AbstractHolidayCalendar . The defaults are shown below. In [269]: AbstractHolidayCalendar . start_date Out[269]: Timestamp('1970-01-01 00:00:00') In [270]: AbstractHolidayCalendar . end_date Out[270]: Timestamp('2200-12-31 00:00:00') These dates can be overwritten by setting the attributes as datetime/Timestamp/string. In [271]: AbstractHolidayCalendar . start_date = datetime . datetime ( 2012 , 1 , 1 ) In [272]: AbstractHolidayCalendar . end_date = datetime . datetime ( 2012 , 12 , 31 ) In [273]: cal . holidays () Out[273]: DatetimeIndex(['2012-05-28', '2012-07-04', '2012-10-08'], dtype='datetime64[ns]', freq=None) Every calendar class is accessible by name using the get_calendar function which returns a holiday class instance. Any imported calendar class will automatically be available by this function. Also, HolidayCalendarFactory provides an easy interface to create calendars that are combinations of calendars or calendars with additional rules. In [274]: from pandas.tseries.holiday import get_calendar , HolidayCalendarFactory , USLaborDay In [275]: cal = get_calendar ( \"ExampleCalendar\" ) In [276]: cal . rules Out[276]: [Holiday: Memorial Day (month=5, day=31, offset=<DateOffset: weekday=MO(-1)>), Holiday: July 4th (month=7, day=4, observance=<function nearest_workday at 0x7f83eb680d30>), Holiday: Columbus Day (month=10, day=1, offset=<DateOffset: weekday=MO(+2)>)] In [277]: new_cal = HolidayCalendarFactory ( \"NewExampleCalendar\" , cal , USLaborDay ) In [278]: new_cal . rules Out[278]: [Holiday: Labor Day (month=9, day=1, offset=<DateOffset: weekday=MO(+1)>), Holiday: Memorial Day (month=5, day=31, offset=<DateOffset: weekday=MO(-1)>), Holiday: July 4th (month=7, day=4, observance=<function nearest_workday at 0x7f83eb680d30>), Holiday: Columbus Day (month=10, day=1, offset=<DateOffset: weekday=MO(+2)>)] Time Series-related instance methods # Shifting / lagging # One may want to shift or lag the values in a time series back and forward in time. The method for this is shift() , which is available on all of the pandas objects. In [279]: ts = pd . Series ( range ( len ( rng )), index = rng ) In [280]: ts = ts [: 5 ] In [281]: ts . shift ( 1 ) Out[281]: 2012-01-01 NaN 2012-01-02 0.0 2012-01-03 1.0 Freq: D, dtype: float64 The shift method accepts an freq argument which can accept a DateOffset class or other timedelta -like object or also an offset alias . When freq is specified, shift method changes all the dates in the index rather than changing the alignment of the data and the index: In [282]: ts . shift ( 5 , freq = \"D\" ) Out[282]: 2012-01-06 0 2012-01-07 1 2012-01-08 2 Freq: D, dtype: int64 In [283]: ts . shift ( 5 , freq = pd . offsets . BDay ()) Out[283]: 2012-01-06 0 2012-01-09 1 2012-01-10 2 dtype: int64 In [284]: ts . shift ( 5 , freq = \"BME\" ) Out[284]: 2012-05-31 0 2012-05-31 1 2012-05-31 2 dtype: int64 Note that with when freq is specified, the leading entry is no longer NaN because the data is not being realigned. Frequency conversion # The primary function for changing frequencies is the asfreq() method. For a DatetimeIndex , this is basically just a thin, but convenient wrapper around reindex() which generates a date_range and calls reindex . In [285]: dr = pd . date_range ( \"1/1/2010\" , periods = 3 , freq = 3 * pd . offsets . BDay ()) In [286]: ts = pd . Series ( np . random . randn ( 3 ), index = dr ) In [287]: ts Out[287]: 2010-01-01 1.494522 2010-01-06 -0.778425 2010-01-11 -0.253355 Freq: 3B, dtype: float64 In [288]: ts . asfreq ( pd . offsets . BDay ()) Out[288]: 2010-01-01 1.494522 2010-01-04 NaN 2010-01-05 NaN 2010-01-06 -0.778425 2010-01-07 NaN 2010-01-08 NaN 2010-01-11 -0.253355 Freq: B, dtype: float64 asfreq provides a further convenience so you can specify an interpolation method for any gaps that may appear after the frequency conversion. In [289]: ts . asfreq ( pd . offsets . BDay (), method = \"pad\" ) Out[289]: 2010-01-01 1.494522 2010-01-04 1.494522 2010-01-05 1.494522 2010-01-06 -0.778425 2010-01-07 -0.778425 2010-01-08 -0.778425 2010-01-11 -0.253355 Freq: B, dtype: float64 Filling forward / backward # Related to asfreq and reindex is fillna() , which is documented in the missing data section . Converting to Python datetimes # DatetimeIndex can be converted to an array of Python native datetime.datetime objects using the to_pydatetime method. Resampling # pandas has a simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. resample() is a time-based groupby, followed by a reduction method on each of its groups. See some cookbook examples for some advanced strategies. The resample() method can be used directly from DataFrameGroupBy objects, see the groupby docs . Basics # In [290]: rng = pd . date_range ( \"1/1/2012\" , periods = 100 , freq = \"s\" ) In [291]: ts = pd . Series ( np . random . randint ( 0 , 500 , len ( rng )), index = rng ) In [292]: ts . resample ( \"5Min\" ) . sum () Out[292]: 2012-01-01 25103 Freq: 5min, dtype: int64 The resample function is very flexible and allows you to specify many different parameters to control the frequency conversion and resampling operation. Any built-in method available via GroupBy is available as a method of the returned object, including sum , mean , std , sem , max , min , median , first , last , ohlc : In [293]: ts . resample ( \"5Min\" ) . mean () Out[293]: 2012-01-01 251.03 Freq: 5min, dtype: float64 In [294]: ts . resample ( \"5Min\" ) . ohlc () Out[294]: open high low close 2012-01-01 308 460 9 205 In [295]: ts . resample ( \"5Min\" ) . max () Out[295]: 2012-01-01 460 Freq: 5min, dtype: int64 For downsampling, closed can be set to âleftâ or ârightâ to specify which end of the interval is closed: In [296]: ts . resample ( \"5Min\" , closed = \"right\" ) . mean () Out[296]: 2011-12-31 23:55:00 308.000000 2012-01-01 00:00:00 250.454545 Freq: 5min, dtype: float64 In [297]: ts . resample ( \"5Min\" , closed = \"left\" ) . mean () Out[297]: 2012-01-01 251.03 Freq: 5min, dtype: float64 Parameters like label are used to manipulate the resulting labels. label specifies whether the result is labeled with the beginning or the end of the interval. In [298]: ts . resample ( \"5Min\" ) . mean () # by default label='left' Out[298]: 2012-01-01 251.03 Freq: 5min, dtype: float64 In [299]: ts . resample ( \"5Min\" , label = \"left\" ) . mean () Out[299]: 2012-01-01 251.03 Freq: 5min, dtype: float64 Warning The default values for label and closed is â left â for all frequency offsets except for âMEâ, âYEâ, âQEâ, âBMEâ, âBYEâ, âBQEâ, and âWâ which all have a default of ârightâ. This might unintendedly lead to looking ahead, where the value for a later time is pulled back to a previous time as in the following example with the BusinessDay frequency: In [300]: s = pd . date_range ( \"2000-01-01\" , \"2000-01-05\" ) . to_series () In [301]: s . iloc [ 2 ] = pd . NaT In [302]: s . dt . day_name () Out[302]: 2000-01-01 Saturday 2000-01-02 Sunday 2000-01-03 NaN 2000-01-04 Tuesday 2000-01-05 Wednesday Freq: D, dtype: object # default: label='left', closed='left' In [303]: s . resample ( \"B\" ) . last () . dt . day_name () Out[303]: 1999-12-31 Sunday 2000-01-03 NaN 2000-01-04 Tuesday 2000-01-05 Wednesday Freq: B, dtype: object Notice how the value for Sunday got pulled back to the previous Friday. To get the behavior where the value for Sunday is pushed to Monday, use instead In [304]: s . resample ( \"B\" , label = \"right\" , closed = \"right\" ) . last () . dt . day_name () Out[304]: 2000-01-03 Sunday 2000-01-04 Tuesday 2000-01-05 Wednesday 2000-01-06 NaN Freq: B, dtype: object The axis parameter can be set to 0 or 1 and allows you to resample the specified axis for a DataFrame . kind can be set to âtimestampâ or âperiodâ to convert the resulting index to/from timestamp and time span representations. By default resample retains the input representation. convention can be set to âstartâ or âendâ when resampling period data (detail below). It specifies how low frequency periods are converted to higher frequency periods. Upsampling # For upsampling, you can specify a way to upsample and the limit parameter to interpolate over the gaps that are created: # from secondly to every 250 milliseconds In [305]: ts [: 2 ] . resample ( \"250ms\" ) . asfreq () Out[305]: 2012-01-01 00:00:00.000 308.0 2012-01-01 00:00:00.250 NaN 2012-01-01 00:00:00.500 NaN 2012-01-01 00:00:00.750 NaN 2012-01-01 00:00:01.000 204.0 Freq: 250ms, dtype: float64 In [306]: ts [: 2 ] . resample ( \"250ms\" ) . ffill () Out[306]: 2012-01-01 00:00:00.000 308 2012-01-01 00:00:00.250 308 2012-01-01 00:00:00.500 308 2012-01-01 00:00:00.750 308 2012-01-01 00:00:01.000 204 Freq: 250ms, dtype: int64 In [307]: ts [: 2 ] . resample ( \"250ms\" ) . ffill ( limit = 2 ) Out[307]: 2012-01-01 00:00:00.000 308.0 2012-01-01 00:00:00.250 308.0 2012-01-01 00:00:00.500 308.0 2012-01-01 00:00:00.750 NaN 2012-01-01 00:00:01.000 204.0 Freq: 250ms, dtype: float64 Sparse resampling # Sparse timeseries are the ones where you have a lot fewer points relative to the amount of time you are looking to resample. Naively upsampling a sparse series can potentially generate lots of intermediate values. When you donât want to use a method to fill these values, e.g. fill_method is None , then intermediate values will be filled with NaN . Since resample is a time-based groupby, the following is a method to efficiently resample only the groups that are not all NaN . In [308]: rng = pd . date_range ( \"2014-1-1\" , periods = 100 , freq = \"D\" ) + pd . Timedelta ( \"1s\" ) In [309]: ts = pd . Series ( range ( 100 ), index = rng ) If we want to resample to the full range of the series: In [310]: ts . resample ( \"3min\" ) . sum () Out[310]: 2014-01-01 00:00:00 0 2014-01-01 00:03:00 0 2014-01-01 00:06:00 0 2014-01-01 00:09:00 0 2014-01-01 00:12:00 0 .. 2014-04-09 23:48:00 0 2014-04-09 23:51:00 0 2014-04-09 23:54:00 0 2014-04-09 23:57:00 0 2014-04-10 00:00:00 99 Freq: 3min, Length: 47521, dtype: int64 We can instead only resample those groups where we have points as follows: In [311]: from functools import partial In [312]: from pandas.tseries.frequencies import to_offset In [313]: def round ( t , freq ): .....: freq = to_offset ( freq ) .....: td = pd . Timedelta ( freq ) .....: return pd . Timestamp (( t . value // td . value ) * td . value ) .....: In [314]: ts . groupby ( partial ( round , freq = \"3min\" )) . sum () Out[314]: 2014-01-01 0 2014-01-02 1 2014-01-03 2 2014-01-04 3 2014-01-05 4 .. 2014-04-06 95 2014-04-07 96 2014-04-08 97 2014-04-09 98 2014-04-10 99 Length: 100, dtype: int64 Aggregation # The resample() method returns a pandas.api.typing.Resampler instance. Similar to the aggregating API , groupby API , and the window API , a Resampler can be selectively resampled. Resampling a DataFrame , the default will be to act on all columns with the same function. In [315]: df = pd . DataFrame ( .....: np . random . randn ( 1000 , 3 ), .....: index = pd . date_range ( \"1/1/2012\" , freq = \"s\" , periods = 1000 ), .....: columns = [ \"A\" , \"B\" , \"C\" ], .....: ) .....: In [316]: r = df . resample ( \"3min\" ) In [317]: r . mean () Out[317]: A B C 2012-01-01 00:00:00 -0.033823 -0.121514 -0.081447 2012-01-01 00:03:00 0.056909 0.146731 -0.024320 2012-01-01 00:06:00 -0.058837 0.047046 -0.052021 2012-01-01 00:09:00 0.063123 -0.026158 -0.066533 2012-01-01 00:12:00 0.186340 -0.003144 0.074752 2012-01-01 00:15:00 -0.085954 -0.016287 -0.050046 We can select a specific column or columns using standard getitem. In [318]: r [ \"A\" ] . mean () Out[318]: 2012-01-01 00:00:00 -0.033823 2012-01-01 00:03:00 0.056909 2012-01-01 00:06:00 -0.058837 2012-01-01 00:09:00 0.063123 2012-01-01 00:12:00 0.186340 2012-01-01 00:15:00 -0.085954 Freq: 3min, Name: A, dtype: float64 In [319]: r [[ \"A\" , \"B\" ]] . mean () Out[319]: A B 2012-01-01 00:00:00 -0.033823 -0.121514 2012-01-01 00:03:00 0.056909 0.146731 2012-01-01 00:06:00 -0.058837 0.047046 2012-01-01 00:09:00 0.063123 -0.026158 2012-01-01 00:12:00 0.186340 -0.003144 2012-01-01 00:15:00 -0.085954 -0.016287 You can pass a list or dict of functions to do aggregation with, outputting a DataFrame : In [320]: r [ \"A\" ] . agg ([ \"sum\" , \"mean\" , \"std\" ]) Out[320]: sum mean std 2012-01-01 00:00:00 -6.088060 -0.033823 1.043263 2012-01-01 00:03:00 10.243678 0.056909 1.058534 2012-01-01 00:06:00 -10.590584 -0.058837 0.949264 2012-01-01 00:09:00 11.362228 0.063123 1.028096 2012-01-01 00:12:00 33.541257 0.186340 0.884586 2012-01-01 00:15:00 -8.595393 -0.085954 1.035476 On a resampled DataFrame , you can pass a list of functions to apply to each column, which produces an aggregated result with a hierarchical index: In [321]: r . agg ([ \"sum\" , \"mean\" ]) Out[321]: A ... C sum mean ... sum mean 2012-01-01 00:00:00 -6.088060 -0.033823 ... -14.660515 -0.081447 2012-01-01 00:03:00 10.243678 0.056909 ... -4.377642 -0.024320 2012-01-01 00:06:00 -10.590584 -0.058837 ... -9.363825 -0.052021 2012-01-01 00:09:00 11.362228 0.063123 ... -11.975895 -0.066533 2012-01-01 00:12:00 33.541257 0.186340 ... 13.455299 0.074752 2012-01-01 00:15:00 -8.595393 -0.085954 ... -5.004580 -0.050046 [6 rows x 6 columns] By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame : In [322]: r . agg ({ \"A\" : \"sum\" , \"B\" : lambda x : np . std ( x , ddof = 1 )}) Out[322]: A B 2012-01-01 00:00:00 -6.088060 1.001294 2012-01-01 00:03:00 10.243678 1.074597 2012-01-01 00:06:00 -10.590584 0.987309 2012-01-01 00:09:00 11.362228 0.944953 2012-01-01 00:12:00 33.541257 1.095025 2012-01-01 00:15:00 -8.595393 1.035312 The function names can also be strings. In order for a string to be valid it must be implemented on the resampled object: In [323]: r . agg ({ \"A\" : \"sum\" , \"B\" : \"std\" }) Out[323]: A B 2012-01-01 00:00:00 -6.088060 1.001294 2012-01-01 00:03:00 10.243678 1.074597 2012-01-01 00:06:00 -10.590584 0.987309 2012-01-01 00:09:00 11.362228 0.944953 2012-01-01 00:12:00 33.541257 1.095025 2012-01-01 00:15:00 -8.595393 1.035312 Furthermore, you can also specify multiple aggregation functions for each column separately. In [324]: r . agg ({ \"A\" : [ \"sum\" , \"std\" ], \"B\" : [ \"mean\" , \"std\" ]}) Out[324]: A B sum std mean std 2012-01-01 00:00:00 -6.088060 1.043263 -0.121514 1.001294 2012-01-01 00:03:00 10.243678 1.058534 0.146731 1.074597 2012-01-01 00:06:00 -10.590584 0.949264 0.047046 0.987309 2012-01-01 00:09:00 11.362228 1.028096 -0.026158 0.944953 2012-01-01 00:12:00 33.541257 0.884586 -0.003144 1.095025 2012-01-01 00:15:00 -8.595393 1.035476 -0.016287 1.035312 If a DataFrame does not have a datetimelike index, but instead you want to resample based on datetimelike column in the frame, it can passed to the on keyword. In [325]: df = pd . DataFrame ( .....: { \"date\" : pd . date_range ( \"2015-01-01\" , freq = \"W\" , periods = 5 ), \"a\" : np . arange ( 5 )}, .....: index = pd . MultiIndex . from_arrays ( .....: [[ 1 , 2 , 3 , 4 , 5 ], pd . date_range ( \"2015-01-01\" , freq = \"W\" , periods = 5 )], .....: names = [ \"v\" , \"d\" ], .....: ), .....: ) .....: In [326]: df Out[326]: date a v d 1 2015-01-04 2015-01-04 0 2 2015-01-11 2015-01-11 1 3 2015-01-18 2015-01-18 2 4 2015-01-25 2015-01-25 3 5 2015-02-01 2015-02-01 4 In [327]: df . resample ( \"ME\" , on = \"date\" )[[ \"a\" ]] . sum () Out[327]: a date 2015-01-31 6 2015-02-28 4 Similarly, if you instead want to resample by a datetimelike level of MultiIndex , its name or location can be passed to the level keyword. In [328]: df . resample ( \"ME\" , level = \"d\" )[[ \"a\" ]] . sum () Out[328]: a d 2015-01-31 6 2015-02-28 4 Iterating through groups # With the Resampler object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby() : In [329]: small = pd . Series ( .....: range ( 6 ), .....: index = pd . to_datetime ( .....: [ .....: \"2017-01-01T00:00:00\" , .....: \"2017-01-01T00:30:00\" , .....: \"2017-01-01T00:31:00\" , .....: \"2017-01-01T01:00:00\" , .....: \"2017-01-01T03:00:00\" , .....: \"2017-01-01T03:05:00\" , .....: ] .....: ), .....: ) .....: In [330]: resampled = small . resample ( \"h\" ) In [331]: for name , group in resampled : .....: print ( \"Group: \" , name ) .....: print ( \"-\" * 27 ) .....: print ( group , end = \" \\n\\n \" ) .....: Group: 2017-01-01 00:00:00 --------------------------- 2017 - 01 - 01 00 : 00 : 00 0 2017 - 01 - 01 00 : 30 : 00 1 2017 - 01 - 01 00 : 31 : 00 2 dtype : int64 Group : 2017-01-01 01:00:00 --------------------------- 2017 - 01 - 01 01 : 00 : 00 3 dtype : int64 Group : 2017-01-01 02:00:00 --------------------------- Series ([], dtype : int64 ) Group : 2017-01-01 03:00:00 --------------------------- 2017 - 01 - 01 03 : 00 : 00 4 2017 - 01 - 01 03 : 05 : 00 5 dtype : int64 See Iterating through groups or Resampler.__iter__ for more. Use origin or offset to adjust the start of the bins # The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like 30D ) or that divide a day evenly (like 90s or 1min ). This can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can specify a fixed Timestamp with the argument origin . For example: In [332]: start , end = \"2000-10-01 23:30:00\" , \"2000-10-02 00:30:00\" In [333]: middle = \"2000-10-02 00:00:00\" In [334]: rng = pd . date_range ( start , end , freq = \"7min\" ) In [335]: ts = pd . Series ( np . arange ( len ( rng )) * 3 , index = rng ) In [336]: ts Out[336]: 2000-10-01 23:30:00 0 2000-10-01 23:37:00 3 2000-10-01 23:44:00 6 2000-10-01 23:51:00 9 2000-10-01 23:58:00 12 2000-10-02 00:05:00 15 2000-10-02 00:12:00 18 2000-10-02 00:19:00 21 2000-10-02 00:26:00 24 Freq: 7min, dtype: int64 Here we can see that, when using origin with its default value ( 'start_day' ), the result after '2000-10-02 00:00:00' are not identical depending on the start of time series: In [337]: ts . resample ( \"17min\" , origin = \"start_day\" ) . sum () Out[337]: 2000-10-01 23:14:00 0 2000-10-01 23:31:00 9 2000-10-01 23:48:00 21 2000-10-02 00:05:00 54 2000-10-02 00:22:00 24 Freq: 17min, dtype: int64 In [338]: ts [ middle : end ] . resample ( \"17min\" , origin = \"start_day\" ) . sum () Out[338]: 2000-10-02 00:00:00 33 2000-10-02 00:17:00 45 Freq: 17min, dtype: int64 Here we can see that, when setting origin to 'epoch' , the result after '2000-10-02 00:00:00' are identical depending on the start of time series: In [339]: ts . resample ( \"17min\" , origin = \"epoch\" ) . sum () Out[339]: 2000-10-01 23:18:00 0 2000-10-01 23:35:00 18 2000-10-01 23:52:00 27 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17min, dtype: int64 In [340]: ts [ middle : end ] . resample ( \"17min\" , origin = \"epoch\" ) . sum () Out[340]: 2000-10-01 23:52:00 15 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17min, dtype: int64 If needed you can use a custom timestamp for origin : In [341]: ts . resample ( \"17min\" , origin = \"2001-01-01\" ) . sum () Out[341]: 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 In [342]: ts [ middle : end ] . resample ( \"17min\" , origin = pd . Timestamp ( \"2001-01-01\" )) . sum () Out[342]: 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 If needed you can just adjust the bins with an offset Timedelta that would be added to the default origin . Those two examples are equivalent for this time series: In [343]: ts . resample ( \"17min\" , origin = \"start\" ) . sum () Out[343]: 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 In [344]: ts . resample ( \"17min\" , offset = \"23h30min\" ) . sum () Out[344]: 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17min, dtype: int64 Note the use of 'start' for origin on the last example. In that case, origin will be set to the first value of the timeseries. Backward resample # Added in version 1.3.0. Instead of adjusting the beginning of bins, sometimes we need to fix the end of the bins to make a backward resample with a given freq . The backward resample sets closed to 'right' by default since the last value should be considered as the edge point for the last bin. We can set origin to 'end' . The value for a specific Timestamp index stands for the resample result from the current Timestamp minus freq to the current Timestamp with a right close. In [345]: ts . resample ( '17min' , origin = 'end' ) . sum () Out[345]: 2000-10-01 23:35:00 0 2000-10-01 23:52:00 18 2000-10-02 00:09:00 27 2000-10-02 00:26:00 63 Freq: 17min, dtype: int64 Besides, in contrast with the 'start_day' option, end_day is supported. This will set the origin as the ceiling midnight of the largest Timestamp . In [346]: ts . resample ( '17min' , origin = 'end_day' ) . sum () Out[346]: 2000-10-01 23:38:00 3 2000-10-01 23:55:00 15 2000-10-02 00:12:00 45 2000-10-02 00:29:00 45 Freq: 17min, dtype: int64 The above result uses 2000-10-02 00:29:00 as the last binâs right edge since the following computation. In [347]: ceil_mid = rng . max () . ceil ( 'D' ) In [348]: freq = pd . offsets . Minute ( 17 ) In [349]: bin_res = ceil_mid - freq * (( ceil_mid - rng . max ()) // freq ) In [350]: bin_res Out[350]: Timestamp('2000-10-02 00:29:00') Time span representation # Regular intervals of time are represented by Period objects in pandas while sequences of Period objects are collected in a PeriodIndex , which can be created with the convenience function period_range . Period # A Period represents a span of time (e.g., a day, a month, a quarter, etc). You can specify the span via freq keyword using a frequency alias like below. Because freq represents a span of Period , it cannot be negative like â-3Dâ. In [351]: pd . Period ( \"2012\" , freq = \"Y-DEC\" ) Out[351]: Period('2012', 'Y-DEC') In [352]: pd . Period ( \"2012-1-1\" , freq = \"D\" ) Out[352]: Period('2012-01-01', 'D') In [353]: pd . Period ( \"2012-1-1 19:00\" , freq = \"h\" ) Out[353]: Period('2012-01-01 19:00', 'h') In [354]: pd . Period ( \"2012-1-1 19:00\" , freq = \"5h\" ) Out[354]: Period('2012-01-01 19:00', '5h') Adding and subtracting integers from periods shifts the period by its own frequency. Arithmetic is not allowed between Period with different freq (span). In [355]: p = pd . Period ( \"2012\" , freq = \"Y-DEC\" ) In [356]: p + 1 Out[356]: Period('2013', 'Y-DEC') In [357]: p - 3 Out[357]: Period('2009', 'Y-DEC') In [358]: p = pd . Period ( \"2012-01\" , freq = \"2M\" ) In [359]: p + 2 Out[359]: Period('2012-05', '2M') In [360]: p - 1 Out[360]: Period('2011-11', '2M') In [361]: p == pd . Period ( \"2012-01\" , freq = \"3M\" ) Out[361]: False If Period freq is daily or higher ( D , h , min , s , ms , us , and ns ), offsets and timedelta -like can be added if the result can have the same freq. Otherwise, ValueError will be raised. In [362]: p = pd . Period ( \"2014-07-01 09:00\" , freq = \"h\" ) In [363]: p + pd . offsets . Hour ( 2 ) Out[363]: Period('2014-07-01 11:00', 'h') In [364]: p + datetime . timedelta ( minutes = 120 ) Out[364]: Period('2014-07-01 11:00', 'h') In [365]: p + np . timedelta64 ( 7200 , \"s\" ) Out[365]: Period('2014-07-01 11:00', 'h') In [366]: p + pd . offsets . Minute ( 5 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1824, in pandas._libs.tslibs.period._Period._add_timedeltalike_scalar () File ~/work/pandas/pandas/pandas/_libs/tslibs/timedeltas.pyx:278, in pandas._libs.tslibs.timedeltas.delta_to_nanoseconds () File ~/work/pandas/pandas/pandas/_libs/tslibs/np_datetime.pyx:661, in pandas._libs.tslibs.np_datetime.convert_reso () ValueError : Cannot losslessly convert units The above exception was the direct cause of the following exception : IncompatibleFrequency Traceback (most recent call last) Cell In [ 366 ], line 1 ----> 1 p + pd . offsets . Minute ( 5 ) File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1845, in pandas._libs.tslibs.period._Period.__add__ () File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1826, in pandas._libs.tslibs.period._Period._add_timedeltalike_scalar () IncompatibleFrequency : Input cannot be converted to Period(freq=h) If Period has other frequencies, only the same offsets can be added. Otherwise, ValueError will be raised. In [367]: p = pd . Period ( \"2014-07\" , freq = \"M\" ) In [368]: p + pd . offsets . MonthEnd ( 3 ) Out[368]: Period('2014-10', 'M') In [369]: p + pd . offsets . MonthBegin ( 3 ) --------------------------------------------------------------------------- IncompatibleFrequency Traceback (most recent call last) Cell In [ 369 ], line 1 ----> 1 p + pd . offsets . MonthBegin ( 3 ) File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1847, in pandas._libs.tslibs.period._Period.__add__ () File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1837, in pandas._libs.tslibs.period._Period._add_offset () File ~/work/pandas/pandas/pandas/_libs/tslibs/period.pyx:1732, in pandas._libs.tslibs.period.PeriodMixin._require_matching_freq () IncompatibleFrequency : Input has different freq=3M from Period(freq=M) Taking the difference of Period instances with the same frequency will return the number of frequency units between them: In [370]: pd . Period ( \"2012\" , freq = \"Y-DEC\" ) - pd . Period ( \"2002\" , freq = \"Y-DEC\" ) Out[370]: <10 * YearEnds: month=12> PeriodIndex and period_range # Regular sequences of Period objects can be collected in a PeriodIndex , which can be constructed using the period_range convenience function: In [371]: prng = pd . period_range ( \"1/1/2011\" , \"1/1/2012\" , freq = \"M\" ) In [372]: prng Out[372]: PeriodIndex(['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07', '2011-08', '2011-09', '2011-10', '2011-11', '2011-12', '2012-01'], dtype='period[M]') The PeriodIndex constructor can also be used directly: In [373]: pd . PeriodIndex ([ \"2011-1\" , \"2011-2\" , \"2011-3\" ], freq = \"M\" ) Out[373]: PeriodIndex(['2011-01', '2011-02', '2011-03'], dtype='period[M]') Passing multiplied frequency outputs a sequence of Period which has multiplied span. In [374]: pd . period_range ( start = \"2014-01\" , freq = \"3M\" , periods = 4 ) Out[374]: PeriodIndex(['2014-01', '2014-04', '2014-07', '2014-10'], dtype='period[3M]') If start or end are Period objects, they will be used as anchor endpoints for a PeriodIndex with frequency matching that of the PeriodIndex constructor. In [375]: pd . period_range ( .....: start = pd . Period ( \"2017Q1\" , freq = \"Q\" ), end = pd . Period ( \"2017Q2\" , freq = \"Q\" ), freq = \"M\" .....: ) .....: Out[375]: PeriodIndex(['2017-03', '2017-04', '2017-05', '2017-06'], dtype='period[M]') Just like DatetimeIndex , a PeriodIndex can also be used to index pandas objects: In [376]: ps = pd . Series ( np . random . randn ( len ( prng )), prng ) In [377]: ps Out[377]: 2011-01 -2.916901 2011-02 0.514474 2011-03 1.346470 2011-04 0.816397 2011-05 2.258648 2011-06 0.494789 2011-07 0.301239 2011-08 0.464776 2011-09 -1.393581 2011-10 0.056780 2011-11 0.197035 2011-12 2.261385 2012-01 -0.329583 Freq: M, dtype: float64 PeriodIndex supports addition and subtraction with the same rule as Period . In [378]: idx = pd . period_range ( \"2014-07-01 09:00\" , periods = 5 , freq = \"h\" ) In [379]: idx Out[379]: PeriodIndex(['2014-07-01 09:00', '2014-07-01 10:00', '2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00'], dtype='period[h]') In [380]: idx + pd . offsets . Hour ( 2 ) Out[380]: PeriodIndex(['2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00', '2014-07-01 14:00', '2014-07-01 15:00'], dtype='period[h]') In [381]: idx = pd . period_range ( \"2014-07\" , periods = 5 , freq = \"M\" ) In [382]: idx Out[382]: PeriodIndex(['2014-07', '2014-08', '2014-09', '2014-10', '2014-11'], dtype='period[M]') In [383]: idx + pd . offsets . MonthEnd ( 3 ) Out[383]: PeriodIndex(['2014-10', '2014-11', '2014-12', '2015-01', '2015-02'], dtype='period[M]') PeriodIndex has its own dtype named period , refer to Period Dtypes . Period dtypes # PeriodIndex has a custom period dtype. This is a pandas extension dtype similar to the timezone aware dtype ( datetime64[ns, tz] ). The period dtype holds the freq attribute and is represented with period[freq] like period[D] or period[M] , using frequency strings . In [384]: pi = pd . period_range ( \"2016-01-01\" , periods = 3 , freq = \"M\" ) In [385]: pi Out[385]: PeriodIndex(['2016-01', '2016-02', '2016-03'], dtype='period[M]') In [386]: pi . dtype Out[386]: period[M] The period dtype can be used in .astype(...) . It allows one to change the freq of a PeriodIndex like .asfreq() and convert a DatetimeIndex to PeriodIndex like to_period() : # change monthly freq to daily freq In [387]: pi . astype ( \"period[D]\" ) Out[387]: PeriodIndex(['2016-01-31', '2016-02-29', '2016-03-31'], dtype='period[D]') # convert to DatetimeIndex In [388]: pi . astype ( \"datetime64[ns]\" ) Out[388]: DatetimeIndex(['2016-01-01', '2016-02-01', '2016-03-01'], dtype='datetime64[ns]', freq='MS') # convert to PeriodIndex In [389]: dti = pd . date_range ( \"2011-01-01\" , freq = \"ME\" , periods = 3 ) In [390]: dti Out[390]: DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31'], dtype='datetime64[ns]', freq='ME') In [391]: dti . astype ( \"period[M]\" ) Out[391]: PeriodIndex(['2011-01', '2011-02', '2011-03'], dtype='period[M]') PeriodIndex partial string indexing # PeriodIndex now supports partial string slicing with non-monotonic indexes. You can pass in dates and strings to Series and DataFrame with PeriodIndex , in the same manner as DatetimeIndex . For details, refer to DatetimeIndex Partial String Indexing . In [392]: ps [ \"2011-01\" ] Out[392]: -2.9169013294054507 In [393]: ps [ datetime . datetime ( 2011 , 12 , 25 ):] Out[393]: 2011-12 2.261385 2012-01 -0.329583 Freq: M, dtype: float64 In [394]: ps [ \"10/31/2011\" : \"12/31/2011\" ] Out[394]: 2011-10 0.056780 2011-11 0.197035 2011-12 2.261385 Freq: M, dtype: float64 Passing a string representing a lower frequency than PeriodIndex returns partial sliced data. In [395]: ps [ \"2011\" ] Out[395]: 2011-01 -2.916901 2011-02 0.514474 2011-03 1.346470 2011-04 0.816397 2011-05 2.258648 2011-06 0.494789 2011-07 0.301239 2011-08 0.464776 2011-09 -1.393581 2011-10 0.056780 2011-11 0.197035 2011-12 2.261385 Freq: M, dtype: float64 In [396]: dfp = pd . DataFrame ( .....: np . random . randn ( 600 , 1 ), .....: columns = [ \"A\" ], .....: index = pd . period_range ( \"2013-01-01 9:00\" , periods = 600 , freq = \"min\" ), .....: ) .....: In [397]: dfp Out[397]: A 2013-01-01 09:00 -0.538468 2013-01-01 09:01 -1.365819 2013-01-01 09:02 -0.969051 2013-01-01 09:03 -0.331152 2013-01-01 09:04 -0.245334 ... ... 2013-01-01 18:55 0.522460 2013-01-01 18:56 0.118710 2013-01-01 18:57 0.167517 2013-01-01 18:58 0.922883 2013-01-01 18:59 1.721104 [600 rows x 1 columns] In [398]: dfp . loc [ \"2013-01-01 10h\" ] Out[398]: A 2013-01-01 10:00 -0.308975 2013-01-01 10:01 0.542520 2013-01-01 10:02 1.061068 2013-01-01 10:03 0.754005 2013-01-01 10:04 0.352933 ... ... 2013-01-01 10:55 -0.865621 2013-01-01 10:56 -1.167818 2013-01-01 10:57 -2.081748 2013-01-01 10:58 -0.527146 2013-01-01 10:59 0.802298 [60 rows x 1 columns] As with DatetimeIndex , the endpoints will be included in the result. The example below slices data starting from 10:00 to 11:59. In [399]: dfp [ \"2013-01-01 10h\" : \"2013-01-01 11h\" ] Out[399]: A 2013-01-01 10:00 -0.308975 2013-01-01 10:01 0.542520 2013-01-01 10:02 1.061068 2013-01-01 10:03 0.754005 2013-01-01 10:04 0.352933 ... ... 2013-01-01 11:55 -0.590204 2013-01-01 11:56 1.539990 2013-01-01 11:57 -1.224826 2013-01-01 11:58 0.578798 2013-01-01 11:59 -0.685496 [120 rows x 1 columns] Frequency conversion and resampling with PeriodIndex # The frequency of Period and PeriodIndex can be converted via the asfreq method. Letâs start with the fiscal year 2011, ending in December: In [400]: p = pd . Period ( \"2011\" , freq = \"Y-DEC\" ) In [401]: p Out[401]: Period('2011', 'Y-DEC') We can convert it to a monthly frequency. Using the how parameter, we can specify whether to return the starting or ending month: In [402]: p . asfreq ( \"M\" , how = \"start\" ) Out[402]: Period('2011-01', 'M') In [403]: p . asfreq ( \"M\" , how = \"end\" ) Out[403]: Period('2011-12', 'M') The shorthands âsâ and âeâ are provided for convenience: In [404]: p . asfreq ( \"M\" , \"s\" ) Out[404]: Period('2011-01', 'M') In [405]: p . asfreq ( \"M\" , \"e\" ) Out[405]: Period('2011-12', 'M') Converting to a âsuper-periodâ (e.g., annual frequency is a super-period of quarterly frequency) automatically returns the super-period that includes the input period: In [406]: p = pd . Period ( \"2011-12\" , freq = \"M\" ) In [407]: p . asfreq ( \"Y-NOV\" ) Out[407]: Period('2012', 'Y-NOV') Note that since we converted to an annual frequency that ends the year in November, the monthly period of December 2011 is actually in the 2012 Y-NOV period. Period conversions with anchored frequencies are particularly useful for working with various quarterly data common to economics, business, and other fields. Many organizations define quarters relative to the month in which their fiscal year starts and ends. Thus, first quarter of 2011 could start in 2010 or a few months into 2011. Via anchored frequencies, pandas works for all quarterly frequencies Q-JAN through Q-DEC . Q-DEC define regular calendar quarters: In [408]: p = pd . Period ( \"2012Q1\" , freq = \"Q-DEC\" ) In [409]: p . asfreq ( \"D\" , \"s\" ) Out[409]: Period('2012-01-01', 'D') In [410]: p . asfreq ( \"D\" , \"e\" ) Out[410]: Period('2012-03-31', 'D') Q-MAR defines fiscal year end in March: In [411]: p = pd . Period ( \"2011Q4\" , freq = \"Q-MAR\" ) In [412]: p . asfreq ( \"D\" , \"s\" ) Out[412]: Period('2011-01-01', 'D') In [413]: p . asfreq ( \"D\" , \"e\" ) Out[413]: Period('2011-03-31', 'D') Converting between representations # Timestamped data can be converted to PeriodIndex-ed data using to_period and vice-versa using to_timestamp : In [414]: rng = pd . date_range ( \"1/1/2012\" , periods = 5 , freq = \"ME\" ) In [415]: ts = pd . Series ( np . random . randn ( len ( rng )), index = rng ) In [416]: ts Out[416]: 2012-01-31 1.931253 2012-02-29 -0.184594 2012-03-31 0.249656 2012-04-30 -0.978151 2012-05-31 -0.873389 Freq: ME, dtype: float64 In [417]: ps = ts . to_period () In [418]: ps Out[418]: 2012-01 1.931253 2012-02 -0.184594 2012-03 0.249656 2012-04 -0.978151 2012-05 -0.873389 Freq: M, dtype: float64 In [419]: ps . to_timestamp () Out[419]: 2012-01-01 1.931253 2012-02-01 -0.184594 2012-03-01 0.249656 2012-04-01 -0.978151 2012-05-01 -0.873389 Freq: MS, dtype: float64 Remember that âsâ and âeâ can be used to return the timestamps at the start or end of the period: In [420]: ps . to_timestamp ( \"D\" , how = \"s\" ) Out[420]: 2012-01-01 1.931253 2012-02-01 -0.184594 2012-03-01 0.249656 2012-04-01 -0.978151 2012-05-01 -0.873389 Freq: MS, dtype: float64 Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end: In [421]: prng = pd . period_range ( \"1990Q1\" , \"2000Q4\" , freq = \"Q-NOV\" ) In [422]: ts = pd . Series ( np . random . randn ( len ( prng )), prng ) In [423]: ts . index = ( prng . asfreq ( \"M\" , \"e\" ) + 1 ) . asfreq ( \"h\" , \"s\" ) + 9 In [424]: ts . head () Out[424]: 1990-03-01 09:00 -0.109291 1990-06-01 09:00 -0.637235 1990-09-01 09:00 -1.735925 1990-12-01 09:00 2.096946 1991-03-01 09:00 -1.039926 Freq: h, dtype: float64 Representing out-of-bounds spans # If you have data that is outside of the Timestamp bounds, see Timestamp limitations , then you can use a PeriodIndex and/or Series of Periods to do computations. In [425]: span = pd . period_range ( \"1215-01-01\" , \"1381-01-01\" , freq = \"D\" ) In [426]: span Out[426]: PeriodIndex(['1215-01-01', '1215-01-02', '1215-01-03', '1215-01-04', '1215-01-05', '1215-01-06', '1215-01-07', '1215-01-08', '1215-01-09', '1215-01-10', ... '1380-12-23', '1380-12-24', '1380-12-25', '1380-12-26', '1380-12-27', '1380-12-28', '1380-12-29', '1380-12-30', '1380-12-31', '1381-01-01'], dtype='period[D]', length=60632) To convert from an int64 based YYYYMMDD representation. In [427]: s = pd . Series ([ 20121231 , 20141130 , 99991231 ]) In [428]: s Out[428]: 0 20121231 1 20141130 2 99991231 dtype: int64 In [429]: def conv ( x ): .....: return pd . Period ( year = x // 10000 , month = x // 100 % 100 , day = x % 100 , freq = \"D\" ) .....: In [430]: s . apply ( conv ) Out[430]: 0 2012-12-31 1 2014-11-30 2 9999-12-31 dtype: period[D] In [431]: s . apply ( conv )[ 2 ] Out[431]: Period('9999-12-31', 'D') These can easily be converted to a PeriodIndex : In [432]: span = pd . PeriodIndex ( s . apply ( conv )) In [433]: span Out[433]: PeriodIndex(['2012-12-31', '2014-11-30', '9999-12-31'], dtype='period[D]') Time zone handling # pandas provides rich support for working with timestamps in different time zones using the pytz and dateutil libraries or datetime.timezone objects from the standard library. Working with time zones # By default, pandas objects are time zone unaware: In [434]: rng = pd . date_range ( \"3/6/2012 00:00\" , periods = 15 , freq = \"D\" ) In [435]: rng . tz is None Out[435]: True To localize these dates to a time zone (assign a particular time zone to a naive date), you can use the tz_localize method or the tz keyword argument in date_range() , Timestamp , or DatetimeIndex . You can either pass pytz or dateutil time zone objects or Olson time zone database strings. Olson time zone strings will return pytz time zone objects by default. To return dateutil time zone objects, append dateutil/ before the string. In pytz you can find a list of common (and less common) time zones using from pytz import common_timezones, all_timezones . dateutil uses the OS time zones so there isnât a fixed list available. For common zones, the names are the same as pytz . In [436]: import dateutil # pytz In [437]: rng_pytz = pd . date_range ( \"3/6/2012 00:00\" , periods = 3 , freq = \"D\" , tz = \"Europe/London\" ) In [438]: rng_pytz . tz Out[438]: <DstTzInfo 'Europe/London' LMT-1 day, 23:59:00 STD> # dateutil In [439]: rng_dateutil = pd . date_range ( \"3/6/2012 00:00\" , periods = 3 , freq = \"D\" ) In [440]: rng_dateutil = rng_dateutil . tz_localize ( \"dateutil/Europe/London\" ) In [441]: rng_dateutil . tz Out[441]: tzfile('/usr/share/zoneinfo/Europe/London') # dateutil - utc special case In [442]: rng_utc = pd . date_range ( .....: \"3/6/2012 00:00\" , .....: periods = 3 , .....: freq = \"D\" , .....: tz = dateutil . tz . tzutc (), .....: ) .....: In [443]: rng_utc . tz Out[443]: tzutc() # datetime.timezone In [444]: rng_utc = pd . date_range ( .....: \"3/6/2012 00:00\" , .....: periods = 3 , .....: freq = \"D\" , .....: tz = datetime . timezone . utc , .....: ) .....: In [445]: rng_utc . tz Out[445]: datetime.timezone.utc Note that the UTC time zone is a special case in dateutil and should be constructed explicitly as an instance of dateutil.tz.tzutc . You can also construct other time zones objects explicitly first. In [446]: import pytz # pytz In [447]: tz_pytz = pytz . timezone ( \"Europe/London\" ) In [448]: rng_pytz = pd . date_range ( \"3/6/2012 00:00\" , periods = 3 , freq = \"D\" ) In [449]: rng_pytz = rng_pytz . tz_localize ( tz_pytz ) In [450]: rng_pytz . tz == tz_pytz Out[450]: True # dateutil In [451]: tz_dateutil = dateutil . tz . gettz ( \"Europe/London\" ) In [452]: rng_dateutil = pd . date_range ( \"3/6/2012 00:00\" , periods = 3 , freq = \"D\" , tz = tz_dateutil ) In [453]: rng_dateutil . tz == tz_dateutil Out[453]: True To convert a time zone aware pandas object from one time zone to another, you can use the tz_convert method. In [454]: rng_pytz . tz_convert ( \"US/Eastern\" ) Out[454]: DatetimeIndex(['2012-03-05 19:00:00-05:00', '2012-03-06 19:00:00-05:00', '2012-03-07 19:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None) Note When using pytz time zones, DatetimeIndex will construct a different time zone object than a Timestamp for the same time zone input. A DatetimeIndex can hold a collection of Timestamp objects that may have different UTC offsets and cannot be succinctly represented by one pytz time zone instance while one Timestamp represents one point in time with a specific UTC offset. In [455]: dti = pd . date_range ( \"2019-01-01\" , periods = 3 , freq = \"D\" , tz = \"US/Pacific\" ) In [456]: dti . tz Out[456]: <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD> In [457]: ts = pd . Timestamp ( \"2019-01-01\" , tz = \"US/Pacific\" ) In [458]: ts . tz Out[458]: <DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD> Warning Be wary of conversions between libraries. For some time zones, pytz and dateutil have different definitions of the zone. This is more of a problem for unusual time zones than for âstandardâ zones like US/Eastern . Warning Be aware that a time zone definition across versions of time zone libraries may not be considered equal. This may cause problems when working with stored data that is localized using one version and operated on with a different version. See here for how to handle such a situation. Warning For pytz time zones, it is incorrect to pass a time zone object directly into the datetime.datetime constructor (e.g., datetime.datetime(2011, 1, 1, tzinfo=pytz.timezone('US/Eastern')) . Instead, the datetime needs to be localized using the localize method on the pytz time zone object. Warning Be aware that for times in the future, correct conversion between time zones (and UTC) cannot be guaranteed by any time zone library because a timezoneâs offset from UTC may be changed by the respective government. Warning If you are using dates beyond 2038-01-18, due to current deficiencies in the underlying libraries caused by the year 2038 problem, daylight saving time (DST) adjustments to timezone aware dates will not be applied. If and when the underlying libraries are fixed, the DST transitions will be applied. For example, for two dates that are in British Summer Time (and so would normally be GMT+1), both the following asserts evaluate as true: In [459]: d_2037 = \"2037-03-31T010101\" In [460]: d_2038 = \"2038-03-31T010101\" In [461]: DST = \"Europe/London\" In [462]: assert pd . Timestamp ( d_2037 , tz = DST ) != pd . Timestamp ( d_2037 , tz = \"GMT\" ) In [463]: assert pd . Timestamp ( d_2038 , tz = DST ) == pd . Timestamp ( d_2038 , tz = \"GMT\" ) Under the hood, all timestamps are stored in UTC. Values from a time zone aware DatetimeIndex or Timestamp will have their fields (day, hour, minute, etc.) localized to the time zone. However, timestamps with the same UTC value are still considered to be equal even if they are in different time zones: In [464]: rng_eastern = rng_utc . tz_convert ( \"US/Eastern\" ) In [465]: rng_berlin = rng_utc . tz_convert ( \"Europe/Berlin\" ) In [466]: rng_eastern [ 2 ] Out[466]: Timestamp('2012-03-07 19:00:00-0500', tz='US/Eastern') In [467]: rng_berlin [ 2 ] Out[467]: Timestamp('2012-03-08 01:00:00+0100', tz='Europe/Berlin') In [468]: rng_eastern [ 2 ] == rng_berlin [ 2 ] Out[468]: True Operations between Series in different time zones will yield UTC Series , aligning the data on the UTC timestamps: In [469]: ts_utc = pd . Series ( range ( 3 ), pd . date_range ( \"20130101\" , periods = 3 , tz = \"UTC\" )) In [470]: eastern = ts_utc . tz_convert ( \"US/Eastern\" ) In [471]: berlin = ts_utc . tz_convert ( \"Europe/Berlin\" ) In [472]: result = eastern + berlin In [473]: result Out[473]: 2013-01-01 00:00:00+00:00 0 2013-01-02 00:00:00+00:00 2 2013-01-03 00:00:00+00:00 4 Freq: D, dtype: int64 In [474]: result . index Out[474]: DatetimeIndex(['2013-01-01 00:00:00+00:00', '2013-01-02 00:00:00+00:00', '2013-01-03 00:00:00+00:00'], dtype='datetime64[ns, UTC]', freq='D') To remove time zone information, use tz_localize(None) or tz_convert(None) . tz_localize(None) will remove the time zone yielding the local time representation. tz_convert(None) will remove the time zone after converting to UTC time. In [475]: didx = pd . date_range ( start = \"2014-08-01 09:00\" , freq = \"h\" , periods = 3 , tz = \"US/Eastern\" ) In [476]: didx Out[476]: DatetimeIndex(['2014-08-01 09:00:00-04:00', '2014-08-01 10:00:00-04:00', '2014-08-01 11:00:00-04:00'], dtype='datetime64[ns, US/Eastern]', freq='h') In [477]: didx . tz_localize ( None ) Out[477]: DatetimeIndex(['2014-08-01 09:00:00', '2014-08-01 10:00:00', '2014-08-01 11:00:00'], dtype='datetime64[ns]', freq=None) In [478]: didx . tz_convert ( None ) Out[478]: DatetimeIndex(['2014-08-01 13:00:00', '2014-08-01 14:00:00', '2014-08-01 15:00:00'], dtype='datetime64[ns]', freq='h') # tz_convert(None) is identical to tz_convert('UTC').tz_localize(None) In [479]: didx . tz_convert ( \"UTC\" ) . tz_localize ( None ) Out[479]: DatetimeIndex(['2014-08-01 13:00:00', '2014-08-01 14:00:00', '2014-08-01 15:00:00'], dtype='datetime64[ns]', freq=None) Fold # For ambiguous times, pandas supports explicitly specifying the keyword-only fold argument. Due to daylight saving time, one wall clock time can occur twice when shifting from summer to winter time; fold describes whether the datetime-like corresponds to the first (0) or the second time (1) the wall clock hits the ambiguous time. Fold is supported only for constructing from naive datetime.datetime (see datetime documentation for details) or from Timestamp or for constructing from components (see below). Only dateutil timezones are supported (see dateutil documentation for dateutil methods that deal with ambiguous datetimes) as pytz timezones do not support fold (see pytz documentation for details on how pytz deals with ambiguous datetimes). To localize an ambiguous datetime with pytz , please use Timestamp.tz_localize() . In general, we recommend to rely on Timestamp.tz_localize() when localizing ambiguous datetimes if you need direct control over how they are handled. In [480]: pd . Timestamp ( .....: datetime . datetime ( 2019 , 10 , 27 , 1 , 30 , 0 , 0 ), .....: tz = \"dateutil/Europe/London\" , .....: fold = 0 , .....: ) .....: Out[480]: Timestamp('2019-10-27 01:30:00+0100', tz='dateutil//usr/share/zoneinfo/Europe/London') In [481]: pd . Timestamp ( .....: year = 2019 , .....: month = 10 , .....: day = 27 , .....: hour = 1 , .....: minute = 30 , .....: tz = \"dateutil/Europe/London\" , .....: fold = 1 , .....: ) .....: Out[481]: Timestamp('2019-10-27 01:30:00+0000', tz='dateutil//usr/share/zoneinfo/Europe/London') Ambiguous times when localizing # tz_localize may not be able to determine the UTC offset of a timestamp because daylight savings time (DST) in a local time zone causes some times to occur twice within one day (âclocks fall backâ). The following options are available: 'raise' : Raises a pytz.AmbiguousTimeError (the default behavior) 'infer' : Attempt to determine the correct offset base on the monotonicity of the timestamps 'NaT' : Replaces ambiguous times with NaT bool : True represents a DST time, False represents non-DST time. An array-like of bool values is supported for a sequence of times. In [482]: rng_hourly = pd . DatetimeIndex ( .....: [ \"11/06/2011 00:00\" , \"11/06/2011 01:00\" , \"11/06/2011 01:00\" , \"11/06/2011 02:00\" ] .....: ) .....: This will fail as there are ambiguous times ( '11/06/2011 01:00' ) In [483]: rng_hourly . tz_localize ( 'US/Eastern' ) --------------------------------------------------------------------------- AmbiguousTimeError Traceback (most recent call last) Cell In [ 483 ], line 1 ----> 1 rng_hourly . tz_localize ( 'US/Eastern' ) File ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:293, in DatetimeIndex.tz_localize (self, tz, ambiguous, nonexistent) 286 @doc ( DatetimeArray . tz_localize ) 287 def tz_localize ( 288 self , ( ... ) 291 nonexistent : TimeNonexistent = \"raise\" , 292 ) -> Self : --> 293 arr = self . _data . tz_localize ( tz , ambiguous , nonexistent ) 294 return type ( self ) . _simple_new ( arr , name = self . name ) File ~/work/pandas/pandas/pandas/core/arrays/_mixins.py:81, in ravel_compat.<locals>.method (self, *args, **kwargs) 78 @wraps ( meth ) 79 def method ( self , * args , ** kwargs ): 80 if self . ndim == 1 : ---> 81 return meth ( self , * args , ** kwargs ) 83 flags = self . _ndarray . flags 84 flat = self . ravel ( \"K\" ) File ~/work/pandas/pandas/pandas/core/arrays/datetimes.py:1090, in DatetimeArray.tz_localize (self, tz, ambiguous, nonexistent) 1087 tz = timezones . maybe_get_tz ( tz ) 1088 # Convert to UTC -> 1090 new_dates = tzconversion . tz_localize_to_utc ( 1091 self . asi8 , 1092 tz , 1093 ambiguous = ambiguous , 1094 nonexistent = nonexistent , 1095 creso = self . _creso , 1096 ) 1097 new_dates_dt64 = new_dates . view ( f \"M8[ { self . unit } ]\" ) 1098 dtype = tz_to_dtype ( tz , unit = self . unit ) File ~/work/pandas/pandas/pandas/_libs/tslibs/tzconversion.pyx:371, in pandas._libs.tslibs.tzconversion.tz_localize_to_utc () AmbiguousTimeError : Cannot infer dst time from 2011-11-06 01:00:00, try using the 'ambiguous' argument Handle these ambiguous times by specifying the following. In [484]: rng_hourly . tz_localize ( \"US/Eastern\" , ambiguous = \"infer\" ) Out[484]: DatetimeIndex(['2011-11-06 00:00:00-04:00', '2011-11-06 01:00:00-04:00', '2011-11-06 01:00:00-05:00', '2011-11-06 02:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None) In [485]: rng_hourly . tz_localize ( \"US/Eastern\" , ambiguous = \"NaT\" ) Out[485]: DatetimeIndex(['2011-11-06 00:00:00-04:00', 'NaT', 'NaT', '2011-11-06 02:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None) In [486]: rng_hourly . tz_localize ( \"US/Eastern\" , ambiguous = [ True , True , False , False ]) Out[486]: DatetimeIndex(['2011-11-06 00:00:00-04:00', '2011-11-06 01:00:00-04:00', '2011-11-06 01:00:00-05:00', '2011-11-06 02:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None) Nonexistent times when localizing # A DST transition may also shift the local time ahead by 1 hour creating nonexistent local times (âclocks spring forwardâ). The behavior of localizing a timeseries with nonexistent times can be controlled by the nonexistent argument. The following options are available: 'raise' : Raises a pytz.NonExistentTimeError (the default behavior) 'NaT' : Replaces nonexistent times with NaT 'shift_forward' : Shifts nonexistent times forward to the closest real time 'shift_backward' : Shifts nonexistent times backward to the closest real time timedelta object: Shifts nonexistent times by the timedelta duration In [487]: dti = pd . date_range ( start = \"2015-03-29 02:30:00\" , periods = 3 , freq = \"h\" ) # 2:30 is a nonexistent time Localization of nonexistent times will raise an error by default. In [488]: dti . tz_localize ( 'Europe/Warsaw' ) --------------------------------------------------------------------------- NonExistentTimeError Traceback (most recent call last) Cell In [ 488 ], line 1 ----> 1 dti . tz_localize ( 'Europe/Warsaw' ) File ~/work/pandas/pandas/pandas/core/indexes/datetimes.py:293, in DatetimeIndex.tz_localize (self, tz, ambiguous, nonexistent) 286 @doc ( DatetimeArray . tz_localize ) 287 def tz_localize ( 288 self , ( ... ) 291 nonexistent : TimeNonexistent = \"raise\" , 292 ) -> Self : --> 293 arr = self . _data . tz_localize ( tz , ambiguous , nonexistent ) 294 return type ( self ) . _simple_new ( arr , name = self . name ) File ~/work/pandas/pandas/pandas/core/arrays/_mixins.py:81, in ravel_compat.<locals>.method (self, *args, **kwargs) 78 @wraps ( meth ) 79 def method ( self , * args , ** kwargs ): 80 if self . ndim == 1 : ---> 81 return meth ( self , * args , ** kwargs ) 83 flags = self . _ndarray . flags 84 flat = self . ravel ( \"K\" ) File ~/work/pandas/pandas/pandas/core/arrays/datetimes.py:1090, in DatetimeArray.tz_localize (self, tz, ambiguous, nonexistent) 1087 tz = timezones . maybe_get_tz ( tz ) 1088 # Convert to UTC -> 1090 new_dates = tzconversion . tz_localize_to_utc ( 1091 self . asi8 , 1092 tz , 1093 ambiguous = ambiguous , 1094 nonexistent = nonexistent , 1095 creso = self . _creso , 1096 ) 1097 new_dates_dt64 = new_dates . view ( f \"M8[ { self . unit } ]\" ) 1098 dtype = tz_to_dtype ( tz , unit = self . unit ) File ~/work/pandas/pandas/pandas/_libs/tslibs/tzconversion.pyx:431, in pandas._libs.tslibs.tzconversion.tz_localize_to_utc () NonExistentTimeError : 2015-03-29 02:30:00 Transform nonexistent times to NaT or shift the times. In [489]: dti Out[489]: DatetimeIndex(['2015-03-29 02:30:00', '2015-03-29 03:30:00', '2015-03-29 04:30:00'], dtype='datetime64[ns]', freq='h') In [490]: dti . tz_localize ( \"Europe/Warsaw\" , nonexistent = \"shift_forward\" ) Out[490]: DatetimeIndex(['2015-03-29 03:00:00+02:00', '2015-03-29 03:30:00+02:00', '2015-03-29 04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None) In [491]: dti . tz_localize ( \"Europe/Warsaw\" , nonexistent = \"shift_backward\" ) Out[491]: DatetimeIndex(['2015-03-29 01:59:59.999999999+01:00', '2015-03-29 03:30:00+02:00', '2015-03-29 04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None) In [492]: dti . tz_localize ( \"Europe/Warsaw\" , nonexistent = pd . Timedelta ( 1 , unit = \"h\" )) Out[492]: DatetimeIndex(['2015-03-29 03:30:00+02:00', '2015-03-29 03:30:00+02:00', '2015-03-29 04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None) In [493]: dti . tz_localize ( \"Europe/Warsaw\" , nonexistent = \"NaT\" ) Out[493]: DatetimeIndex(['NaT', '2015-03-29 03:30:00+02:00', '2015-03-29 04:30:00+02:00'], dtype='datetime64[ns, Europe/Warsaw]', freq=None) Time zone Series operations # A Series with time zone naive values is represented with a dtype of datetime64[ns] . In [494]: s_naive = pd . Series ( pd . date_range ( \"20130101\" , periods = 3 )) In [495]: s_naive Out[495]: 0 2013-01-01 1 2013-01-02 2 2013-01-03 dtype: datetime64[ns] A Series with a time zone aware values is represented with a dtype of datetime64[ns, tz] where tz is the time zone In [496]: s_aware = pd . Series ( pd . date_range ( \"20130101\" , periods = 3 , tz = \"US/Eastern\" )) In [497]: s_aware Out[497]: 0 2013-01-01 00:00:00-05:00 1 2013-01-02 00:00:00-05:00 2 2013-01-03 00:00:00-05:00 dtype: datetime64[ns, US/Eastern] Both of these Series time zone information can be manipulated via the .dt accessor, see the dt accessor section . For example, to localize and convert a naive stamp to time zone aware. In [498]: s_naive . dt . tz_localize ( \"UTC\" ) . dt . tz_convert ( \"US/Eastern\" ) Out[498]: 0 2012-12-31 19:00:00-05:00 1 2013-01-01 19:00:00-05:00 2 2013-01-02 19:00:00-05:00 dtype: datetime64[ns, US/Eastern] Time zone information can also be manipulated using the astype method. This method can convert between different timezone-aware dtypes. # convert to a new time zone In [499]: s_aware . astype ( \"datetime64[ns, CET]\" ) Out[499]: 0 2013-01-01 06:00:00+01:00 1 2013-01-02 06:00:00+01:00 2 2013-01-03 06:00:00+01:00 dtype: datetime64[ns, CET] Note Using Series.to_numpy() on a Series , returns a NumPy array of the data. NumPy does not currently support time zones (even though it is printing in the local time zone!), therefore an object array of Timestamps is returned for time zone aware data: In [500]: s_naive . to_numpy () Out[500]: array(['2013-01-01T00:00:00.000000000', '2013-01-02T00:00:00.000000000', '2013-01-03T00:00:00.000000000'], dtype='datetime64[ns]') In [501]: s_aware . to_numpy () Out[501]: array([Timestamp('2013-01-01 00:00:00-0500', tz='US/Eastern'), Timestamp('2013-01-02 00:00:00-0500', tz='US/Eastern'), Timestamp('2013-01-03 00:00:00-0500', tz='US/Eastern')], dtype=object) By converting to an object array of Timestamps, it preserves the time zone information. For example, when converting back to a Series: In [502]: pd . Series ( s_aware . to_numpy ()) Out[502]: 0 2013-01-01 00:00:00-05:00 1 2013-01-02 00:00:00-05:00 2 2013-01-03 00:00:00-05:00 dtype: datetime64[ns, US/Eastern] However, if you want an actual NumPy datetime64[ns] array (with the values converted to UTC) instead of an array of objects, you can specify the dtype argument: In [503]: s_aware . to_numpy ( dtype = \"datetime64[ns]\" ) Out[503]: array(['2013-01-01T05:00:00.000000000', '2013-01-02T05:00:00.000000000', '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]') previous Windowing operations next Time deltas On this page Overview Timestamps vs. time spans Converting to timestamps Providing a format argument Assembling datetime from multiple DataFrame columns Invalid data Epoch timestamps From timestamps to epoch Using the origin parameter Generating ranges of timestamps Custom frequency ranges Timestamp limitations Indexing Partial string indexing Slice vs. exact match Exact indexing Truncating & fancy indexing Time/date components DateOffset objects Parametric offsets Using offsets with Series / DatetimeIndex Custom business days Business hour Custom business hour Offset aliases Period aliases Combining aliases Anchored offsets Anchored offset semantics Holidays / holiday calendars Time Series-related instance methods Shifting / lagging Frequency conversion Filling forward / backward Converting to Python datetimes Resampling Basics Upsampling Sparse resampling Aggregation Iterating through groups Use origin or offset to adjust the start of the bins Backward resample Time span representation Period PeriodIndex and period_range Period dtypes PeriodIndex partial string indexing Frequency conversion and resampling with PeriodIndex Converting between representations Representing out-of-bounds spans Time zone handling Working with time zones Fold Ambiguous times when localizing Nonexistent times when localizing Time zone Series operations Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/timeseries.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Intro to... Intro to data structures # Weâll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, axis labeling, and alignment apply across all of the objects. To get started, import NumPy and load pandas into your namespace: In [1]: import numpy as np In [2]: import pandas as pd Fundamentally, data alignment is intrinsic . The link between labels and data will not be broken unless done so explicitly by you. Weâll give a brief intro to the data structures, then consider all of the broad categories of functionality and methods in separate sections. Series # Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index . The basic method to create a Series is to call: s = pd . Series ( data , index = index ) Here, data can be many different things: a Python dict an ndarray a scalar value (like 5) The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is : From ndarray If data is an ndarray, index must be the same length as data . If no index is passed, one will be created having values [0, ..., len(data) - 1] . In [3]: s = pd . Series ( np . random . randn ( 5 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) In [4]: s Out[4]: a 0.469112 b -0.282863 c -1.509059 d -1.135632 e 1.212112 dtype: float64 In [5]: s . index Out[5]: Index(['a', 'b', 'c', 'd', 'e'], dtype='object') In [6]: pd . Series ( np . random . randn ( 5 )) Out[6]: 0 -0.173215 1 0.119209 2 -1.044236 3 -0.861849 4 -2.104569 dtype: float64 Note pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time. From dict Series can be instantiated from dicts: In [7]: d = { \"b\" : 1 , \"a\" : 0 , \"c\" : 2 } In [8]: pd . Series ( d ) Out[8]: b 1 a 0 c 2 dtype: int64 If an index is passed, the values in data corresponding to the labels in the index will be pulled out. In [9]: d = { \"a\" : 0.0 , \"b\" : 1.0 , \"c\" : 2.0 } In [10]: pd . Series ( d ) Out[10]: a 0.0 b 1.0 c 2.0 dtype: float64 In [11]: pd . Series ( d , index = [ \"b\" , \"c\" , \"d\" , \"a\" ]) Out[11]: b 1.0 c 2.0 d NaN a 0.0 dtype: float64 Note NaN (not a number) is the standard missing data marker used in pandas. From scalar value If data is a scalar value, an index must be provided. The value will be repeated to match the length of index . In [12]: pd . Series ( 5.0 , index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) Out[12]: a 5.0 b 5.0 c 5.0 d 5.0 e 5.0 dtype: float64 Series is ndarray-like # Series acts very similarly to a ndarray and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index. In [13]: s . iloc [ 0 ] Out[13]: 0.4691122999071863 In [14]: s . iloc [: 3 ] Out[14]: a 0.469112 b -0.282863 c -1.509059 dtype: float64 In [15]: s [ s > s . median ()] Out[15]: a 0.469112 e 1.212112 dtype: float64 In [16]: s . iloc [[ 4 , 3 , 1 ]] Out[16]: e 1.212112 d -1.135632 b -0.282863 dtype: float64 In [17]: np . exp ( s ) Out[17]: a 1.598575 b 0.753623 c 0.221118 d 0.321219 e 3.360575 dtype: float64 Note We will address array-based indexing like s.iloc[[4, 3, 1]] in section on indexing . Like a NumPy array, a pandas Series has a single dtype . In [18]: s . dtype Out[18]: dtype('float64') This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPyâs type system in a few places, in which case the dtype would be an ExtensionDtype . Some examples within pandas are Categorical data and Nullable integer data type . See dtypes for more. If you need the actual array backing a Series , use Series.array . In [19]: s . array Out[19]: <NumpyExtensionArray> [ 0.4691122999071863, -0.2828633443286633, -1.5090585031735124, -1.1356323710171934, 1.2121120250208506] Length: 5, dtype: float64 Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment , for example). Series.array will always be an ExtensionArray . Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray . pandas knows how to take an ExtensionArray and store it in a Series or a column of a DataFrame . See dtypes for more. While Series is ndarray-like, if you need an actual ndarray, then use Series.to_numpy() . In [20]: s . to_numpy () Out[20]: array([ 0.4691, -0.2829, -1.5091, -1.1356, 1.2121]) Even if the Series is backed by a ExtensionArray , Series.to_numpy() will return a NumPy ndarray. Series is dict-like # A Series is also like a fixed-size dict in that you can get and set values by index label: In [21]: s [ \"a\" ] Out[21]: 0.4691122999071863 In [22]: s [ \"e\" ] = 12.0 In [23]: s Out[23]: a 0.469112 b -0.282863 c -1.509059 d -1.135632 e 12.000000 dtype: float64 In [24]: \"e\" in s Out[24]: True In [25]: \"f\" in s Out[25]: False If a label is not contained in the index, an exception is raised: In [26]: s [ \"f\" ] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) File ~/work/pandas/pandas/pandas/core/indexes/base.py:3812, in Index.get_loc (self, key) 3811 try : -> 3812 return self . _engine . get_loc ( casted_key ) 3813 except KeyError as err : File ~/work/pandas/pandas/pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc () File ~/work/pandas/pandas/pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc () File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item () File pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item () KeyError : 'f' The above exception was the direct cause of the following exception : KeyError Traceback (most recent call last) Cell In [ 26 ], line 1 ----> 1 s [ \"f\" ] File ~/work/pandas/pandas/pandas/core/series.py:1133, in Series.__getitem__ (self, key) 1130 return self . _values [ key ] 1132 elif key_is_scalar : -> 1133 return self . _get_value ( key ) 1135 # Convert generator to list before going through hashable part 1136 # (We will iterate through the generator there to check for slices) 1137 if is_iterator ( key ): File ~/work/pandas/pandas/pandas/core/series.py:1249, in Series._get_value (self, label, takeable) 1246 return self . _values [ label ] 1248 # Similar to Index.get_value, but we do not fall back to positional -> 1249 loc = self . index . get_loc ( label ) 1251 if is_integer ( loc ): 1252 return self . _values [ loc ] File ~/work/pandas/pandas/pandas/core/indexes/base.py:3819, in Index.get_loc (self, key) 3814 if isinstance ( casted_key , slice ) or ( 3815 isinstance ( casted_key , abc . Iterable ) 3816 and any ( isinstance ( x , slice ) for x in casted_key ) 3817 ): 3818 raise InvalidIndexError ( key ) -> 3819 raise KeyError ( key ) from err 3820 except TypeError : 3821 # If we have a listlike key, _check_indexing_error will raise 3822 # InvalidIndexError. Otherwise we fall through and re-raise 3823 # the TypeError. 3824 self . _check_indexing_error ( key ) KeyError : 'f' Using the Series.get() method, a missing label will return None or specified default: In [27]: s . get ( \"f\" ) In [28]: s . get ( \"f\" , np . nan ) Out[28]: nan These labels can also be accessed by attribute . Vectorized operations and label alignment with Series # When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with Series in pandas. Series can also be passed into most NumPy methods expecting an ndarray. In [29]: s + s Out[29]: a 0.938225 b -0.565727 c -3.018117 d -2.271265 e 24.000000 dtype: float64 In [30]: s * 2 Out[30]: a 0.938225 b -0.565727 c -3.018117 d -2.271265 e 24.000000 dtype: float64 In [31]: np . exp ( s ) Out[31]: a 1.598575 b 0.753623 c 0.221118 d 0.321219 e 162754.791419 dtype: float64 A key difference between Series and ndarray is that operations between Series automatically align the data based on label. Thus, you can write computations without giving consideration to whether the Series involved have the same labels. In [32]: s . iloc [ 1 :] + s . iloc [: - 1 ] Out[32]: a NaN b -0.565727 c -3.018117 d -2.271265 e NaN dtype: float64 The result of an operation between unaligned Series will have the union of the indexes involved. If a label is not found in one Series or the other, the result will be marked as missing NaN . Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data. Note In general, we chose to make the default result of operations between differently indexed objects yield the union of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the dropna function. Name attribute # Series also has a name attribute: In [33]: s = pd . Series ( np . random . randn ( 5 ), name = \"something\" ) In [34]: s Out[34]: 0 -0.494929 1 1.071804 2 0.721555 3 -0.706771 4 -1.039575 Name: something, dtype: float64 In [35]: s . name Out[35]: 'something' The Series name can be assigned automatically in many cases, in particular, when selecting a single column from a DataFrame , the name will be assigned the column label. You can rename a Series with the pandas.Series.rename() method. In [36]: s2 = s . rename ( \"different\" ) In [37]: s2 . name Out[37]: 'different' Note that s and s2 refer to different objects. DataFrame # DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input: Dict of 1D ndarrays, lists, dicts, or Series 2-D numpy.ndarray Structured or record ndarray A Series Another DataFrame Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index. If axis labels are not passed, they will be constructed from the input data based on common sense rules. From dict of Series or dicts # The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys. In [38]: d = { ....: \"one\" : pd . Series ([ 1.0 , 2.0 , 3.0 ], index = [ \"a\" , \"b\" , \"c\" ]), ....: \"two\" : pd . Series ([ 1.0 , 2.0 , 3.0 , 4.0 ], index = [ \"a\" , \"b\" , \"c\" , \"d\" ]), ....: } ....: In [39]: df = pd . DataFrame ( d ) In [40]: df Out[40]: one two a 1.0 1.0 b 2.0 2.0 c 3.0 3.0 d NaN 4.0 In [41]: pd . DataFrame ( d , index = [ \"d\" , \"b\" , \"a\" ]) Out[41]: one two d NaN 4.0 b 2.0 2.0 a 1.0 1.0 In [42]: pd . DataFrame ( d , index = [ \"d\" , \"b\" , \"a\" ], columns = [ \"two\" , \"three\" ]) Out[42]: two three d 4.0 NaN b 2.0 NaN a 1.0 NaN The row and column labels can be accessed respectively by accessing the index and columns attributes: Note When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict. In [43]: df . index Out[43]: Index(['a', 'b', 'c', 'd'], dtype='object') In [44]: df . columns Out[44]: Index(['one', 'two'], dtype='object') From dict of ndarrays / lists # All ndarrays must share the same length. If an index is passed, it must also be the same length as the arrays. If no index is passed, the result will be range(n) , where n is the array length. In [45]: d = { \"one\" : [ 1.0 , 2.0 , 3.0 , 4.0 ], \"two\" : [ 4.0 , 3.0 , 2.0 , 1.0 ]} In [46]: pd . DataFrame ( d ) Out[46]: one two 0 1.0 4.0 1 2.0 3.0 2 3.0 2.0 3 4.0 1.0 In [47]: pd . DataFrame ( d , index = [ \"a\" , \"b\" , \"c\" , \"d\" ]) Out[47]: one two a 1.0 4.0 b 2.0 3.0 c 3.0 2.0 d 4.0 1.0 From structured or record array # This case is handled identically to a dict of arrays. In [48]: data = np . zeros (( 2 ,), dtype = [( \"A\" , \"i4\" ), ( \"B\" , \"f4\" ), ( \"C\" , \"a10\" )]) In [49]: data [:] = [( 1 , 2.0 , \"Hello\" ), ( 2 , 3.0 , \"World\" )] In [50]: pd . DataFrame ( data ) Out[50]: A B C 0 1 2.0 b'Hello' 1 2 3.0 b'World' In [51]: pd . DataFrame ( data , index = [ \"first\" , \"second\" ]) Out[51]: A B C first 1 2.0 b'Hello' second 2 3.0 b'World' In [52]: pd . DataFrame ( data , columns = [ \"C\" , \"A\" , \"B\" ]) Out[52]: C A B 0 b'Hello' 1 2.0 1 b'World' 2 3.0 Note DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray. From a list of dicts # In [53]: data2 = [{ \"a\" : 1 , \"b\" : 2 }, { \"a\" : 5 , \"b\" : 10 , \"c\" : 20 }] In [54]: pd . DataFrame ( data2 ) Out[54]: a b c 0 1 2 NaN 1 5 10 20.0 In [55]: pd . DataFrame ( data2 , index = [ \"first\" , \"second\" ]) Out[55]: a b c first 1 2 NaN second 5 10 20.0 In [56]: pd . DataFrame ( data2 , columns = [ \"a\" , \"b\" ]) Out[56]: a b 0 1 2 1 5 10 From a dict of tuples # You can automatically create a MultiIndexed frame by passing a tuples dictionary. In [57]: pd . DataFrame ( ....: { ....: ( \"a\" , \"b\" ): {( \"A\" , \"B\" ): 1 , ( \"A\" , \"C\" ): 2 }, ....: ( \"a\" , \"a\" ): {( \"A\" , \"C\" ): 3 , ( \"A\" , \"B\" ): 4 }, ....: ( \"a\" , \"c\" ): {( \"A\" , \"B\" ): 5 , ( \"A\" , \"C\" ): 6 }, ....: ( \"b\" , \"a\" ): {( \"A\" , \"C\" ): 7 , ( \"A\" , \"B\" ): 8 }, ....: ( \"b\" , \"b\" ): {( \"A\" , \"D\" ): 9 , ( \"A\" , \"B\" ): 10 }, ....: } ....: ) ....: Out[57]: a b b a c a b A B 1.0 4.0 5.0 8.0 10.0 C 2.0 3.0 6.0 7.0 NaN D NaN NaN NaN NaN 9.0 From a Series # The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided). In [58]: ser = pd . Series ( range ( 3 ), index = list ( \"abc\" ), name = \"ser\" ) In [59]: pd . DataFrame ( ser ) Out[59]: ser a 0 b 1 c 2 From a list of namedtuples # The field names of the first namedtuple in the list determine the columns of the DataFrame . The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the DataFrame . If any of those tuples is shorter than the first namedtuple then the later columns in the corresponding row are marked as missing values. If any are longer than the first namedtuple , a ValueError is raised. In [60]: from collections import namedtuple In [61]: Point = namedtuple ( \"Point\" , \"x y\" ) In [62]: pd . DataFrame ([ Point ( 0 , 0 ), Point ( 0 , 3 ), ( 2 , 3 )]) Out[62]: x y 0 0 0 1 0 3 2 2 3 In [63]: Point3D = namedtuple ( \"Point3D\" , \"x y z\" ) In [64]: pd . DataFrame ([ Point3D ( 0 , 0 , 0 ), Point3D ( 0 , 3 , 5 ), Point ( 2 , 3 )]) Out[64]: x y z 0 0 0 0.0 1 0 3 5.0 2 2 3 NaN From a list of dataclasses # Data Classes as introduced in PEP557 , can be passed into the DataFrame constructor. Passing a list of dataclasses is equivalent to passing a list of dictionaries. Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a TypeError . In [65]: from dataclasses import make_dataclass In [66]: Point = make_dataclass ( \"Point\" , [( \"x\" , int ), ( \"y\" , int )]) In [67]: pd . DataFrame ([ Point ( 0 , 0 ), Point ( 0 , 3 ), Point ( 2 , 3 )]) Out[67]: x y 0 0 0 1 0 3 2 2 3 Missing data To construct a DataFrame with missing data, we use np.nan to represent missing values. Alternatively, you may pass a numpy.MaskedArray as the data argument to the DataFrame constructor, and its masked entries will be considered missing. See Missing data for more. Alternate constructors # DataFrame.from_dict DataFrame.from_dict() takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the DataFrame constructor except for the orient parameter which is 'columns' by default, but which can be set to 'index' in order to use the dict keys as row labels. In [68]: pd . DataFrame . from_dict ( dict ([( \"A\" , [ 1 , 2 , 3 ]), ( \"B\" , [ 4 , 5 , 6 ])])) Out[68]: A B 0 1 4 1 2 5 2 3 6 If you pass orient='index' , the keys will be the row labels. In this case, you can also pass the desired column names: In [69]: pd . DataFrame . from_dict ( ....: dict ([( \"A\" , [ 1 , 2 , 3 ]), ( \"B\" , [ 4 , 5 , 6 ])]), ....: orient = \"index\" , ....: columns = [ \"one\" , \"two\" , \"three\" ], ....: ) ....: Out[69]: one two three A 1 2 3 B 4 5 6 DataFrame.from_records DataFrame.from_records() takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal DataFrame constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. In [70]: data Out[70]: array([(1, 2., b'Hello'), (2, 3., b'World')], dtype=[('A', '<i4'), ('B', '<f4'), ('C', 'S10')]) In [71]: pd . DataFrame . from_records ( data , index = \"C\" ) Out[71]: A B C b'Hello' 1 2.0 b'World' 2 3.0 Column selection, addition, deletion # You can treat a DataFrame semantically like a dict of like-indexed Series objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations: In [72]: df [ \"one\" ] Out[72]: a 1.0 b 2.0 c 3.0 d NaN Name: one, dtype: float64 In [73]: df [ \"three\" ] = df [ \"one\" ] * df [ \"two\" ] In [74]: df [ \"flag\" ] = df [ \"one\" ] > 2 In [75]: df Out[75]: one two three flag a 1.0 1.0 1.0 False b 2.0 2.0 4.0 False c 3.0 3.0 9.0 True d NaN 4.0 NaN False Columns can be deleted or popped like with a dict: In [76]: del df [ \"two\" ] In [77]: three = df . pop ( \"three\" ) In [78]: df Out[78]: one flag a 1.0 False b 2.0 False c 3.0 True d NaN False When inserting a scalar value, it will naturally be propagated to fill the column: In [79]: df [ \"foo\" ] = \"bar\" In [80]: df Out[80]: one flag foo a 1.0 False bar b 2.0 False bar c 3.0 True bar d NaN False bar When inserting a Series that does not have the same index as the DataFrame , it will be conformed to the DataFrameâs index: In [81]: df [ \"one_trunc\" ] = df [ \"one\" ][: 2 ] In [82]: df Out[82]: one flag foo one_trunc a 1.0 False bar 1.0 b 2.0 False bar 2.0 c 3.0 True bar NaN d NaN False bar NaN You can insert raw ndarrays but their length must match the length of the DataFrameâs index. By default, columns get inserted at the end. DataFrame.insert() inserts at a particular location in the columns: In [83]: df . insert ( 1 , \"bar\" , df [ \"one\" ]) In [84]: df Out[84]: one bar flag foo one_trunc a 1.0 1.0 False bar 1.0 b 2.0 2.0 False bar 2.0 c 3.0 3.0 True bar NaN d NaN NaN False bar NaN Assigning new columns in method chains # Inspired by dplyrâs mutate verb, DataFrame has an assign() method that allows you to easily create new columns that are potentially derived from existing columns. In [85]: iris = pd . read_csv ( \"data/iris.data\" ) In [86]: iris . head () Out[86]: SepalLength SepalWidth PetalLength PetalWidth Name 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa In [87]: iris . assign ( sepal_ratio = iris [ \"SepalWidth\" ] / iris [ \"SepalLength\" ]) . head () Out[87]: SepalLength SepalWidth PetalLength PetalWidth Name sepal_ratio 0 5.1 3.5 1.4 0.2 Iris-setosa 0.686275 1 4.9 3.0 1.4 0.2 Iris-setosa 0.612245 2 4.7 3.2 1.3 0.2 Iris-setosa 0.680851 3 4.6 3.1 1.5 0.2 Iris-setosa 0.673913 4 5.0 3.6 1.4 0.2 Iris-setosa 0.720000 In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to. In [88]: iris . assign ( sepal_ratio = lambda x : ( x [ \"SepalWidth\" ] / x [ \"SepalLength\" ])) . head () Out[88]: SepalLength SepalWidth PetalLength PetalWidth Name sepal_ratio 0 5.1 3.5 1.4 0.2 Iris-setosa 0.686275 1 4.9 3.0 1.4 0.2 Iris-setosa 0.612245 2 4.7 3.2 1.3 0.2 Iris-setosa 0.680851 3 4.6 3.1 1.5 0.2 Iris-setosa 0.673913 4 5.0 3.6 1.4 0.2 Iris-setosa 0.720000 assign() always returns a copy of the data, leaving the original DataFrame untouched. Passing a callable, as opposed to an actual value to be inserted, is useful when you donât have a reference to the DataFrame at hand. This is common when using assign() in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot: In [89]: ( ....: iris . query ( \"SepalLength > 5\" ) ....: . assign ( ....: SepalRatio = lambda x : x . SepalWidth / x . SepalLength , ....: PetalRatio = lambda x : x . PetalWidth / x . PetalLength , ....: ) ....: . plot ( kind = \"scatter\" , x = \"SepalRatio\" , y = \"PetalRatio\" ) ....: ) ....: Out[89]: <Axes: xlabel='SepalRatio', ylabel='PetalRatio'> Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame thatâs been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didnât have a reference to the filtered DataFrame available. The function signature for assign() is simply **kwargs . The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a Series or NumPy array), or a function of one argument to be called on the DataFrame . A copy of the original DataFrame is returned, with the new values inserted. The order of **kwargs is preserved. This allows for dependent assignment, where an expression later in **kwargs can refer to a column created earlier in the same assign() . In [90]: dfa = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) In [91]: dfa . assign ( C = lambda x : x [ \"A\" ] + x [ \"B\" ], D = lambda x : x [ \"A\" ] + x [ \"C\" ]) Out[91]: A B C D 0 1 4 5 6 1 2 5 7 9 2 3 6 9 12 In the second expression, x['C'] will refer to the newly created column, thatâs equal to dfa['A'] + dfa['B'] . Indexing / selection # The basics of indexing are as follows: Operation Syntax Result Select column df[col] Series Select row by label df.loc[label] Series Select row by integer location df.iloc[loc] Series Slice rows df[5:10] DataFrame Select rows by boolean vector df[bool_vec] DataFrame Row selection, for example, returns a Series whose index is the columns of the DataFrame : In [92]: df . loc [ \"b\" ] Out[92]: one 2.0 bar 2.0 flag False foo bar one_trunc 2.0 Name: b, dtype: object In [93]: df . iloc [ 2 ] Out[93]: one 3.0 bar 3.0 flag True foo bar one_trunc NaN Name: c, dtype: object For a more exhaustive treatment of sophisticated label-based indexing and slicing, see the section on indexing . We will address the fundamentals of reindexing / conforming to new sets of labels in the section on reindexing . Data alignment and arithmetic # Data alignment between DataFrame objects automatically align on both the columns and the index (row labels) . Again, the resulting object will have the union of the column and row labels. In [94]: df = pd . DataFrame ( np . random . randn ( 10 , 4 ), columns = [ \"A\" , \"B\" , \"C\" , \"D\" ]) In [95]: df2 = pd . DataFrame ( np . random . randn ( 7 , 3 ), columns = [ \"A\" , \"B\" , \"C\" ]) In [96]: df + df2 Out[96]: A B C D 0 0.045691 -0.014138 1.380871 NaN 1 -0.955398 -1.501007 0.037181 NaN 2 -0.662690 1.534833 -0.859691 NaN 3 -2.452949 1.237274 -0.133712 NaN 4 1.414490 1.951676 -2.320422 NaN 5 -0.494922 -1.649727 -1.084601 NaN 6 -1.047551 -0.748572 -0.805479 NaN 7 NaN NaN NaN NaN 8 NaN NaN NaN NaN 9 NaN NaN NaN NaN When doing an operation between DataFrame and Series , the default behavior is to align the Series index on the DataFrame columns , thus broadcasting row-wise. For example: In [97]: df - df . iloc [ 0 ] Out[97]: A B C D 0 0.000000 0.000000 0.000000 0.000000 1 -1.359261 -0.248717 -0.453372 -1.754659 2 0.253128 0.829678 0.010026 -1.991234 3 -1.311128 0.054325 -1.724913 -1.620544 4 0.573025 1.500742 -0.676070 1.367331 5 -1.741248 0.781993 -1.241620 -2.053136 6 -1.240774 -0.869551 -0.153282 0.000430 7 -0.743894 0.411013 -0.929563 -0.282386 8 -1.194921 1.320690 0.238224 -1.482644 9 2.293786 1.856228 0.773289 -1.446531 For explicit control over the matching and broadcasting behavior, see the section on flexible binary operations . Arithmetic operations with scalars operate element-wise: In [98]: df * 5 + 2 Out[98]: A B C D 0 3.359299 -0.124862 4.835102 3.381160 1 -3.437003 -1.368449 2.568242 -5.392133 2 4.624938 4.023526 4.885230 -6.575010 3 -3.196342 0.146766 -3.789461 -4.721559 4 6.224426 7.378849 1.454750 10.217815 5 -5.346940 3.785103 -1.373001 -6.884519 6 -2.844569 -4.472618 4.068691 3.383309 7 -0.360173 1.930201 0.187285 1.969232 8 -2.615303 6.478587 6.026220 -4.032059 9 14.828230 9.156280 8.701544 -3.851494 In [99]: 1 / df Out[99]: A B C D 0 3.678365 -2.353094 1.763605 3.620145 1 -0.919624 -1.484363 8.799067 -0.676395 2 1.904807 2.470934 1.732964 -0.583090 3 -0.962215 -2.697986 -0.863638 -0.743875 4 1.183593 0.929567 -9.170108 0.608434 5 -0.680555 2.800959 -1.482360 -0.562777 6 -1.032084 -0.772485 2.416988 3.614523 7 -2.118489 -71.634509 -2.758294 -162.507295 8 -1.083352 1.116424 1.241860 -0.828904 9 0.389765 0.698687 0.746097 -0.854483 In [100]: df ** 4 Out[100]: A B C D 0 0.005462 3.261689e-02 0.103370 5.822320e-03 1 1.398165 2.059869e-01 0.000167 4.777482e+00 2 0.075962 2.682596e-02 0.110877 8.650845e+00 3 1.166571 1.887302e-02 1.797515 3.265879e+00 4 0.509555 1.339298e+00 0.000141 7.297019e+00 5 4.661717 1.624699e-02 0.207103 9.969092e+00 6 0.881334 2.808277e+00 0.029302 5.858632e-03 7 0.049647 3.797614e-08 0.017276 1.433866e-09 8 0.725974 6.437005e-01 0.420446 2.118275e+00 9 43.329821 4.196326e+00 3.227153 1.875802e+00 Boolean operators operate element-wise as well: In [101]: df1 = pd . DataFrame ({ \"a\" : [ 1 , 0 , 1 ], \"b\" : [ 0 , 1 , 1 ]}, dtype = bool ) In [102]: df2 = pd . DataFrame ({ \"a\" : [ 0 , 1 , 1 ], \"b\" : [ 1 , 1 , 0 ]}, dtype = bool ) In [103]: df1 & df2 Out[103]: a b 0 False False 1 False True 2 True False In [104]: df1 | df2 Out[104]: a b 0 True True 1 True True 2 True True In [105]: df1 ^ df2 Out[105]: a b 0 True True 1 True False 2 False True In [106]: - df1 Out[106]: a b 0 False True 1 True False 2 False False Transposing # To transpose, access the T attribute or DataFrame.transpose() , similar to an ndarray: # only show the first 5 rows In [107]: df [: 5 ] . T Out[107]: 0 1 2 3 4 A 0.271860 -1.087401 0.524988 -1.039268 0.844885 B -0.424972 -0.673690 0.404705 -0.370647 1.075770 C 0.567020 0.113648 0.577046 -1.157892 -0.109050 D 0.276232 -1.478427 -1.715002 -1.344312 1.643563 DataFrame interoperability with NumPy functions # Most NumPy functions can be called directly on Series and DataFrame . In [108]: np . exp ( df ) Out[108]: A B C D 0 1.312403 0.653788 1.763006 1.318154 1 0.337092 0.509824 1.120358 0.227996 2 1.690438 1.498861 1.780770 0.179963 3 0.353713 0.690288 0.314148 0.260719 4 2.327710 2.932249 0.896686 5.173571 5 0.230066 1.429065 0.509360 0.169161 6 0.379495 0.274028 1.512461 1.318720 7 0.623732 0.986137 0.695904 0.993865 8 0.397301 2.449092 2.237242 0.299269 9 13.009059 4.183951 3.820223 0.310274 In [109]: np . asarray ( df ) Out[109]: array([[ 0.2719, -0.425 , 0.567 , 0.2762], [-1.0874, -0.6737, 0.1136, -1.4784], [ 0.525 , 0.4047, 0.577 , -1.715 ], [-1.0393, -0.3706, -1.1579, -1.3443], [ 0.8449, 1.0758, -0.109 , 1.6436], [-1.4694, 0.357 , -0.6746, -1.7769], [-0.9689, -1.2945, 0.4137, 0.2767], [-0.472 , -0.014 , -0.3625, -0.0062], [-0.9231, 0.8957, 0.8052, -1.2064], [ 2.5656, 1.4313, 1.3403, -1.1703]]) DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array. Series implements __array_ufunc__ , which allows it to work with NumPyâs universal functions . The ufunc is applied to the underlying array in a Series . In [110]: ser = pd . Series ([ 1 , 2 , 3 , 4 ]) In [111]: np . exp ( ser ) Out[111]: 0 2.718282 1 7.389056 2 20.085537 3 54.598150 dtype: float64 When multiple Series are passed to a ufunc, they are aligned before performing the operation. Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using numpy.remainder() on two Series with differently ordered labels will align before the operation. In [112]: ser1 = pd . Series ([ 1 , 2 , 3 ], index = [ \"a\" , \"b\" , \"c\" ]) In [113]: ser2 = pd . Series ([ 1 , 3 , 5 ], index = [ \"b\" , \"a\" , \"c\" ]) In [114]: ser1 Out[114]: a 1 b 2 c 3 dtype: int64 In [115]: ser2 Out[115]: b 1 a 3 c 5 dtype: int64 In [116]: np . remainder ( ser1 , ser2 ) Out[116]: a 1 b 0 c 3 dtype: int64 As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values. In [117]: ser3 = pd . Series ([ 2 , 4 , 6 ], index = [ \"b\" , \"c\" , \"d\" ]) In [118]: ser3 Out[118]: b 2 c 4 d 6 dtype: int64 In [119]: np . remainder ( ser1 , ser3 ) Out[119]: a NaN b 0.0 c 3.0 d NaN dtype: float64 When a binary ufunc is applied to a Series and Index , the Series implementation takes precedence and a Series is returned. In [120]: ser = pd . Series ([ 1 , 2 , 3 ]) In [121]: idx = pd . Index ([ 4 , 5 , 6 ]) In [122]: np . maximum ( ser , idx ) Out[122]: 0 4 1 5 2 6 dtype: int64 NumPy ufuncs are safe to apply to Series backed by non-ndarray arrays, for example arrays.SparseArray (see Sparse calculation ). If possible, the ufunc is applied without converting the underlying data to an ndarray. Console display # A very large DataFrame will be truncated to display them in the console. You can also get a summary using info() . (The baseball dataset is from the plyr R package): In [123]: baseball = pd . read_csv ( \"data/baseball.csv\" ) In [124]: print ( baseball ) id player year stint team lg ... so ibb hbp sh sf gidp 0 88641 womacto01 2006 2 CHN NL ... 4.0 0.0 0.0 3.0 0.0 0.0 1 88643 schilcu01 2006 1 BOS AL ... 1.0 0.0 0.0 0.0 0.0 0.0 .. ... ... ... ... ... .. ... ... ... ... ... ... ... 98 89533 aloumo01 2007 1 NYN NL ... 30.0 5.0 2.0 0.0 3.0 13.0 99 89534 alomasa02 2007 1 NYN NL ... 3.0 0.0 0.0 0.0 0.0 0.0 [100 rows x 23 columns] In [125]: baseball . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 100 entries, 0 to 99 Data columns (total 23 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 100 non-null int64 1 player 100 non-null object 2 year 100 non-null int64 3 stint 100 non-null int64 4 team 100 non-null object 5 lg 100 non-null object 6 g 100 non-null int64 7 ab 100 non-null int64 8 r 100 non-null int64 9 h 100 non-null int64 10 X2b 100 non-null int64 11 X3b 100 non-null int64 12 hr 100 non-null int64 13 rbi 100 non-null float64 14 sb 100 non-null float64 15 cs 100 non-null float64 16 bb 100 non-null int64 17 so 100 non-null float64 18 ibb 100 non-null float64 19 hbp 100 non-null float64 20 sh 100 non-null float64 21 sf 100 non-null float64 22 gidp 100 non-null float64 dtypes: float64(9), int64(11), object(3) memory usage: 18.1+ KB However, using DataFrame.to_string() will return a string representation of the DataFrame in tabular form, though it wonât always fit the console width: In [126]: print ( baseball . iloc [ - 20 :, : 12 ] . to_string ()) id player year stint team lg g ab r h X2b X3b 80 89474 finlest01 2007 1 COL NL 43 94 9 17 3 0 81 89480 embreal01 2007 1 OAK AL 4 0 0 0 0 0 82 89481 edmonji01 2007 1 SLN NL 117 365 39 92 15 2 83 89482 easleda01 2007 1 NYN NL 76 193 24 54 6 0 84 89489 delgaca01 2007 1 NYN NL 139 538 71 139 30 0 85 89493 cormirh01 2007 1 CIN NL 6 0 0 0 0 0 86 89494 coninje01 2007 2 NYN NL 21 41 2 8 2 0 87 89495 coninje01 2007 1 CIN NL 80 215 23 57 11 1 88 89497 clemero02 2007 1 NYA AL 2 2 0 1 0 0 89 89498 claytro01 2007 2 BOS AL 8 6 1 0 0 0 90 89499 claytro01 2007 1 TOR AL 69 189 23 48 14 0 91 89501 cirilje01 2007 2 ARI NL 28 40 6 8 4 0 92 89502 cirilje01 2007 1 MIN AL 50 153 18 40 9 2 93 89521 bondsba01 2007 1 SFN NL 126 340 75 94 14 0 94 89523 biggicr01 2007 1 HOU NL 141 517 68 130 31 3 95 89525 benitar01 2007 2 FLO NL 34 0 0 0 0 0 96 89526 benitar01 2007 1 SFN NL 19 0 0 0 0 0 97 89530 ausmubr01 2007 1 HOU NL 117 349 38 82 16 3 98 89533 aloumo01 2007 1 NYN NL 87 328 51 112 19 1 99 89534 alomasa02 2007 1 NYN NL 8 22 1 3 1 0 Wide DataFrames will be printed across multiple rows by default: In [127]: pd . DataFrame ( np . random . randn ( 3 , 12 )) Out[127]: 0 1 2 ... 9 10 11 0 -1.226825 0.769804 -1.281247 ... -1.110336 -0.619976 0.149748 1 -0.732339 0.687738 0.176444 ... 1.462696 -1.743161 -0.826591 2 -0.345352 1.314232 0.690579 ... 0.896171 -0.487602 -0.082240 [3 rows x 12 columns] You can change how much to print on a single row by setting the display.width option: In [128]: pd . set_option ( \"display.width\" , 40 ) # default is 80 In [129]: pd . DataFrame ( np . random . randn ( 3 , 12 )) Out[129]: 0 1 2 ... 9 10 11 0 -2.182937 0.380396 0.084844 ... -0.023688 2.410179 1.450520 1 0.206053 -0.251905 -2.213588 ... -0.025747 -0.988387 0.094055 2 1.262731 1.289997 0.082423 ... -0.281461 0.030711 0.109121 [3 rows x 12 columns] You can adjust the max width of the individual columns by setting display.max_colwidth In [130]: datafile = { .....: \"filename\" : [ \"filename_01\" , \"filename_02\" ], .....: \"path\" : [ .....: \"media/user_name/storage/folder_01/filename_01\" , .....: \"media/user_name/storage/folder_02/filename_02\" , .....: ], .....: } .....: In [131]: pd . set_option ( \"display.max_colwidth\" , 30 ) In [132]: pd . DataFrame ( datafile ) Out[132]: filename path 0 filename_01 media/user_name/storage/fo... 1 filename_02 media/user_name/storage/fo... In [133]: pd . set_option ( \"display.max_colwidth\" , 100 ) In [134]: pd . DataFrame ( datafile ) Out[134]: filename path 0 filename_01 media/user_name/storage/folder_01/filename_01 1 filename_02 media/user_name/storage/folder_02/filename_02 You can also disable this feature via the expand_frame_repr option. This will print the table in one block. DataFrame column attribute access and IPython completion # If a DataFrame column label is a valid Python variable name, the column can be accessed like an attribute: In [135]: df = pd . DataFrame ({ \"foo1\" : np . random . randn ( 5 ), \"foo2\" : np . random . randn ( 5 )}) In [136]: df Out[136]: foo1 foo2 0 1.126203 0.781836 1 -0.977349 -1.071357 2 1.474071 0.441153 3 -0.064034 2.353925 4 -1.282782 0.583787 In [137]: df . foo1 Out[137]: 0 1.126203 1 -0.977349 2 1.474071 3 -0.064034 4 -1.282782 Name: foo1, dtype: float64 The columns are also connected to the IPython completion mechanism so they can be tab-completed: In [5]: df . foo < TAB > # noqa: E225, E999 df.foo1 df.foo2 previous 10 minutes to pandas next Essential basic functionality On this page Series Series is ndarray-like Series is dict-like Vectorized operations and label alignment with Series Name attribute DataFrame From dict of Series or dicts From dict of ndarrays / lists From structured or record array From a list of dicts From a dict of tuples From a Series From a list of namedtuples From a list of dataclasses Alternate constructors Column selection, addition, deletion Assigning new columns in method chains Indexing / selection Data alignment and arithmetic Transposing DataFrame interoperability with NumPy functions Console display DataFrame column attribute access and IPython completion Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/dsintro.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Testing Testing # Assertion functions # testing.assert_frame_equal (left,Â right[,Â ...]) Check that left and right DataFrame are equal. testing.assert_series_equal (left,Â right[,Â ...]) Check that left and right Series are equal. testing.assert_index_equal (left,Â right[,Â ...]) Check that left and right Index are equal. testing.assert_extension_array_equal (left,Â right) Check that left and right ExtensionArrays are equal. Exceptions and warnings # errors.AbstractMethodError (class_instance[,Â ...]) Raise this error instead of NotImplementedError for abstract methods. errors.AttributeConflictWarning Warning raised when index attributes conflict when using HDFStore. errors.CategoricalConversionWarning Warning is raised when reading a partial labeled Stata file using a iterator. errors.ChainedAssignmentError Warning raised when trying to set using chained assignment. errors.ClosedFileError Exception is raised when trying to perform an operation on a closed HDFStore file. errors.CSSWarning Warning is raised when converting css styling fails. errors.DatabaseError Error is raised when executing sql with bad syntax or sql that throws an error. errors.DataError Exceptionn raised when performing an operation on non-numerical data. errors.DtypeWarning Warning raised when reading different dtypes in a column from a file. errors.DuplicateLabelError Error raised when an operation would introduce duplicate labels. errors.EmptyDataError Exception raised in pd.read_csv when empty data or header is encountered. errors.IncompatibilityWarning Warning raised when trying to use where criteria on an incompatible HDF5 file. errors.IndexingError Exception is raised when trying to index and there is a mismatch in dimensions. errors.InvalidColumnName Warning raised by to_stata the column contains a non-valid stata name. errors.InvalidComparison Exception is raised by _validate_comparison_value to indicate an invalid comparison. errors.InvalidIndexError Exception raised when attempting to use an invalid index key. errors.InvalidVersion An invalid version was found, users should refer to PEP 440. errors.IntCastingNaNError Exception raised when converting ( astype ) an array with NaN to an integer type. errors.LossySetitemError Raised when trying to do a __setitem__ on an np.ndarray that is not lossless. errors.MergeError Exception raised when merging data. errors.NoBufferPresent Exception is raised in _get_data_buffer to signal that there is no requested buffer. errors.NullFrequencyError Exception raised when a freq cannot be null. errors.NumbaUtilError Error raised for unsupported Numba engine routines. errors.NumExprClobberingError Exception raised when trying to use a built-in numexpr name as a variable name. errors.OptionError Exception raised for pandas.options. errors.OutOfBoundsDatetime Raised when the datetime is outside the range that can be represented. errors.OutOfBoundsTimedelta Raised when encountering a timedelta value that cannot be represented. errors.ParserError Exception that is raised by an error encountered in parsing file contents. errors.ParserWarning Warning raised when reading a file that doesn't use the default 'c' parser. errors.PerformanceWarning Warning raised when there is a possible performance impact. errors.PossibleDataLossError Exception raised when trying to open a HDFStore file when already opened. errors.PossiblePrecisionLoss Warning raised by to_stata on a column with a value outside or equal to int64. errors.PyperclipException Exception raised when clipboard functionality is unsupported. errors.PyperclipWindowsException (message) Exception raised when clipboard functionality is unsupported by Windows. errors.SettingWithCopyError Exception raised when trying to set on a copied slice from a DataFrame . errors.SettingWithCopyWarning Warning raised when trying to set on a copied slice from a DataFrame . errors.SpecificationError Exception raised by agg when the functions are ill-specified. errors.UndefinedVariableError (name[,Â is_local]) Exception raised by query or eval when using an undefined variable name. errors.UnsortedIndexError Error raised when slicing a MultiIndex which has not been lexsorted. errors.UnsupportedFunctionCall Exception raised when attempting to call a unsupported numpy function. errors.ValueLabelTypeMismatch Warning raised by to_stata on a category column that contains non-string values. Bug report function # show_versions ([as_json]) Provide useful information, important for bug reports. Test suite runner # test ([extra_args,Â run_doctests]) Run the pandas test suite using pytest. previous pandas.api.indexers.check_array_indexer next pandas.testing.assert_frame_equal On this page Assertion functions Exceptions and warnings Bug report function Test suite runner Show Source",
    "url": "https://pandas.pydata.org/docs/reference/testing.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Extensions pandas.api.e... pandas.api.extensions.register_extension_dtype # pandas.api.extensions. register_extension_dtype ( cls ) [source] # Register an ExtensionType with pandas as class decorator. This enables operations like .astype(name) for the name of the ExtensionDtype. Returns : callable A class decorator. Examples >>> from pandas.api.extensions import register_extension_dtype , ExtensionDtype >>> @register_extension_dtype ... class MyExtensionDtype ( ExtensionDtype ): ... name = \"myextension\" previous Extensions next pandas.api.extensions.register_dataframe_accessor On this page register_extension_dtype() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_extension_dtype.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Time deltas Time deltas # Timedeltas are differences in times, expressed in difference units, e.g. days, hours, minutes, seconds. They can be both positive and negative. Timedelta is a subclass of datetime.timedelta , and behaves in a similar manner, but allows compatibility with np.timedelta64 types as well as a host of custom representation, parsing, and attributes. Parsing # You can construct a Timedelta scalar through various arguments, including ISO 8601 Duration strings. In [1]: import datetime # strings In [2]: pd . Timedelta ( \"1 days\" ) Out[2]: Timedelta('1 days 00:00:00') In [3]: pd . Timedelta ( \"1 days 00:00:00\" ) Out[3]: Timedelta('1 days 00:00:00') In [4]: pd . Timedelta ( \"1 days 2 hours\" ) Out[4]: Timedelta('1 days 02:00:00') In [5]: pd . Timedelta ( \"-1 days 2 min 3us\" ) Out[5]: Timedelta('-2 days +23:57:59.999997') # like datetime.timedelta # note: these MUST be specified as keyword arguments In [6]: pd . Timedelta ( days = 1 , seconds = 1 ) Out[6]: Timedelta('1 days 00:00:01') # integers with a unit In [7]: pd . Timedelta ( 1 , unit = \"d\" ) Out[7]: Timedelta('1 days 00:00:00') # from a datetime.timedelta/np.timedelta64 In [8]: pd . Timedelta ( datetime . timedelta ( days = 1 , seconds = 1 )) Out[8]: Timedelta('1 days 00:00:01') In [9]: pd . Timedelta ( np . timedelta64 ( 1 , \"ms\" )) Out[9]: Timedelta('0 days 00:00:00.001000') # negative Timedeltas have this string repr # to be more consistent with datetime.timedelta conventions In [10]: pd . Timedelta ( \"-1us\" ) Out[10]: Timedelta('-1 days +23:59:59.999999') # a NaT In [11]: pd . Timedelta ( \"nan\" ) Out[11]: NaT In [12]: pd . Timedelta ( \"nat\" ) Out[12]: NaT # ISO 8601 Duration strings In [13]: pd . Timedelta ( \"P0DT0H1M0S\" ) Out[13]: Timedelta('0 days 00:01:00') In [14]: pd . Timedelta ( \"P0DT0H0M0.000000123S\" ) Out[14]: Timedelta('0 days 00:00:00.000000123') DateOffsets ( Day, Hour, Minute, Second, Milli, Micro, Nano ) can also be used in construction. In [15]: pd . Timedelta ( pd . offsets . Second ( 2 )) Out[15]: Timedelta('0 days 00:00:02') Further, operations among the scalars yield another scalar Timedelta . In [16]: pd . Timedelta ( pd . offsets . Day ( 2 )) + pd . Timedelta ( pd . offsets . Second ( 2 )) + pd . Timedelta ( ....: \"00:00:00.000123\" ....: ) ....: Out[16]: Timedelta('2 days 00:00:02.000123') to_timedelta # Using the top-level pd.to_timedelta , you can convert a scalar, array, list, or Series from a recognized timedelta format / value into a Timedelta type. It will construct Series if the input is a Series, a scalar if the input is scalar-like, otherwise it will output a TimedeltaIndex . You can parse a single string to a Timedelta: In [17]: pd . to_timedelta ( \"1 days 06:05:01.00003\" ) Out[17]: Timedelta('1 days 06:05:01.000030') In [18]: pd . to_timedelta ( \"15.5us\" ) Out[18]: Timedelta('0 days 00:00:00.000015500') or a list/array of strings: In [19]: pd . to_timedelta ([ \"1 days 06:05:01.00003\" , \"15.5us\" , \"nan\" ]) Out[19]: TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015500', NaT], dtype='timedelta64[ns]', freq=None) The unit keyword argument specifies the unit of the Timedelta if the input is numeric: In [20]: pd . to_timedelta ( np . arange ( 5 ), unit = \"s\" ) Out[20]: TimedeltaIndex(['0 days 00:00:00', '0 days 00:00:01', '0 days 00:00:02', '0 days 00:00:03', '0 days 00:00:04'], dtype='timedelta64[ns]', freq=None) In [21]: pd . to_timedelta ( np . arange ( 5 ), unit = \"d\" ) Out[21]: TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'], dtype='timedelta64[ns]', freq=None) Warning If a string or array of strings is passed as an input then the unit keyword argument will be ignored. If a string without units is passed then the default unit of nanoseconds is assumed. Timedelta limitations # pandas represents Timedeltas in nanosecond resolution using 64 bit integers. As such, the 64 bit integer limits determine the Timedelta limits. In [22]: pd . Timedelta . min Out[22]: Timedelta('-106752 days +00:12:43.145224193') In [23]: pd . Timedelta . max Out[23]: Timedelta('106751 days 23:47:16.854775807') Operations # You can operate on Series/DataFrames and construct timedelta64[ns] Series through subtraction operations on datetime64[ns] Series, or Timestamps . In [24]: s = pd . Series ( pd . date_range ( \"2012-1-1\" , periods = 3 , freq = \"D\" )) In [25]: td = pd . Series ([ pd . Timedelta ( days = i ) for i in range ( 3 )]) In [26]: df = pd . DataFrame ({ \"A\" : s , \"B\" : td }) In [27]: df Out[27]: A B 0 2012-01-01 0 days 1 2012-01-02 1 days 2 2012-01-03 2 days In [28]: df [ \"C\" ] = df [ \"A\" ] + df [ \"B\" ] In [29]: df Out[29]: A B C 0 2012-01-01 0 days 2012-01-01 1 2012-01-02 1 days 2012-01-03 2 2012-01-03 2 days 2012-01-05 In [30]: df . dtypes Out[30]: A datetime64[ns] B timedelta64[ns] C datetime64[ns] dtype: object In [31]: s - s . max () Out[31]: 0 -2 days 1 -1 days 2 0 days dtype: timedelta64[ns] In [32]: s - datetime . datetime ( 2011 , 1 , 1 , 3 , 5 ) Out[32]: 0 364 days 20:55:00 1 365 days 20:55:00 2 366 days 20:55:00 dtype: timedelta64[ns] In [33]: s + datetime . timedelta ( minutes = 5 ) Out[33]: 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] In [34]: s + pd . offsets . Minute ( 5 ) Out[34]: 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] In [35]: s + pd . offsets . Minute ( 5 ) + pd . offsets . Milli ( 5 ) Out[35]: 0 2012-01-01 00:05:00.005 1 2012-01-02 00:05:00.005 2 2012-01-03 00:05:00.005 dtype: datetime64[ns] Operations with scalars from a timedelta64[ns] series: In [36]: y = s - s [ 0 ] In [37]: y Out[37]: 0 0 days 1 1 days 2 2 days dtype: timedelta64[ns] Series of timedeltas with NaT values are supported: In [38]: y = s - s . shift () In [39]: y Out[39]: 0 NaT 1 1 days 2 1 days dtype: timedelta64[ns] Elements can be set to NaT using np.nan analogously to datetimes: In [40]: y [ 1 ] = np . nan In [41]: y Out[41]: 0 NaT 1 NaT 2 1 days dtype: timedelta64[ns] Operands can also appear in a reversed order (a singular object operated with a Series): In [42]: s . max () - s Out[42]: 0 2 days 1 1 days 2 0 days dtype: timedelta64[ns] In [43]: datetime . datetime ( 2011 , 1 , 1 , 3 , 5 ) - s Out[43]: 0 -365 days +03:05:00 1 -366 days +03:05:00 2 -367 days +03:05:00 dtype: timedelta64[ns] In [44]: datetime . timedelta ( minutes = 5 ) + s Out[44]: 0 2012-01-01 00:05:00 1 2012-01-02 00:05:00 2 2012-01-03 00:05:00 dtype: datetime64[ns] min, max and the corresponding idxmin, idxmax operations are supported on frames: In [45]: A = s - pd . Timestamp ( \"20120101\" ) - pd . Timedelta ( \"00:05:05\" ) In [46]: B = s - pd . Series ( pd . date_range ( \"2012-1-2\" , periods = 3 , freq = \"D\" )) In [47]: df = pd . DataFrame ({ \"A\" : A , \"B\" : B }) In [48]: df Out[48]: A B 0 -1 days +23:54:55 -1 days 1 0 days 23:54:55 -1 days 2 1 days 23:54:55 -1 days In [49]: df . min () Out[49]: A -1 days +23:54:55 B -1 days +00:00:00 dtype: timedelta64[ns] In [50]: df . min ( axis = 1 ) Out[50]: 0 -1 days 1 -1 days 2 -1 days dtype: timedelta64[ns] In [51]: df . idxmin () Out[51]: A 0 B 0 dtype: int64 In [52]: df . idxmax () Out[52]: A 2 B 0 dtype: int64 min, max, idxmin, idxmax operations are supported on Series as well. A scalar result will be a Timedelta . In [53]: df . min () . max () Out[53]: Timedelta('-1 days +23:54:55') In [54]: df . min ( axis = 1 ) . min () Out[54]: Timedelta('-1 days +00:00:00') In [55]: df . min () . idxmax () Out[55]: 'A' In [56]: df . min ( axis = 1 ) . idxmin () Out[56]: 0 You can fillna on timedeltas, passing a timedelta to get a particular value. In [57]: y . fillna ( pd . Timedelta ( 0 )) Out[57]: 0 0 days 1 0 days 2 1 days dtype: timedelta64[ns] In [58]: y . fillna ( pd . Timedelta ( 10 , unit = \"s\" )) Out[58]: 0 0 days 00:00:10 1 0 days 00:00:10 2 1 days 00:00:00 dtype: timedelta64[ns] In [59]: y . fillna ( pd . Timedelta ( \"-1 days, 00:00:05\" )) Out[59]: 0 -1 days +00:00:05 1 -1 days +00:00:05 2 1 days 00:00:00 dtype: timedelta64[ns] You can also negate, multiply and use abs on Timedeltas : In [60]: td1 = pd . Timedelta ( \"-1 days 2 hours 3 seconds\" ) In [61]: td1 Out[61]: Timedelta('-2 days +21:59:57') In [62]: - 1 * td1 Out[62]: Timedelta('1 days 02:00:03') In [63]: - td1 Out[63]: Timedelta('1 days 02:00:03') In [64]: abs ( td1 ) Out[64]: Timedelta('1 days 02:00:03') Reductions # Numeric reduction operation for timedelta64[ns] will return Timedelta objects. As usual NaT are skipped during evaluation. In [65]: y2 = pd . Series ( ....: pd . to_timedelta ([ \"-1 days +00:00:05\" , \"nat\" , \"-1 days +00:00:05\" , \"1 days\" ]) ....: ) ....: In [66]: y2 Out[66]: 0 -1 days +00:00:05 1 NaT 2 -1 days +00:00:05 3 1 days 00:00:00 dtype: timedelta64[ns] In [67]: y2 . mean () Out[67]: Timedelta('-1 days +16:00:03.333333334') In [68]: y2 . median () Out[68]: Timedelta('-1 days +00:00:05') In [69]: y2 . quantile ( 0.1 ) Out[69]: Timedelta('-1 days +00:00:05') In [70]: y2 . sum () Out[70]: Timedelta('-1 days +00:00:10') Frequency conversion # Timedelta Series and TimedeltaIndex , and Timedelta can be converted to other frequencies by astyping to a specific timedelta dtype. In [71]: december = pd . Series ( pd . date_range ( \"20121201\" , periods = 4 )) In [72]: january = pd . Series ( pd . date_range ( \"20130101\" , periods = 4 )) In [73]: td = january - december In [74]: td [ 2 ] += datetime . timedelta ( minutes = 5 , seconds = 3 ) In [75]: td [ 3 ] = np . nan In [76]: td Out[76]: 0 31 days 00:00:00 1 31 days 00:00:00 2 31 days 00:05:03 3 NaT dtype: timedelta64[ns] # to seconds In [77]: td . astype ( \"timedelta64[s]\" ) Out[77]: 0 31 days 00:00:00 1 31 days 00:00:00 2 31 days 00:05:03 3 NaT dtype: timedelta64[s] For timedelta64 resolutions other than the supported âsâ, âmsâ, âusâ, ânsâ, an alternative is to divide by another timedelta object. Note that division by the NumPy scalar is true division, while astyping is equivalent of floor division. # to days In [78]: td / np . timedelta64 ( 1 , \"D\" ) Out[78]: 0 31.000000 1 31.000000 2 31.003507 3 NaN dtype: float64 Dividing or multiplying a timedelta64[ns] Series by an integer or integer Series yields another timedelta64[ns] dtypes Series. In [79]: td * - 1 Out[79]: 0 -31 days +00:00:00 1 -31 days +00:00:00 2 -32 days +23:54:57 3 NaT dtype: timedelta64[ns] In [80]: td * pd . Series ([ 1 , 2 , 3 , 4 ]) Out[80]: 0 31 days 00:00:00 1 62 days 00:00:00 2 93 days 00:15:09 3 NaT dtype: timedelta64[ns] Rounded division (floor-division) of a timedelta64[ns] Series by a scalar Timedelta gives a series of integers. In [81]: td // pd . Timedelta ( days = 3 , hours = 4 ) Out[81]: 0 9.0 1 9.0 2 9.0 3 NaN dtype: float64 In [82]: pd . Timedelta ( days = 3 , hours = 4 ) // td Out[82]: 0 0.0 1 0.0 2 0.0 3 NaN dtype: float64 The mod (%) and divmod operations are defined for Timedelta when operating with another timedelta-like or with a numeric argument. In [83]: pd . Timedelta ( hours = 37 ) % datetime . timedelta ( hours = 2 ) Out[83]: Timedelta('0 days 01:00:00') # divmod against a timedelta-like returns a pair (int, Timedelta) In [84]: divmod ( datetime . timedelta ( hours = 2 ), pd . Timedelta ( minutes = 11 )) Out[84]: (10, Timedelta('0 days 00:10:00')) # divmod against a numeric returns a pair (Timedelta, Timedelta) In [85]: divmod ( pd . Timedelta ( hours = 25 ), 86400000000000 ) Out[85]: (Timedelta('0 days 00:00:00.000000001'), Timedelta('0 days 01:00:00')) Attributes # You can access various components of the Timedelta or TimedeltaIndex directly using the attributes days,seconds,microseconds,nanoseconds . These are identical to the values returned by datetime.timedelta , in that, for example, the .seconds attribute represents the number of seconds >= 0 and < 1 day. These are signed according to whether the Timedelta is signed. These operations can also be directly accessed via the .dt property of the Series as well. Note Note that the attributes are NOT the displayed values of the Timedelta . Use .components to retrieve the displayed values. For a Series : In [86]: td . dt . days Out[86]: 0 31.0 1 31.0 2 31.0 3 NaN dtype: float64 In [87]: td . dt . seconds Out[87]: 0 0.0 1 0.0 2 303.0 3 NaN dtype: float64 You can access the value of the fields for a scalar Timedelta directly. In [88]: tds = pd . Timedelta ( \"31 days 5 min 3 sec\" ) In [89]: tds . days Out[89]: 31 In [90]: tds . seconds Out[90]: 303 In [91]: ( - tds ) . seconds Out[91]: 86097 You can use the .components property to access a reduced form of the timedelta. This returns a DataFrame indexed similarly to the Series . These are the displayed values of the Timedelta . In [92]: td . dt . components Out[92]: days hours minutes seconds milliseconds microseconds nanoseconds 0 31.0 0.0 0.0 0.0 0.0 0.0 0.0 1 31.0 0.0 0.0 0.0 0.0 0.0 0.0 2 31.0 0.0 5.0 3.0 0.0 0.0 0.0 3 NaN NaN NaN NaN NaN NaN NaN In [93]: td . dt . components . seconds Out[93]: 0 0.0 1 0.0 2 3.0 3 NaN Name: seconds, dtype: float64 You can convert a Timedelta to an ISO 8601 Duration string with the .isoformat method In [94]: pd . Timedelta ( ....: days = 6 , minutes = 50 , seconds = 3 , milliseconds = 10 , microseconds = 10 , nanoseconds = 12 ....: ) . isoformat () ....: Out[94]: 'P6DT0H50M3.010010012S' TimedeltaIndex # To generate an index with time delta, you can use either the TimedeltaIndex or the timedelta_range() constructor. Using TimedeltaIndex you can pass string-like, Timedelta , timedelta , or np.timedelta64 objects. Passing np.nan/pd.NaT/nat will represent missing values. In [95]: pd . TimedeltaIndex ( ....: [ ....: \"1 days\" , ....: \"1 days, 00:00:05\" , ....: np . timedelta64 ( 2 , \"D\" ), ....: datetime . timedelta ( days = 2 , seconds = 2 ), ....: ] ....: ) ....: Out[95]: TimedeltaIndex(['1 days 00:00:00', '1 days 00:00:05', '2 days 00:00:00', '2 days 00:00:02'], dtype='timedelta64[ns]', freq=None) The string âinferâ can be passed in order to set the frequency of the index as the inferred frequency upon creation: In [96]: pd . TimedeltaIndex ([ \"0 days\" , \"10 days\" , \"20 days\" ], freq = \"infer\" ) Out[96]: TimedeltaIndex(['0 days', '10 days', '20 days'], dtype='timedelta64[ns]', freq='10D') Generating ranges of time deltas # Similar to date_range() , you can construct regular ranges of a TimedeltaIndex using timedelta_range() . The default frequency for timedelta_range is calendar day: In [97]: pd . timedelta_range ( start = \"1 days\" , periods = 5 ) Out[97]: TimedeltaIndex(['1 days', '2 days', '3 days', '4 days', '5 days'], dtype='timedelta64[ns]', freq='D') Various combinations of start , end , and periods can be used with timedelta_range : In [98]: pd . timedelta_range ( start = \"1 days\" , end = \"5 days\" ) Out[98]: TimedeltaIndex(['1 days', '2 days', '3 days', '4 days', '5 days'], dtype='timedelta64[ns]', freq='D') In [99]: pd . timedelta_range ( end = \"10 days\" , periods = 4 ) Out[99]: TimedeltaIndex(['7 days', '8 days', '9 days', '10 days'], dtype='timedelta64[ns]', freq='D') The freq parameter can passed a variety of frequency aliases : In [100]: pd . timedelta_range ( start = \"1 days\" , end = \"2 days\" , freq = \"30min\" ) Out[100]: TimedeltaIndex(['1 days 00:00:00', '1 days 00:30:00', '1 days 01:00:00', '1 days 01:30:00', '1 days 02:00:00', '1 days 02:30:00', '1 days 03:00:00', '1 days 03:30:00', '1 days 04:00:00', '1 days 04:30:00', '1 days 05:00:00', '1 days 05:30:00', '1 days 06:00:00', '1 days 06:30:00', '1 days 07:00:00', '1 days 07:30:00', '1 days 08:00:00', '1 days 08:30:00', '1 days 09:00:00', '1 days 09:30:00', '1 days 10:00:00', '1 days 10:30:00', '1 days 11:00:00', '1 days 11:30:00', '1 days 12:00:00', '1 days 12:30:00', '1 days 13:00:00', '1 days 13:30:00', '1 days 14:00:00', '1 days 14:30:00', '1 days 15:00:00', '1 days 15:30:00', '1 days 16:00:00', '1 days 16:30:00', '1 days 17:00:00', '1 days 17:30:00', '1 days 18:00:00', '1 days 18:30:00', '1 days 19:00:00', '1 days 19:30:00', '1 days 20:00:00', '1 days 20:30:00', '1 days 21:00:00', '1 days 21:30:00', '1 days 22:00:00', '1 days 22:30:00', '1 days 23:00:00', '1 days 23:30:00', '2 days 00:00:00'], dtype='timedelta64[ns]', freq='30min') In [101]: pd . timedelta_range ( start = \"1 days\" , periods = 5 , freq = \"2D5h\" ) Out[101]: TimedeltaIndex(['1 days 00:00:00', '3 days 05:00:00', '5 days 10:00:00', '7 days 15:00:00', '9 days 20:00:00'], dtype='timedelta64[ns]', freq='53h') Specifying start , end , and periods will generate a range of evenly spaced timedeltas from start to end inclusively, with periods number of elements in the resulting TimedeltaIndex : In [102]: pd . timedelta_range ( \"0 days\" , \"4 days\" , periods = 5 ) Out[102]: TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'], dtype='timedelta64[ns]', freq=None) In [103]: pd . timedelta_range ( \"0 days\" , \"4 days\" , periods = 10 ) Out[103]: TimedeltaIndex(['0 days 00:00:00', '0 days 10:40:00', '0 days 21:20:00', '1 days 08:00:00', '1 days 18:40:00', '2 days 05:20:00', '2 days 16:00:00', '3 days 02:40:00', '3 days 13:20:00', '4 days 00:00:00'], dtype='timedelta64[ns]', freq=None) Using the TimedeltaIndex # Similarly to other of the datetime-like indices, DatetimeIndex and PeriodIndex , you can use TimedeltaIndex as the index of pandas objects. In [104]: s = pd . Series ( .....: np . arange ( 100 ), .....: index = pd . timedelta_range ( \"1 days\" , periods = 100 , freq = \"h\" ), .....: ) .....: In [105]: s Out[105]: 1 days 00:00:00 0 1 days 01:00:00 1 1 days 02:00:00 2 1 days 03:00:00 3 1 days 04:00:00 4 .. 4 days 23:00:00 95 5 days 00:00:00 96 5 days 01:00:00 97 5 days 02:00:00 98 5 days 03:00:00 99 Freq: h, Length: 100, dtype: int64 Selections work similarly, with coercion on string-likes and slices: In [106]: s [ \"1 day\" : \"2 day\" ] Out[106]: 1 days 00:00:00 0 1 days 01:00:00 1 1 days 02:00:00 2 1 days 03:00:00 3 1 days 04:00:00 4 .. 2 days 19:00:00 43 2 days 20:00:00 44 2 days 21:00:00 45 2 days 22:00:00 46 2 days 23:00:00 47 Freq: h, Length: 48, dtype: int64 In [107]: s [ \"1 day 01:00:00\" ] Out[107]: 1 In [108]: s [ pd . Timedelta ( \"1 day 1h\" )] Out[108]: 1 Furthermore you can use partial string selection and the range will be inferred: In [109]: s [ \"1 day\" : \"1 day 5 hours\" ] Out[109]: 1 days 00:00:00 0 1 days 01:00:00 1 1 days 02:00:00 2 1 days 03:00:00 3 1 days 04:00:00 4 1 days 05:00:00 5 Freq: h, dtype: int64 Operations # Finally, the combination of TimedeltaIndex with DatetimeIndex allow certain combination operations that are NaT preserving: In [110]: tdi = pd . TimedeltaIndex ([ \"1 days\" , pd . NaT , \"2 days\" ]) In [111]: tdi . to_list () Out[111]: [Timedelta('1 days 00:00:00'), NaT, Timedelta('2 days 00:00:00')] In [112]: dti = pd . date_range ( \"20130101\" , periods = 3 ) In [113]: dti . to_list () Out[113]: [Timestamp('2013-01-01 00:00:00'), Timestamp('2013-01-02 00:00:00'), Timestamp('2013-01-03 00:00:00')] In [114]: ( dti + tdi ) . to_list () Out[114]: [Timestamp('2013-01-02 00:00:00'), NaT, Timestamp('2013-01-05 00:00:00')] In [115]: ( dti - tdi ) . to_list () Out[115]: [Timestamp('2012-12-31 00:00:00'), NaT, Timestamp('2013-01-01 00:00:00')] Conversions # Similarly to frequency conversion on a Series above, you can convert these indices to yield another Index. In [116]: tdi / np . timedelta64 ( 1 , \"s\" ) Out[116]: Index([86400.0, nan, 172800.0], dtype='float64') In [117]: tdi . astype ( \"timedelta64[s]\" ) Out[117]: TimedeltaIndex(['1 days', NaT, '2 days'], dtype='timedelta64[s]', freq=None) Scalars type ops work as well. These can potentially return a different type of index. # adding or timedelta and date -> datelike In [118]: tdi + pd . Timestamp ( \"20130101\" ) Out[118]: DatetimeIndex(['2013-01-02', 'NaT', '2013-01-03'], dtype='datetime64[ns]', freq=None) # subtraction of a date and a timedelta -> datelike # note that trying to subtract a date from a Timedelta will raise an exception In [119]: ( pd . Timestamp ( \"20130101\" ) - tdi ) . to_list () Out[119]: [Timestamp('2012-12-31 00:00:00'), NaT, Timestamp('2012-12-30 00:00:00')] # timedelta + timedelta -> timedelta In [120]: tdi + pd . Timedelta ( \"10 days\" ) Out[120]: TimedeltaIndex(['11 days', NaT, '12 days'], dtype='timedelta64[ns]', freq=None) # division can result in a Timedelta if the divisor is an integer In [121]: tdi / 2 Out[121]: TimedeltaIndex(['0 days 12:00:00', NaT, '1 days 00:00:00'], dtype='timedelta64[ns]', freq=None) # or a float64 Index if the divisor is a Timedelta In [122]: tdi / tdi [ 0 ] Out[122]: Index([1.0, nan, 2.0], dtype='float64') Resampling # Similar to timeseries resampling , we can resample with a TimedeltaIndex . In [123]: s . resample ( \"D\" ) . mean () Out[123]: 1 days 11.5 2 days 35.5 3 days 59.5 4 days 83.5 5 days 97.5 Freq: D, dtype: float64 previous Time series / date functionality next Options and settings On this page Parsing to_timedelta Timedelta limitations Operations Reductions Frequency conversion Attributes TimedeltaIndex Generating ranges of time deltas Using the TimedeltaIndex Operations Conversions Resampling Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/timedeltas.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Reshaping... Reshaping and pivot tables # pandas provides methods for manipulating a Series and DataFrame to alter the representation of the data for further data processing or data summarization. pivot() and pivot_table() : Group unique values within one or more discrete categories. stack() and unstack() : Pivot a column or row level to the opposite axis respectively. melt() and wide_to_long() : Unpivot a wide DataFrame to a long format. get_dummies() and from_dummies() : Conversions with indicator variables. explode() : Convert a column of list-like values to individual rows. crosstab() : Calculate a cross-tabulation of multiple 1 dimensional factor arrays. cut() : Transform continuous variables to discrete, categorical values factorize() : Encode 1 dimensional variables into integer labels. pivot() and pivot_table() # pivot() # Data is often stored in so-called âstackedâ or ârecordâ format. In a ârecordâ or âwideâ format, typically there is one row for each subject. In the âstackedâ or âlongâ format there are multiple rows for each subject where applicable. In [1]: data = { ...: \"value\" : range ( 12 ), ...: \"variable\" : [ \"A\" ] * 3 + [ \"B\" ] * 3 + [ \"C\" ] * 3 + [ \"D\" ] * 3 , ...: \"date\" : pd . to_datetime ([ \"2020-01-03\" , \"2020-01-04\" , \"2020-01-05\" ] * 4 ) ...: } ...: In [2]: df = pd . DataFrame ( data ) To perform time series operations with each unique variable, a better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot() ): In [3]: pivoted = df . pivot ( index = \"date\" , columns = \"variable\" , values = \"value\" ) In [4]: pivoted Out[4]: variable A B C D date 2020-01-03 0 3 6 9 2020-01-04 1 4 7 10 2020-01-05 2 5 8 11 If the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot() , then the resulting âpivotedâ DataFrame will have hierarchical columns whose topmost level indicates the respective value column: In [5]: df [ \"value2\" ] = df [ \"value\" ] * 2 In [6]: pivoted = df . pivot ( index = \"date\" , columns = \"variable\" ) In [7]: pivoted Out[7]: value value2 variable A B C D A B C D date 2020-01-03 0 3 6 9 0 6 12 18 2020-01-04 1 4 7 10 2 8 14 20 2020-01-05 2 5 8 11 4 10 16 22 You can then select subsets from the pivoted DataFrame : In [8]: pivoted [ \"value2\" ] Out[8]: variable A B C D date 2020-01-03 0 6 12 18 2020-01-04 2 8 14 20 2020-01-05 4 10 16 22 Note that this returns a view on the underlying data in the case where the data are homogeneously-typed. Note pivot() can only handle unique rows specified by index and columns . If you data contains duplicates, use pivot_table() . pivot_table() # While pivot() provides general purpose pivoting with various data types, pandas also provides pivot_table() or pivot_table() for pivoting with aggregation of numeric data. The function pivot_table() can be used to create spreadsheet-style pivot tables. See the cookbook for some advanced strategies. In [9]: import datetime In [10]: df = pd . DataFrame ( ....: { ....: \"A\" : [ \"one\" , \"one\" , \"two\" , \"three\" ] * 6 , ....: \"B\" : [ \"A\" , \"B\" , \"C\" ] * 8 , ....: \"C\" : [ \"foo\" , \"foo\" , \"foo\" , \"bar\" , \"bar\" , \"bar\" ] * 4 , ....: \"D\" : np . random . randn ( 24 ), ....: \"E\" : np . random . randn ( 24 ), ....: \"F\" : [ datetime . datetime ( 2013 , i , 1 ) for i in range ( 1 , 13 )] ....: + [ datetime . datetime ( 2013 , i , 15 ) for i in range ( 1 , 13 )], ....: } ....: ) ....: In [11]: df Out[11]: A B C D E F 0 one A foo 0.469112 0.404705 2013-01-01 1 one B foo -0.282863 0.577046 2013-02-01 2 two C foo -1.509059 -1.715002 2013-03-01 3 three A bar -1.135632 -1.039268 2013-04-01 4 one B bar 1.212112 -0.370647 2013-05-01 .. ... .. ... ... ... ... 19 three B foo -1.087401 -0.472035 2013-08-15 20 one C foo -0.673690 -0.013960 2013-09-15 21 one A bar 0.113648 -0.362543 2013-10-15 22 two B bar -1.478427 -0.006154 2013-11-15 23 three C bar 0.524988 -0.923061 2013-12-15 [24 rows x 6 columns] In [12]: pd . pivot_table ( df , values = \"D\" , index = [ \"A\" , \"B\" ], columns = [ \"C\" ]) Out[12]: C bar foo A B one A -0.995460 0.595334 B 0.393570 -0.494817 C 0.196903 -0.767769 three A -0.431886 NaN B NaN -1.065818 C 0.798396 NaN two A NaN 0.197720 B -0.986678 NaN C NaN -1.274317 In [13]: pd . pivot_table ( ....: df , values = [ \"D\" , \"E\" ], ....: index = [ \"B\" ], ....: columns = [ \"A\" , \"C\" ], ....: aggfunc = \"sum\" , ....: ) ....: Out[13]: D ... E A one three ... three two C bar foo bar ... foo bar foo B ... A -1.990921 1.190667 -0.863772 ... NaN NaN -1.067650 B 0.787140 -0.989634 NaN ... 0.372851 1.63741 NaN C 0.393806 -1.535539 1.596791 ... NaN NaN -3.491906 [3 rows x 12 columns] In [14]: pd . pivot_table ( ....: df , values = \"E\" , ....: index = [ \"B\" , \"C\" ], ....: columns = [ \"A\" ], ....: aggfunc = [ \"sum\" , \"mean\" ], ....: ) ....: Out[14]: sum mean A one three two one three two B C A bar -0.471593 -2.008182 NaN -0.235796 -1.004091 NaN foo 0.761726 NaN -1.067650 0.380863 NaN -0.533825 B bar -1.665170 NaN 1.637410 -0.832585 NaN 0.818705 foo -0.097554 0.372851 NaN -0.048777 0.186425 NaN C bar -0.744154 -2.392449 NaN -0.372077 -1.196224 NaN foo 1.061810 NaN -3.491906 0.530905 NaN -1.745953 The result is a DataFrame potentially having a MultiIndex on the index or column. If the values column name is not given, the pivot table will include all of the data in an additional level of hierarchy in the columns: In [15]: pd . pivot_table ( df [[ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" ]], index = [ \"A\" , \"B\" ], columns = [ \"C\" ]) Out[15]: D E C bar foo bar foo A B one A -0.995460 0.595334 -0.235796 0.380863 B 0.393570 -0.494817 -0.832585 -0.048777 C 0.196903 -0.767769 -0.372077 0.530905 three A -0.431886 NaN -1.004091 NaN B NaN -1.065818 NaN 0.186425 C 0.798396 NaN -1.196224 NaN two A NaN 0.197720 NaN -0.533825 B -0.986678 NaN 0.818705 NaN C NaN -1.274317 NaN -1.745953 Also, you can use Grouper for index and columns keywords. For detail of Grouper , see Grouping with a Grouper specification . In [16]: pd . pivot_table ( df , values = \"D\" , index = pd . Grouper ( freq = \"ME\" , key = \"F\" ), columns = \"C\" ) Out[16]: C bar foo F 2013-01-31 NaN 0.595334 2013-02-28 NaN -0.494817 2013-03-31 NaN -1.274317 2013-04-30 -0.431886 NaN 2013-05-31 0.393570 NaN 2013-06-30 0.196903 NaN 2013-07-31 NaN 0.197720 2013-08-31 NaN -1.065818 2013-09-30 NaN -0.767769 2013-10-31 -0.995460 NaN 2013-11-30 -0.986678 NaN 2013-12-31 0.798396 NaN Adding margins # Passing margins=True to pivot_table() will add a row and column with an All label with partial group aggregates across the categories on the rows and columns: In [17]: table = df . pivot_table ( ....: index = [ \"A\" , \"B\" ], ....: columns = \"C\" , ....: values = [ \"D\" , \"E\" ], ....: margins = True , ....: aggfunc = \"std\" ....: ) ....: In [18]: table Out[18]: D E C bar foo All bar foo All A B one A 1.568517 0.178504 1.293926 0.179247 0.033718 0.371275 B 1.157593 0.299748 0.860059 0.653280 0.885047 0.779837 C 0.523425 0.133049 0.638297 1.111310 0.770555 0.938819 three A 0.995247 NaN 0.995247 0.049748 NaN 0.049748 B NaN 0.030522 0.030522 NaN 0.931203 0.931203 C 0.386657 NaN 0.386657 0.386312 NaN 0.386312 two A NaN 0.111032 0.111032 NaN 1.146201 1.146201 B 0.695438 NaN 0.695438 1.166526 NaN 1.166526 C NaN 0.331975 0.331975 NaN 0.043771 0.043771 All 1.014073 0.713941 0.871016 0.881376 0.984017 0.923568 Additionally, you can call DataFrame.stack() to display a pivoted DataFrame as having a multi-level index: In [19]: table . stack ( future_stack = True ) Out[19]: D E A B C one A bar 1.568517 0.179247 foo 0.178504 0.033718 All 1.293926 0.371275 B bar 1.157593 0.653280 foo 0.299748 0.885047 ... ... ... two C foo 0.331975 0.043771 All 0.331975 0.043771 All bar 1.014073 0.881376 foo 0.713941 0.984017 All 0.871016 0.923568 [30 rows x 2 columns] stack() and unstack() # Closely related to the pivot() method are the related stack() and unstack() methods available on Series and DataFrame . These methods are designed to work together with MultiIndex objects (see the section on hierarchical indexing ). stack() : âpivotâ a level of the (possibly hierarchical) column labels, returning a DataFrame with an index with a new inner-most level of row labels. unstack() : (inverse operation of stack() ) âpivotâ a level of the (possibly hierarchical) row index to the column axis, producing a reshaped DataFrame with a new inner-most level of column labels. In [20]: tuples = [ ....: [ \"bar\" , \"bar\" , \"baz\" , \"baz\" , \"foo\" , \"foo\" , \"qux\" , \"qux\" ], ....: [ \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" ], ....: ] ....: In [21]: index = pd . MultiIndex . from_arrays ( tuples , names = [ \"first\" , \"second\" ]) In [22]: df = pd . DataFrame ( np . random . randn ( 8 , 2 ), index = index , columns = [ \"A\" , \"B\" ]) In [23]: df2 = df [: 4 ] In [24]: df2 Out[24]: A B first second bar one 0.895717 0.805244 two -1.206412 2.565646 baz one 1.431256 1.340309 two -1.170299 -0.226169 The stack() function âcompressesâ a level in the DataFrame columns to produce either: A Series , in the case of a Index in the columns. A DataFrame , in the case of a MultiIndex in the columns. If the columns have a MultiIndex , you can choose which level to stack. The stacked level becomes the new lowest level in a MultiIndex on the columns: In [25]: stacked = df2 . stack ( future_stack = True ) In [26]: stacked Out[26]: first second bar one A 0.895717 B 0.805244 two A -1.206412 B 2.565646 baz one A 1.431256 B 1.340309 two A -1.170299 B -0.226169 dtype: float64 With a âstackedâ DataFrame or Series (having a MultiIndex as the index ), the inverse operation of stack() is unstack() , which by default unstacks the last level : In [27]: stacked . unstack () Out[27]: A B first second bar one 0.895717 0.805244 two -1.206412 2.565646 baz one 1.431256 1.340309 two -1.170299 -0.226169 In [28]: stacked . unstack ( 1 ) Out[28]: second one two first bar A 0.895717 -1.206412 B 0.805244 2.565646 baz A 1.431256 -1.170299 B 1.340309 -0.226169 In [29]: stacked . unstack ( 0 ) Out[29]: first bar baz second one A 0.895717 1.431256 B 0.805244 1.340309 two A -1.206412 -1.170299 B 2.565646 -0.226169 If the indexes have names, you can use the level names instead of specifying the level numbers: In [30]: stacked . unstack ( \"second\" ) Out[30]: second one two first bar A 0.895717 -1.206412 B 0.805244 2.565646 baz A 1.431256 -1.170299 B 1.340309 -0.226169 Notice that the stack() and unstack() methods implicitly sort the index levels involved. Hence a call to stack() and then unstack() , or vice versa, will result in a sorted copy of the original DataFrame or Series : In [31]: index = pd . MultiIndex . from_product ([[ 2 , 1 ], [ \"a\" , \"b\" ]]) In [32]: df = pd . DataFrame ( np . random . randn ( 4 ), index = index , columns = [ \"A\" ]) In [33]: df Out[33]: A 2 a -1.413681 b 1.607920 1 a 1.024180 b 0.569605 In [34]: all ( df . unstack () . stack ( future_stack = True ) == df . sort_index ()) Out[34]: True Multiple levels # You may also stack or unstack more than one level at a time by passing a list of levels, in which case the end result is as if each level in the list were processed individually. In [35]: columns = pd . MultiIndex . from_tuples ( ....: [ ....: ( \"A\" , \"cat\" , \"long\" ), ....: ( \"B\" , \"cat\" , \"long\" ), ....: ( \"A\" , \"dog\" , \"short\" ), ....: ( \"B\" , \"dog\" , \"short\" ), ....: ], ....: names = [ \"exp\" , \"animal\" , \"hair_length\" ], ....: ) ....: In [36]: df = pd . DataFrame ( np . random . randn ( 4 , 4 ), columns = columns ) In [37]: df Out[37]: exp A B A B animal cat cat dog dog hair_length long long short short 0 0.875906 -2.211372 0.974466 -2.006747 1 -0.410001 -0.078638 0.545952 -1.219217 2 -1.226825 0.769804 -1.281247 -0.727707 3 -0.121306 -0.097883 0.695775 0.341734 In [38]: df . stack ( level = [ \"animal\" , \"hair_length\" ], future_stack = True ) Out[38]: exp A B animal hair_length 0 cat long 0.875906 -2.211372 dog short 0.974466 -2.006747 1 cat long -0.410001 -0.078638 dog short 0.545952 -1.219217 2 cat long -1.226825 0.769804 dog short -1.281247 -0.727707 3 cat long -0.121306 -0.097883 dog short 0.695775 0.341734 The list of levels can contain either level names or level numbers but not a mixture of the two. # df.stack(level=['animal', 'hair_length'], future_stack=True) # from above is equivalent to: In [39]: df . stack ( level = [ 1 , 2 ], future_stack = True ) Out[39]: exp A B animal hair_length 0 cat long 0.875906 -2.211372 dog short 0.974466 -2.006747 1 cat long -0.410001 -0.078638 dog short 0.545952 -1.219217 2 cat long -1.226825 0.769804 dog short -1.281247 -0.727707 3 cat long -0.121306 -0.097883 dog short 0.695775 0.341734 Missing data # Unstacking can result in missing values if subgroups do not have the same set of labels. By default, missing values will be replaced with the default fill value for that data type. In [40]: columns = pd . MultiIndex . from_tuples ( ....: [ ....: ( \"A\" , \"cat\" ), ....: ( \"B\" , \"dog\" ), ....: ( \"B\" , \"cat\" ), ....: ( \"A\" , \"dog\" ), ....: ], ....: names = [ \"exp\" , \"animal\" ], ....: ) ....: In [41]: index = pd . MultiIndex . from_product ( ....: [( \"bar\" , \"baz\" , \"foo\" , \"qux\" ), ( \"one\" , \"two\" )], names = [ \"first\" , \"second\" ] ....: ) ....: In [42]: df = pd . DataFrame ( np . random . randn ( 8 , 4 ), index = index , columns = columns ) In [43]: df3 = df . iloc [[ 0 , 1 , 4 , 7 ], [ 1 , 2 ]] In [44]: df3 Out[44]: exp B animal dog cat first second bar one -1.110336 -0.619976 two 0.687738 0.176444 foo one 1.314232 0.690579 qux two 0.380396 0.084844 In [45]: df3 . unstack () Out[45]: exp B animal dog cat second one two one two first bar -1.110336 0.687738 -0.619976 0.176444 foo 1.314232 NaN 0.690579 NaN qux NaN 0.380396 NaN 0.084844 The missing value can be filled with a specific value with the fill_value argument. In [46]: df3 . unstack ( fill_value =- 1e9 ) Out[46]: exp B animal dog cat second one two one two first bar -1.110336e+00 6.877384e-01 -6.199759e-01 1.764443e-01 foo 1.314232e+00 -1.000000e+09 6.905793e-01 -1.000000e+09 qux -1.000000e+09 3.803956e-01 -1.000000e+09 8.484421e-02 melt() and wide_to_long() # The top-level melt() function and the corresponding DataFrame.melt() are useful to massage a DataFrame into a format where one or more columns are identifier variables , while all other columns, considered measured variables , are âunpivotedâ to the row axis, leaving just two non-identifier columns, âvariableâ and âvalueâ. The names of those columns can be customized by supplying the var_name and value_name parameters. In [47]: cheese = pd . DataFrame ( ....: { ....: \"first\" : [ \"John\" , \"Mary\" ], ....: \"last\" : [ \"Doe\" , \"Bo\" ], ....: \"height\" : [ 5.5 , 6.0 ], ....: \"weight\" : [ 130 , 150 ], ....: } ....: ) ....: In [48]: cheese Out[48]: first last height weight 0 John Doe 5.5 130 1 Mary Bo 6.0 150 In [49]: cheese . melt ( id_vars = [ \"first\" , \"last\" ]) Out[49]: first last variable value 0 John Doe height 5.5 1 Mary Bo height 6.0 2 John Doe weight 130.0 3 Mary Bo weight 150.0 In [50]: cheese . melt ( id_vars = [ \"first\" , \"last\" ], var_name = \"quantity\" ) Out[50]: first last quantity value 0 John Doe height 5.5 1 Mary Bo height 6.0 2 John Doe weight 130.0 3 Mary Bo weight 150.0 When transforming a DataFrame using melt() , the index will be ignored. The original index values can be kept by setting the ignore_index=False parameter to False (default is True ). ignore_index=False will however duplicate index values. In [51]: index = pd . MultiIndex . from_tuples ([( \"person\" , \"A\" ), ( \"person\" , \"B\" )]) In [52]: cheese = pd . DataFrame ( ....: { ....: \"first\" : [ \"John\" , \"Mary\" ], ....: \"last\" : [ \"Doe\" , \"Bo\" ], ....: \"height\" : [ 5.5 , 6.0 ], ....: \"weight\" : [ 130 , 150 ], ....: }, ....: index = index , ....: ) ....: In [53]: cheese Out[53]: first last height weight person A John Doe 5.5 130 B Mary Bo 6.0 150 In [54]: cheese . melt ( id_vars = [ \"first\" , \"last\" ]) Out[54]: first last variable value 0 John Doe height 5.5 1 Mary Bo height 6.0 2 John Doe weight 130.0 3 Mary Bo weight 150.0 In [55]: cheese . melt ( id_vars = [ \"first\" , \"last\" ], ignore_index = False ) Out[55]: first last variable value person A John Doe height 5.5 B Mary Bo height 6.0 A John Doe weight 130.0 B Mary Bo weight 150.0 wide_to_long() is similar to melt() with more customization for column matching. In [56]: dft = pd . DataFrame ( ....: { ....: \"A1970\" : { 0 : \"a\" , 1 : \"b\" , 2 : \"c\" }, ....: \"A1980\" : { 0 : \"d\" , 1 : \"e\" , 2 : \"f\" }, ....: \"B1970\" : { 0 : 2.5 , 1 : 1.2 , 2 : 0.7 }, ....: \"B1980\" : { 0 : 3.2 , 1 : 1.3 , 2 : 0.1 }, ....: \"X\" : dict ( zip ( range ( 3 ), np . random . randn ( 3 ))), ....: } ....: ) ....: In [57]: dft [ \"id\" ] = dft . index In [58]: dft Out[58]: A1970 A1980 B1970 B1980 X id 0 a d 2.5 3.2 1.519970 0 1 b e 1.2 1.3 -0.493662 1 2 c f 0.7 0.1 0.600178 2 In [59]: pd . wide_to_long ( dft , [ \"A\" , \"B\" ], i = \"id\" , j = \"year\" ) Out[59]: X A B id year 0 1970 1.519970 a 2.5 1 1970 -0.493662 b 1.2 2 1970 0.600178 c 0.7 0 1980 1.519970 d 3.2 1 1980 -0.493662 e 1.3 2 1980 0.600178 f 0.1 get_dummies() and from_dummies() # To convert categorical variables of a Series into a âdummyâ or âindicatorâ, get_dummies() creates a new DataFrame with columns of the unique variables and the values representing the presence of those variables per row. In [60]: df = pd . DataFrame ({ \"key\" : list ( \"bbacab\" ), \"data1\" : range ( 6 )}) In [61]: pd . get_dummies ( df [ \"key\" ]) Out[61]: a b c 0 False True False 1 False True False 2 True False False 3 False False True 4 True False False 5 False True False In [62]: df [ \"key\" ] . str . get_dummies () Out[62]: a b c 0 0 1 0 1 0 1 0 2 1 0 0 3 0 0 1 4 1 0 0 5 0 1 0 prefix adds a prefix to the the column names which is useful for merging the result with the original DataFrame : In [63]: dummies = pd . get_dummies ( df [ \"key\" ], prefix = \"key\" ) In [64]: dummies Out[64]: key_a key_b key_c 0 False True False 1 False True False 2 True False False 3 False False True 4 True False False 5 False True False In [65]: df [[ \"data1\" ]] . join ( dummies ) Out[65]: data1 key_a key_b key_c 0 0 False True False 1 1 False True False 2 2 True False False 3 3 False False True 4 4 True False False 5 5 False True False This function is often used along with discretization functions like cut() : In [66]: values = np . random . randn ( 10 ) In [67]: values Out[67]: array([ 0.2742, 0.1329, -0.0237, 2.4102, 1.4505, 0.2061, -0.2519, -2.2136, 1.0633, 1.2661]) In [68]: bins = [ 0 , 0.2 , 0.4 , 0.6 , 0.8 , 1 ] In [69]: pd . get_dummies ( pd . cut ( values , bins )) Out[69]: (0.0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8] (0.8, 1.0] 0 False True False False False 1 True False False False False 2 False False False False False 3 False False False False False 4 False False False False False 5 False True False False False 6 False False False False False 7 False False False False False 8 False False False False False 9 False False False False False get_dummies() also accepts a DataFrame . By default, object , string , or categorical type columns are encoded as dummy variables with other columns unaltered. In [70]: df = pd . DataFrame ({ \"A\" : [ \"a\" , \"b\" , \"a\" ], \"B\" : [ \"c\" , \"c\" , \"b\" ], \"C\" : [ 1 , 2 , 3 ]}) In [71]: pd . get_dummies ( df ) Out[71]: C A_a A_b B_b B_c 0 1 True False False True 1 2 False True False True 2 3 True False True False Specifying the columns keyword will encode a column of any type. In [72]: pd . get_dummies ( df , columns = [ \"A\" ]) Out[72]: B C A_a A_b 0 c 1 True False 1 c 2 False True 2 b 3 True False As with the Series version, you can pass values for the prefix and prefix_sep . By default the column name is used as the prefix and _ as the prefix separator. You can specify prefix and prefix_sep in 3 ways: string: Use the same value for prefix or prefix_sep for each column to be encoded. list: Must be the same length as the number of columns being encoded. dict: Mapping column name to prefix. In [73]: simple = pd . get_dummies ( df , prefix = \"new_prefix\" ) In [74]: simple Out[74]: C new_prefix_a new_prefix_b new_prefix_b new_prefix_c 0 1 True False False True 1 2 False True False True 2 3 True False True False In [75]: from_list = pd . get_dummies ( df , prefix = [ \"from_A\" , \"from_B\" ]) In [76]: from_list Out[76]: C from_A_a from_A_b from_B_b from_B_c 0 1 True False False True 1 2 False True False True 2 3 True False True False In [77]: from_dict = pd . get_dummies ( df , prefix = { \"B\" : \"from_B\" , \"A\" : \"from_A\" }) In [78]: from_dict Out[78]: C from_A_a from_A_b from_B_b from_B_c 0 1 True False False True 1 2 False True False True 2 3 True False True False To avoid collinearity when feeding the result to statistical models, specify drop_first=True . In [79]: s = pd . Series ( list ( \"abcaa\" )) In [80]: pd . get_dummies ( s ) Out[80]: a b c 0 True False False 1 False True False 2 False False True 3 True False False 4 True False False In [81]: pd . get_dummies ( s , drop_first = True ) Out[81]: b c 0 False False 1 True False 2 False True 3 False False 4 False False When a column contains only one level, it will be omitted in the result. In [82]: df = pd . DataFrame ({ \"A\" : list ( \"aaaaa\" ), \"B\" : list ( \"ababc\" )}) In [83]: pd . get_dummies ( df ) Out[83]: A_a B_a B_b B_c 0 True True False False 1 True False True False 2 True True False False 3 True False True False 4 True False False True In [84]: pd . get_dummies ( df , drop_first = True ) Out[84]: B_b B_c 0 False False 1 True False 2 False False 3 True False 4 False True The values can be cast to a different type using the dtype argument. In [85]: df = pd . DataFrame ({ \"A\" : list ( \"abc\" ), \"B\" : [ 1.1 , 2.2 , 3.3 ]}) In [86]: pd . get_dummies ( df , dtype = np . float32 ) . dtypes Out[86]: B float64 A_a float32 A_b float32 A_c float32 dtype: object Added in version 1.5.0. from_dummies() converts the output of get_dummies() back into a Series of categorical values from indicator values. In [87]: df = pd . DataFrame ({ \"prefix_a\" : [ 0 , 1 , 0 ], \"prefix_b\" : [ 1 , 0 , 1 ]}) In [88]: df Out[88]: prefix_a prefix_b 0 0 1 1 1 0 2 0 1 In [89]: pd . from_dummies ( df , sep = \"_\" ) Out[89]: prefix 0 b 1 a 2 b Dummy coded data only requires k - 1 categories to be included, in this case the last category is the default category. The default category can be modified with default_category . In [90]: df = pd . DataFrame ({ \"prefix_a\" : [ 0 , 1 , 0 ]}) In [91]: df Out[91]: prefix_a 0 0 1 1 2 0 In [92]: pd . from_dummies ( df , sep = \"_\" , default_category = \"b\" ) Out[92]: prefix 0 b 1 a 2 b explode() # For a DataFrame column with nested, list-like values, explode() will transform each list-like value to a separate row. The resulting Index will be duplicated corresponding to the index label from the original row: In [93]: keys = [ \"panda1\" , \"panda2\" , \"panda3\" ] In [94]: values = [[ \"eats\" , \"shoots\" ], [ \"shoots\" , \"leaves\" ], [ \"eats\" , \"leaves\" ]] In [95]: df = pd . DataFrame ({ \"keys\" : keys , \"values\" : values }) In [96]: df Out[96]: keys values 0 panda1 [eats, shoots] 1 panda2 [shoots, leaves] 2 panda3 [eats, leaves] In [97]: df [ \"values\" ] . explode () Out[97]: 0 eats 0 shoots 1 shoots 1 leaves 2 eats 2 leaves Name: values, dtype: object DataFrame.explode can also explode the column in the DataFrame . In [98]: df . explode ( \"values\" ) Out[98]: keys values 0 panda1 eats 0 panda1 shoots 1 panda2 shoots 1 panda2 leaves 2 panda3 eats 2 panda3 leaves Series.explode() will replace empty lists with a missing value indicator and preserve scalar entries. In [99]: s = pd . Series ([[ 1 , 2 , 3 ], \"foo\" , [], [ \"a\" , \"b\" ]]) In [100]: s Out[100]: 0 [1, 2, 3] 1 foo 2 [] 3 [a, b] dtype: object In [101]: s . explode () Out[101]: 0 1 0 2 0 3 1 foo 2 NaN 3 a 3 b dtype: object A comma-separated string value can be split into individual values in a list and then exploded to a new row. In [102]: df = pd . DataFrame ([{ \"var1\" : \"a,b,c\" , \"var2\" : 1 }, { \"var1\" : \"d,e,f\" , \"var2\" : 2 }]) In [103]: df . assign ( var1 = df . var1 . str . split ( \",\" )) . explode ( \"var1\" ) Out[103]: var1 var2 0 a 1 0 b 1 0 c 1 1 d 2 1 e 2 1 f 2 crosstab() # Use crosstab() to compute a cross-tabulation of two (or more) factors. By default crosstab() computes a frequency table of the factors unless an array of values and an aggregation function are passed. Any Series passed will have their name attributes used unless row or column names for the cross-tabulation are specified In [104]: a = np . array ([ \"foo\" , \"foo\" , \"bar\" , \"bar\" , \"foo\" , \"foo\" ], dtype = object ) In [105]: b = np . array ([ \"one\" , \"one\" , \"two\" , \"one\" , \"two\" , \"one\" ], dtype = object ) In [106]: c = np . array ([ \"dull\" , \"dull\" , \"shiny\" , \"dull\" , \"dull\" , \"shiny\" ], dtype = object ) In [107]: pd . crosstab ( a , [ b , c ], rownames = [ \"a\" ], colnames = [ \"b\" , \"c\" ]) Out[107]: b one two c dull shiny dull shiny a bar 1 0 0 1 foo 2 1 1 0 If crosstab() receives only two Series , it will provide a frequency table. In [108]: df = pd . DataFrame ( .....: { \"A\" : [ 1 , 2 , 2 , 2 , 2 ], \"B\" : [ 3 , 3 , 4 , 4 , 4 ], \"C\" : [ 1 , 1 , np . nan , 1 , 1 ]} .....: ) .....: In [109]: df Out[109]: A B C 0 1 3 1.0 1 2 3 1.0 2 2 4 NaN 3 2 4 1.0 4 2 4 1.0 In [110]: pd . crosstab ( df [ \"A\" ], df [ \"B\" ]) Out[110]: B 3 4 A 1 1 0 2 1 3 crosstab() can also summarize to Categorical data. In [111]: foo = pd . Categorical ([ \"a\" , \"b\" ], categories = [ \"a\" , \"b\" , \"c\" ]) In [112]: bar = pd . Categorical ([ \"d\" , \"e\" ], categories = [ \"d\" , \"e\" , \"f\" ]) In [113]: pd . crosstab ( foo , bar ) Out[113]: col_0 d e row_0 a 1 0 b 0 1 For Categorical data, to include all of data categories even if the actual data does not contain any instances of a particular category, use dropna=False . In [114]: pd . crosstab ( foo , bar , dropna = False ) Out[114]: col_0 d e f row_0 a 1 0 0 b 0 1 0 c 0 0 0 Normalization # Frequency tables can also be normalized to show percentages rather than counts using the normalize argument: In [115]: pd . crosstab ( df [ \"A\" ], df [ \"B\" ], normalize = True ) Out[115]: B 3 4 A 1 0.2 0.0 2 0.2 0.6 normalize can also normalize values within each row or within each column: In [116]: pd . crosstab ( df [ \"A\" ], df [ \"B\" ], normalize = \"columns\" ) Out[116]: B 3 4 A 1 0.5 0.0 2 0.5 1.0 crosstab() can also accept a third Series and an aggregation function ( aggfunc ) that will be applied to the values of the third Series within each group defined by the first two Series : In [117]: pd . crosstab ( df [ \"A\" ], df [ \"B\" ], values = df [ \"C\" ], aggfunc = \"sum\" ) Out[117]: B 3 4 A 1 1.0 NaN 2 1.0 2.0 Adding margins # margins=True will add a row and column with an All label with partial group aggregates across the categories on the rows and columns: In [118]: pd . crosstab ( .....: df [ \"A\" ], df [ \"B\" ], values = df [ \"C\" ], aggfunc = \"sum\" , normalize = True , margins = True .....: ) .....: Out[118]: B 3 4 All A 1 0.25 0.0 0.25 2 0.25 0.5 0.75 All 0.50 0.5 1.00 cut() # The cut() function computes groupings for the values of the input array and is often used to transform continuous variables to discrete or categorical variables: An integer bins will form equal-width bins. In [119]: ages = np . array ([ 10 , 15 , 13 , 12 , 23 , 25 , 28 , 59 , 60 ]) In [120]: pd . cut ( ages , bins = 3 ) Out[120]: [(9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (26.667, 43.333], (43.333, 60.0], (43.333, 60.0]] Categories (3, interval[float64, right]): [(9.95, 26.667] < (26.667, 43.333] < (43.333, 60.0]] A list of ordered bin edges will assign an interval for each variable. In [121]: pd . cut ( ages , bins = [ 0 , 18 , 35 , 70 ]) Out[121]: [(0, 18], (0, 18], (0, 18], (0, 18], (18, 35], (18, 35], (18, 35], (35, 70], (35, 70]] Categories (3, interval[int64, right]): [(0, 18] < (18, 35] < (35, 70]] If the bins keyword is an IntervalIndex , then these will be used to bin the passed data. In [122]: pd . cut ( ages , bins = pd . IntervalIndex . from_breaks ([ 0 , 40 , 70 ])) Out[122]: [(0, 40], (0, 40], (0, 40], (0, 40], (0, 40], (0, 40], (0, 40], (40, 70], (40, 70]] Categories (2, interval[int64, right]): [(0, 40] < (40, 70]] factorize() # factorize() encodes 1 dimensional values into integer labels. Missing values are encoded as -1 . In [123]: x = pd . Series ([ \"A\" , \"A\" , np . nan , \"B\" , 3.14 , np . inf ]) In [124]: x Out[124]: 0 A 1 A 2 NaN 3 B 4 3.14 5 inf dtype: object In [125]: labels , uniques = pd . factorize ( x ) In [126]: labels Out[126]: array([ 0, 0, -1, 1, 2, 3]) In [127]: uniques Out[127]: Index(['A', 'B', 3.14, inf], dtype='object') Categorical will similarly encode 1 dimensional values for further categorical operations In [128]: pd . Categorical ( x ) Out[128]: ['A', 'A', NaN, 'B', 3.14, inf] Categories (4, object): [3.14, inf, 'A', 'B'] previous Merge, join, concatenate and compare next Working with text data On this page pivot() and pivot_table() pivot() pivot_table() Adding margins stack() and unstack() Multiple levels Missing data melt() and wide_to_long() get_dummies() and from_dummies() explode() crosstab() Normalization Adding margins cut() factorize() Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/reshaping.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Window Window # pandas.api.typing.Rolling instances are returned by .rolling calls: pandas.DataFrame.rolling() and pandas.Series.rolling() . pandas.api.typing.Expanding instances are returned by .expanding calls: pandas.DataFrame.expanding() and pandas.Series.expanding() . pandas.api.typing.ExponentialMovingWindow instances are returned by .ewm calls: pandas.DataFrame.ewm() and pandas.Series.ewm() . Rolling window functions # Rolling.count ([numeric_only]) Calculate the rolling count of non NaN observations. Rolling.sum ([numeric_only,Â engine,Â ...]) Calculate the rolling sum. Rolling.mean ([numeric_only,Â engine,Â ...]) Calculate the rolling mean. Rolling.median ([numeric_only,Â engine,Â ...]) Calculate the rolling median. Rolling.var ([ddof,Â numeric_only,Â engine,Â ...]) Calculate the rolling variance. Rolling.std ([ddof,Â numeric_only,Â engine,Â ...]) Calculate the rolling standard deviation. Rolling.min ([numeric_only,Â engine,Â ...]) Calculate the rolling minimum. Rolling.max ([numeric_only,Â engine,Â ...]) Calculate the rolling maximum. Rolling.corr ([other,Â pairwise,Â ddof,Â ...]) Calculate the rolling correlation. Rolling.cov ([other,Â pairwise,Â ddof,Â ...]) Calculate the rolling sample covariance. Rolling.skew ([numeric_only]) Calculate the rolling unbiased skewness. Rolling.kurt ([numeric_only]) Calculate the rolling Fisher's definition of kurtosis without bias. Rolling.apply (func[,Â raw,Â engine,Â ...]) Calculate the rolling custom aggregation function. Rolling.aggregate (func,Â *args,Â **kwargs) Aggregate using one or more operations over the specified axis. Rolling.quantile (q[,Â interpolation,Â ...]) Calculate the rolling quantile. Rolling.sem ([ddof,Â numeric_only]) Calculate the rolling standard error of mean. Rolling.rank ([method,Â ascending,Â pct,Â ...]) Calculate the rolling rank. Weighted window functions # Window.mean ([numeric_only]) Calculate the rolling weighted window mean. Window.sum ([numeric_only]) Calculate the rolling weighted window sum. Window.var ([ddof,Â numeric_only]) Calculate the rolling weighted window variance. Window.std ([ddof,Â numeric_only]) Calculate the rolling weighted window standard deviation. Expanding window functions # Expanding.count ([numeric_only]) Calculate the expanding count of non NaN observations. Expanding.sum ([numeric_only,Â engine,Â ...]) Calculate the expanding sum. Expanding.mean ([numeric_only,Â engine,Â ...]) Calculate the expanding mean. Expanding.median ([numeric_only,Â engine,Â ...]) Calculate the expanding median. Expanding.var ([ddof,Â numeric_only,Â engine,Â ...]) Calculate the expanding variance. Expanding.std ([ddof,Â numeric_only,Â engine,Â ...]) Calculate the expanding standard deviation. Expanding.min ([numeric_only,Â engine,Â ...]) Calculate the expanding minimum. Expanding.max ([numeric_only,Â engine,Â ...]) Calculate the expanding maximum. Expanding.corr ([other,Â pairwise,Â ddof,Â ...]) Calculate the expanding correlation. Expanding.cov ([other,Â pairwise,Â ddof,Â ...]) Calculate the expanding sample covariance. Expanding.skew ([numeric_only]) Calculate the expanding unbiased skewness. Expanding.kurt ([numeric_only]) Calculate the expanding Fisher's definition of kurtosis without bias. Expanding.apply (func[,Â raw,Â engine,Â ...]) Calculate the expanding custom aggregation function. Expanding.aggregate (func,Â *args,Â **kwargs) Aggregate using one or more operations over the specified axis. Expanding.quantile (q[,Â interpolation,Â ...]) Calculate the expanding quantile. Expanding.sem ([ddof,Â numeric_only]) Calculate the expanding standard error of mean. Expanding.rank ([method,Â ascending,Â pct,Â ...]) Calculate the expanding rank. Exponentially-weighted window functions # ExponentialMovingWindow.mean ([numeric_only,Â ...]) Calculate the ewm (exponential weighted moment) mean. ExponentialMovingWindow.sum ([numeric_only,Â ...]) Calculate the ewm (exponential weighted moment) sum. ExponentialMovingWindow.std ([bias,Â numeric_only]) Calculate the ewm (exponential weighted moment) standard deviation. ExponentialMovingWindow.var ([bias,Â numeric_only]) Calculate the ewm (exponential weighted moment) variance. ExponentialMovingWindow.corr ([other,Â ...]) Calculate the ewm (exponential weighted moment) sample correlation. ExponentialMovingWindow.cov ([other,Â ...]) Calculate the ewm (exponential weighted moment) sample covariance. Window indexer # Base class for defining custom window boundaries. api.indexers.BaseIndexer ([index_array,Â ...]) Base class for window bounds calculations. api.indexers.FixedForwardWindowIndexer ([...]) Creates window boundaries for fixed-length windows that include the current row. api.indexers.VariableOffsetWindowIndexer ([...]) Calculate window boundaries based on a non-fixed offset such as a BusinessDay. previous pandas.tseries.frequencies.to_offset next pandas.core.window.rolling.Rolling.count On this page Rolling window functions Weighted window functions Expanding window functions Exponentially-weighted window functions Window indexer Show Source",
    "url": "https://pandas.pydata.org/docs/reference/window.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Extensions pandas.array... pandas.arrays.NumpyExtensionArray # class pandas.arrays. NumpyExtensionArray ( values , copy = False ) [source] # A pandas ExtensionArray for NumPy data. This is mostly for internal compatibility, and is not especially useful on its own. Parameters : values ndarray The NumPy ndarray to wrap. Must be 1-dimensional. copy bool, default False Whether to copy values . Attributes None Methods None Examples >>> pd . arrays . NumpyExtensionArray ( np . array ([ 0 , 1 , 2 , 3 ])) <NumpyExtensionArray> [0, 1, 2, 3] Length: 4, dtype: int64 previous pandas.api.extensions.ExtensionArray._values_for_factorize next pandas.api.indexers.check_array_indexer On this page NumpyExtensionArray Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.arrays.NumpyExtensionArray.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Group by:... Group by: split-apply-combine # By âgroup byâ we are referring to a process involving one or more of the following steps: Splitting the data into groups based on some criteria. Applying a function to each group independently. Combining the results into a data structure. Out of these, the split step is the most straightforward. In the apply step, we might wish to do one of the following: Aggregation : compute a summary statistic (or statistics) for each group. Some examples: Compute group sums or means. Compute group sizes / counts. Transformation : perform some group-specific computations and return a like-indexed object. Some examples: Standardize data (zscore) within a group. Filling NAs within groups with a value derived from each group. Filtration : discard some groups, according to a group-wise computation that evaluates to True or False. Some examples: Discard data that belong to groups with only a few members. Filter out data based on the group sum or mean. Many of these operations are defined on GroupBy objects. These operations are similar to those of the aggregating API , window API , and resample API . It is possible that a given operation does not fall into one of these categories or is some combination of them. In such a case, it may be possible to compute the operation using GroupByâs apply method. This method will examine the results of the apply step and try to sensibly combine them into a single result if it doesnât fit into either of the above three categories. Note An operation that is split into multiple steps using built-in GroupBy operations will be more efficient than using the apply method with a user-defined Python function. The name GroupBy should be quite familiar to those who have used a SQL-based tool (or itertools ), in which you can write code like: SELECT Column1 , Column2 , mean ( Column3 ), sum ( Column4 ) FROM SomeTable GROUP BY Column1 , Column2 We aim to make operations like this natural and easy to express using pandas. Weâll address each area of GroupBy functionality, then provide some non-trivial examples / use cases. See the cookbook for some advanced strategies. Splitting an object into groups # The abstract definition of grouping is to provide a mapping of labels to group names. To create a GroupBy object (more on what the GroupBy object is later), you may do the following: In [1]: speeds = pd . DataFrame ( ...: [ ...: ( \"bird\" , \"Falconiformes\" , 389.0 ), ...: ( \"bird\" , \"Psittaciformes\" , 24.0 ), ...: ( \"mammal\" , \"Carnivora\" , 80.2 ), ...: ( \"mammal\" , \"Primates\" , np . nan ), ...: ( \"mammal\" , \"Carnivora\" , 58 ), ...: ], ...: index = [ \"falcon\" , \"parrot\" , \"lion\" , \"monkey\" , \"leopard\" ], ...: columns = ( \"class\" , \"order\" , \"max_speed\" ), ...: ) ...: In [2]: speeds Out[2]: class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 In [3]: grouped = speeds . groupby ( \"class\" ) In [4]: grouped = speeds . groupby ([ \"class\" , \"order\" ]) The mapping can be specified many different ways: A Python function, to be called on each of the index labels. A list or NumPy array of the same length as the index. A dict or Series , providing a label -> group name mapping. For DataFrame objects, a string indicating either a column name or an index level name to be used to group. A list of any of the above things. Collectively we refer to the grouping objects as the keys . For example, consider the following DataFrame : Note A string passed to groupby may refer to either a column or an index level. If a string matches both a column name and an index level name, a ValueError will be raised. In [5]: df = pd . DataFrame ( ...: { ...: \"A\" : [ \"foo\" , \"bar\" , \"foo\" , \"bar\" , \"foo\" , \"bar\" , \"foo\" , \"foo\" ], ...: \"B\" : [ \"one\" , \"one\" , \"two\" , \"three\" , \"two\" , \"two\" , \"one\" , \"three\" ], ...: \"C\" : np . random . randn ( 8 ), ...: \"D\" : np . random . randn ( 8 ), ...: } ...: ) ...: In [6]: df Out[6]: A B C D 0 foo one 0.469112 -0.861849 1 bar one -0.282863 -2.104569 2 foo two -1.509059 -0.494929 3 bar three -1.135632 1.071804 4 foo two 1.212112 0.721555 5 bar two -0.173215 -0.706771 6 foo one 0.119209 -1.039575 7 foo three -1.044236 0.271860 On a DataFrame, we obtain a GroupBy object by calling groupby() . This method returns a pandas.api.typing.DataFrameGroupBy instance. We could naturally group by either the A or B columns, or both: In [7]: grouped = df . groupby ( \"A\" ) In [8]: grouped = df . groupby ( \"B\" ) In [9]: grouped = df . groupby ([ \"A\" , \"B\" ]) Note df.groupby('A') is just syntactic sugar for df.groupby(df['A']) . If we also have a MultiIndex on columns A and B , we can group by all the columns except the one we specify: In [10]: df2 = df . set_index ([ \"A\" , \"B\" ]) In [11]: grouped = df2 . groupby ( level = df2 . index . names . difference ([ \"B\" ])) In [12]: grouped . sum () Out[12]: C D A bar -1.591710 -1.739537 foo -0.752861 -1.402938 The above GroupBy will split the DataFrame on its index (rows). To split by columns, first do a transpose: In [13]: def get_letter_type ( letter ): ....: if letter . lower () in 'aeiou' : ....: return 'vowel' ....: else : ....: return 'consonant' ....: In [14]: grouped = df . T . groupby ( get_letter_type ) pandas Index objects support duplicate values. If a non-unique index is used as the group key in a groupby operation, all values for the same index value will be considered to be in one group and thus the output of aggregation functions will only contain unique index values: In [15]: index = [ 1 , 2 , 3 , 1 , 2 , 3 ] In [16]: s = pd . Series ([ 1 , 2 , 3 , 10 , 20 , 30 ], index = index ) In [17]: s Out[17]: 1 1 2 2 3 3 1 10 2 20 3 30 dtype: int64 In [18]: grouped = s . groupby ( level = 0 ) In [19]: grouped . first () Out[19]: 1 1 2 2 3 3 dtype: int64 In [20]: grouped . last () Out[20]: 1 10 2 20 3 30 dtype: int64 In [21]: grouped . sum () Out[21]: 1 11 2 22 3 33 dtype: int64 Note that no splitting occurs until itâs needed. Creating the GroupBy object only verifies that youâve passed a valid mapping. Note Many kinds of complicated data manipulations can be expressed in terms of GroupBy operations (though it canât be guaranteed to be the most efficient implementation). You can get quite creative with the label mapping functions. GroupBy sorting # By default the group keys are sorted during the groupby operation. You may however pass sort=False for potential speedups. With sort=False the order among group-keys follows the order of appearance of the keys in the original dataframe: In [22]: df2 = pd . DataFrame ({ \"X\" : [ \"B\" , \"B\" , \"A\" , \"A\" ], \"Y\" : [ 1 , 2 , 3 , 4 ]}) In [23]: df2 . groupby ([ \"X\" ]) . sum () Out[23]: Y X A 7 B 3 In [24]: df2 . groupby ([ \"X\" ], sort = False ) . sum () Out[24]: Y X B 3 A 7 Note that groupby will preserve the order in which observations are sorted within each group. For example, the groups created by groupby() below are in the order they appeared in the original DataFrame : In [25]: df3 = pd . DataFrame ({ \"X\" : [ \"A\" , \"B\" , \"A\" , \"B\" ], \"Y\" : [ 1 , 4 , 3 , 2 ]}) In [26]: df3 . groupby ( \"X\" ) . get_group ( \"A\" ) Out[26]: X Y 0 A 1 2 A 3 In [27]: df3 . groupby ([ \"X\" ]) . get_group (( \"B\" ,)) Out[27]: X Y 1 B 4 3 B 2 GroupBy dropna # By default NA values are excluded from group keys during the groupby operation. However, in case you want to include NA values in group keys, you could pass dropna=False to achieve it. In [28]: df_list = [[ 1 , 2 , 3 ], [ 1 , None , 4 ], [ 2 , 1 , 3 ], [ 1 , 2 , 2 ]] In [29]: df_dropna = pd . DataFrame ( df_list , columns = [ \"a\" , \"b\" , \"c\" ]) In [30]: df_dropna Out[30]: a b c 0 1 2.0 3 1 1 NaN 4 2 2 1.0 3 3 1 2.0 2 # Default ``dropna`` is set to True, which will exclude NaNs in keys In [31]: df_dropna . groupby ( by = [ \"b\" ], dropna = True ) . sum () Out[31]: a c b 1.0 2 3 2.0 2 5 # In order to allow NaN in keys, set ``dropna`` to False In [32]: df_dropna . groupby ( by = [ \"b\" ], dropna = False ) . sum () Out[32]: a c b 1.0 2 3 2.0 2 5 NaN 1 4 The default setting of dropna argument is True which means NA are not included in group keys. GroupBy object attributes # The groups attribute is a dictionary whose keys are the computed unique groups and corresponding values are the axis labels belonging to each group. In the above example we have: In [33]: df . groupby ( \"A\" ) . groups Out[33]: {'bar': [1, 3, 5], 'foo': [0, 2, 4, 6, 7]} In [34]: df . T . groupby ( get_letter_type ) . groups Out[34]: {'consonant': ['B', 'C', 'D'], 'vowel': ['A']} Calling the standard Python len function on the GroupBy object returns the number of groups, which is the same as the length of the groups dictionary: In [35]: grouped = df . groupby ([ \"A\" , \"B\" ]) In [36]: grouped . groups Out[36]: {('bar', 'one'): [1], ('bar', 'three'): [3], ('bar', 'two'): [5], ('foo', 'one'): [0, 6], ('foo', 'three'): [7], ('foo', 'two'): [2, 4]} In [37]: len ( grouped ) Out[37]: 6 GroupBy will tab complete column names, GroupBy operations, and other attributes: In [38]: n = 10 In [39]: weight = np . random . normal ( 166 , 20 , size = n ) In [40]: height = np . random . normal ( 60 , 10 , size = n ) In [41]: time = pd . date_range ( \"1/1/2000\" , periods = n ) In [42]: gender = np . random . choice ([ \"male\" , \"female\" ], size = n ) In [43]: df = pd . DataFrame ( ....: { \"height\" : height , \"weight\" : weight , \"gender\" : gender }, index = time ....: ) ....: In [44]: df Out[44]: height weight gender 2000-01-01 42.849980 157.500553 male 2000-01-02 49.607315 177.340407 male 2000-01-03 56.293531 171.524640 male 2000-01-04 48.421077 144.251986 female 2000-01-05 46.556882 152.526206 male 2000-01-06 68.448851 168.272968 female 2000-01-07 70.757698 136.431469 male 2000-01-08 58.909500 176.499753 female 2000-01-09 76.435631 174.094104 female 2000-01-10 45.306120 177.540920 male In [45]: gb = df . groupby ( \"gender\" ) In [46]: gb .< TAB > # noqa: E225, E999 gb.agg gb.boxplot gb.cummin gb.describe gb.filter gb.get_group gb.height gb.last gb.median gb.ngroups gb.plot gb.rank gb.std gb.transform gb.aggregate gb.count gb.cumprod gb.dtype gb.first gb.groups gb.hist gb.max gb.min gb.nth gb.prod gb.resample gb.sum gb.var gb.apply gb.cummax gb.cumsum gb.fillna gb.gender gb.head gb.indices gb.mean gb.name gb.ohlc gb.quantile gb.size gb.tail gb.weight GroupBy with MultiIndex # With hierarchically-indexed data , itâs quite natural to group by one of the levels of the hierarchy. Letâs create a Series with a two-level MultiIndex . In [47]: arrays = [ ....: [ \"bar\" , \"bar\" , \"baz\" , \"baz\" , \"foo\" , \"foo\" , \"qux\" , \"qux\" ], ....: [ \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" ], ....: ] ....: In [48]: index = pd . MultiIndex . from_arrays ( arrays , names = [ \"first\" , \"second\" ]) In [49]: s = pd . Series ( np . random . randn ( 8 ), index = index ) In [50]: s Out[50]: first second bar one -0.919854 two -0.042379 baz one 1.247642 two -0.009920 foo one 0.290213 two 0.495767 qux one 0.362949 two 1.548106 dtype: float64 We can then group by one of the levels in s . In [51]: grouped = s . groupby ( level = 0 ) In [52]: grouped . sum () Out[52]: first bar -0.962232 baz 1.237723 foo 0.785980 qux 1.911055 dtype: float64 If the MultiIndex has names specified, these can be passed instead of the level number: In [53]: s . groupby ( level = \"second\" ) . sum () Out[53]: second one 0.980950 two 1.991575 dtype: float64 Grouping with multiple levels is supported. In [54]: arrays = [ ....: [ \"bar\" , \"bar\" , \"baz\" , \"baz\" , \"foo\" , \"foo\" , \"qux\" , \"qux\" ], ....: [ \"doo\" , \"doo\" , \"bee\" , \"bee\" , \"bop\" , \"bop\" , \"bop\" , \"bop\" ], ....: [ \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" ], ....: ] ....: In [55]: index = pd . MultiIndex . from_arrays ( arrays , names = [ \"first\" , \"second\" , \"third\" ]) In [56]: s = pd . Series ( np . random . randn ( 8 ), index = index ) In [57]: s Out[57]: first second third bar doo one -1.131345 two -0.089329 baz bee one 0.337863 two -0.945867 foo bop one -0.932132 two 1.956030 qux bop one 0.017587 two -0.016692 dtype: float64 In [58]: s . groupby ( level = [ \"first\" , \"second\" ]) . sum () Out[58]: first second bar doo -1.220674 baz bee -0.608004 foo bop 1.023898 qux bop 0.000895 dtype: float64 Index level names may be supplied as keys. In [59]: s . groupby ([ \"first\" , \"second\" ]) . sum () Out[59]: first second bar doo -1.220674 baz bee -0.608004 foo bop 1.023898 qux bop 0.000895 dtype: float64 More on the sum function and aggregation later. Grouping DataFrame with Index levels and columns # A DataFrame may be grouped by a combination of columns and index levels. You can specify both column and index names, or use a Grouper . Letâs first create a DataFrame with a MultiIndex: In [60]: arrays = [ ....: [ \"bar\" , \"bar\" , \"baz\" , \"baz\" , \"foo\" , \"foo\" , \"qux\" , \"qux\" ], ....: [ \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" , \"one\" , \"two\" ], ....: ] ....: In [61]: index = pd . MultiIndex . from_arrays ( arrays , names = [ \"first\" , \"second\" ]) In [62]: df = pd . DataFrame ({ \"A\" : [ 1 , 1 , 1 , 1 , 2 , 2 , 3 , 3 ], \"B\" : np . arange ( 8 )}, index = index ) In [63]: df Out[63]: A B first second bar one 1 0 two 1 1 baz one 1 2 two 1 3 foo one 2 4 two 2 5 qux one 3 6 two 3 7 Then we group df by the second index level and the A column. In [64]: df . groupby ([ pd . Grouper ( level = 1 ), \"A\" ]) . sum () Out[64]: B second A one 1 2 2 4 3 6 two 1 4 2 5 3 7 Index levels may also be specified by name. In [65]: df . groupby ([ pd . Grouper ( level = \"second\" ), \"A\" ]) . sum () Out[65]: B second A one 1 2 2 4 3 6 two 1 4 2 5 3 7 Index level names may be specified as keys directly to groupby . In [66]: df . groupby ([ \"second\" , \"A\" ]) . sum () Out[66]: B second A one 1 2 2 4 3 6 two 1 4 2 5 3 7 DataFrame column selection in GroupBy # Once you have created the GroupBy object from a DataFrame, you might want to do something different for each of the columns. Thus, by using [] on the GroupBy object in a similar way as the one used to get a column from a DataFrame, you can do: In [67]: df = pd . DataFrame ( ....: { ....: \"A\" : [ \"foo\" , \"bar\" , \"foo\" , \"bar\" , \"foo\" , \"bar\" , \"foo\" , \"foo\" ], ....: \"B\" : [ \"one\" , \"one\" , \"two\" , \"three\" , \"two\" , \"two\" , \"one\" , \"three\" ], ....: \"C\" : np . random . randn ( 8 ), ....: \"D\" : np . random . randn ( 8 ), ....: } ....: ) ....: In [68]: df Out[68]: A B C D 0 foo one -0.575247 1.346061 1 bar one 0.254161 1.511763 2 foo two -1.143704 1.627081 3 bar three 0.215897 -0.990582 4 foo two 1.193555 -0.441652 5 bar two -0.077118 1.211526 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 In [69]: grouped = df . groupby ([ \"A\" ]) In [70]: grouped_C = grouped [ \"C\" ] In [71]: grouped_D = grouped [ \"D\" ] This is mainly syntactic sugar for the alternative, which is much more verbose: In [72]: df [ \"C\" ] . groupby ( df [ \"A\" ]) Out[72]: <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f83fbbec790> Additionally, this method avoids recomputing the internal grouping information derived from the passed key. You can also include the grouping columns if you want to operate on them. In [73]: grouped [[ \"A\" , \"B\" ]] . sum () Out[73]: A B A bar barbarbar onethreetwo foo foofoofoofoofoo onetwotwoonethree Iterating through groups # With the GroupBy object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby() : In [74]: grouped = df . groupby ( 'A' ) In [75]: for name , group in grouped : ....: print ( name ) ....: print ( group ) ....: bar A B C D 1 bar one 0.254161 1.511763 3 bar three 0.215897 -0.990582 5 bar two -0.077118 1.211526 foo A B C D 0 foo one -0.575247 1.346061 2 foo two -1.143704 1.627081 4 foo two 1.193555 -0.441652 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 In the case of grouping by multiple keys, the group name will be a tuple: In [76]: for name , group in df . groupby ([ 'A' , 'B' ]): ....: print ( name ) ....: print ( group ) ....: ('bar', 'one') A B C D 1 bar one 0.254161 1.511763 ('bar', 'three') A B C D 3 bar three 0.215897 -0.990582 ('bar', 'two') A B C D 5 bar two -0.077118 1.211526 ('foo', 'one') A B C D 0 foo one -0.575247 1.346061 6 foo one -0.408530 0.268520 ('foo', 'three') A B C D 7 foo three -0.862495 0.02458 ('foo', 'two') A B C D 2 foo two -1.143704 1.627081 4 foo two 1.193555 -0.441652 See Iterating through groups . Selecting a group # A single group can be selected using DataFrameGroupBy.get_group() : In [77]: grouped . get_group ( \"bar\" ) Out[77]: A B C D 1 bar one 0.254161 1.511763 3 bar three 0.215897 -0.990582 5 bar two -0.077118 1.211526 Or for an object grouped on multiple columns: In [78]: df . groupby ([ \"A\" , \"B\" ]) . get_group (( \"bar\" , \"one\" )) Out[78]: A B C D 1 bar one 0.254161 1.511763 Aggregation # An aggregation is a GroupBy operation that reduces the dimension of the grouping object. The result of an aggregation is, or at least is treated as, a scalar value for each column in a group. For example, producing the sum of each column in a group of values. In [79]: animals = pd . DataFrame ( ....: { ....: \"kind\" : [ \"cat\" , \"dog\" , \"cat\" , \"dog\" ], ....: \"height\" : [ 9.1 , 6.0 , 9.5 , 34.0 ], ....: \"weight\" : [ 7.9 , 7.5 , 9.9 , 198.0 ], ....: } ....: ) ....: In [80]: animals Out[80]: kind height weight 0 cat 9.1 7.9 1 dog 6.0 7.5 2 cat 9.5 9.9 3 dog 34.0 198.0 In [81]: animals . groupby ( \"kind\" ) . sum () Out[81]: height weight kind cat 18.6 17.8 dog 40.0 205.5 In the result, the keys of the groups appear in the index by default. They can be instead included in the columns by passing as_index=False . In [82]: animals . groupby ( \"kind\" , as_index = False ) . sum () Out[82]: kind height weight 0 cat 18.6 17.8 1 dog 40.0 205.5 Built-in aggregation methods # Many common aggregations are built-in to GroupBy objects as methods. Of the methods listed below, those with a * do not have an efficient, GroupBy-specific, implementation. Method Description any() Compute whether any of the values in the groups are truthy all() Compute whether all of the values in the groups are truthy count() Compute the number of non-NA values in the groups cov() * Compute the covariance of the groups first() Compute the first occurring value in each group idxmax() Compute the index of the maximum value in each group idxmin() Compute the index of the minimum value in each group last() Compute the last occurring value in each group max() Compute the maximum value in each group mean() Compute the mean of each group median() Compute the median of each group min() Compute the minimum value in each group nunique() Compute the number of unique values in each group prod() Compute the product of the values in each group quantile() Compute a given quantile of the values in each group sem() Compute the standard error of the mean of the values in each group size() Compute the number of values in each group skew() * Compute the skew of the values in each group std() Compute the standard deviation of the values in each group sum() Compute the sum of the values in each group var() Compute the variance of the values in each group Some examples: In [83]: df . groupby ( \"A\" )[[ \"C\" , \"D\" ]] . max () Out[83]: C D A bar 0.254161 1.511763 foo 1.193555 1.627081 In [84]: df . groupby ([ \"A\" , \"B\" ]) . mean () Out[84]: C D A B bar one 0.254161 1.511763 three 0.215897 -0.990582 two -0.077118 1.211526 foo one -0.491888 0.807291 three -0.862495 0.024580 two 0.024925 0.592714 Another aggregation example is to compute the size of each group. This is included in GroupBy as the size method. It returns a Series whose index consists of the group names and the values are the sizes of each group. In [85]: grouped = df . groupby ([ \"A\" , \"B\" ]) In [86]: grouped . size () Out[86]: A B bar one 1 three 1 two 1 foo one 2 three 1 two 2 dtype: int64 While the DataFrameGroupBy.describe() method is not itself a reducer, it can be used to conveniently produce a collection of summary statistics about each of the groups. In [87]: grouped . describe () Out[87]: C ... D count mean std ... 50% 75% max A B ... bar one 1.0 0.254161 NaN ... 1.511763 1.511763 1.511763 three 1.0 0.215897 NaN ... -0.990582 -0.990582 -0.990582 two 1.0 -0.077118 NaN ... 1.211526 1.211526 1.211526 foo one 2.0 -0.491888 0.117887 ... 0.807291 1.076676 1.346061 three 1.0 -0.862495 NaN ... 0.024580 0.024580 0.024580 two 2.0 0.024925 1.652692 ... 0.592714 1.109898 1.627081 [6 rows x 16 columns] Another aggregation example is to compute the number of unique values of each group. This is similar to the DataFrameGroupBy.value_counts() function, except that it only counts the number of unique values. In [88]: ll = [[ 'foo' , 1 ], [ 'foo' , 2 ], [ 'foo' , 2 ], [ 'bar' , 1 ], [ 'bar' , 1 ]] In [89]: df4 = pd . DataFrame ( ll , columns = [ \"A\" , \"B\" ]) In [90]: df4 Out[90]: A B 0 foo 1 1 foo 2 2 foo 2 3 bar 1 4 bar 1 In [91]: df4 . groupby ( \"A\" )[ \"B\" ] . nunique () Out[91]: A bar 1 foo 2 Name: B, dtype: int64 Note Aggregation functions will not return the groups that you are aggregating over as named columns when as_index=True , the default. The grouped columns will be the indices of the returned object. Passing as_index=False will return the groups that you are aggregating over as named columns, regardless if they are named indices or columns in the inputs. The aggregate() method # Note The aggregate() method can accept many different types of inputs. This section details using string aliases for various GroupBy methods; other inputs are detailed in the sections below. Any reduction method that pandas implements can be passed as a string to aggregate() . Users are encouraged to use the shorthand, agg . It will operate as if the corresponding method was called. In [92]: grouped = df . groupby ( \"A\" ) In [93]: grouped [[ \"C\" , \"D\" ]] . aggregate ( \"sum\" ) Out[93]: C D A bar 0.392940 1.732707 foo -1.796421 2.824590 In [94]: grouped = df . groupby ([ \"A\" , \"B\" ]) In [95]: grouped . agg ( \"sum\" ) Out[95]: C D A B bar one 0.254161 1.511763 three 0.215897 -0.990582 two -0.077118 1.211526 foo one -0.983776 1.614581 three -0.862495 0.024580 two 0.049851 1.185429 The result of the aggregation will have the group names as the new index. In the case of multiple keys, the result is a MultiIndex by default. As mentioned above, this can be changed by using the as_index option: In [96]: grouped = df . groupby ([ \"A\" , \"B\" ], as_index = False ) In [97]: grouped . agg ( \"sum\" ) Out[97]: A B C D 0 bar one 0.254161 1.511763 1 bar three 0.215897 -0.990582 2 bar two -0.077118 1.211526 3 foo one -0.983776 1.614581 4 foo three -0.862495 0.024580 5 foo two 0.049851 1.185429 In [98]: df . groupby ( \"A\" , as_index = False )[[ \"C\" , \"D\" ]] . agg ( \"sum\" ) Out[98]: A C D 0 bar 0.392940 1.732707 1 foo -1.796421 2.824590 Note that you could use the DataFrame.reset_index() DataFrame function to achieve the same result as the column names are stored in the resulting MultiIndex , although this will make an extra copy. In [99]: df . groupby ([ \"A\" , \"B\" ]) . agg ( \"sum\" ) . reset_index () Out[99]: A B C D 0 bar one 0.254161 1.511763 1 bar three 0.215897 -0.990582 2 bar two -0.077118 1.211526 3 foo one -0.983776 1.614581 4 foo three -0.862495 0.024580 5 foo two 0.049851 1.185429 Aggregation with User-Defined Functions # Users can also provide their own User-Defined Functions (UDFs) for custom aggregations. Warning When aggregating with a UDF, the UDF should not mutate the provided Series . See Mutating with User Defined Function (UDF) methods for more information. Note Aggregating with a UDF is often less performant than using the pandas built-in methods on GroupBy. Consider breaking up a complex operation into a chain of operations that utilize the built-in methods. In [100]: animals Out[100]: kind height weight 0 cat 9.1 7.9 1 dog 6.0 7.5 2 cat 9.5 9.9 3 dog 34.0 198.0 In [101]: animals . groupby ( \"kind\" )[[ \"height\" ]] . agg ( lambda x : set ( x )) Out[101]: height kind cat {9.1, 9.5} dog {34.0, 6.0} The resulting dtype will reflect that of the aggregating function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction. In [102]: animals . groupby ( \"kind\" )[[ \"height\" ]] . agg ( lambda x : x . astype ( int ) . sum ()) Out[102]: height kind cat 18 dog 40 Applying multiple functions at once # On a grouped Series , you can pass a list or dict of functions to SeriesGroupBy.agg() , outputting a DataFrame: In [103]: grouped = df . groupby ( \"A\" ) In [104]: grouped [ \"C\" ] . agg ([ \"sum\" , \"mean\" , \"std\" ]) Out[104]: sum mean std A bar 0.392940 0.130980 0.181231 foo -1.796421 -0.359284 0.912265 On a grouped DataFrame , you can pass a list of functions to DataFrameGroupBy.agg() to aggregate each column, which produces an aggregated result with a hierarchical column index: In [105]: grouped [[ \"C\" , \"D\" ]] . agg ([ \"sum\" , \"mean\" , \"std\" ]) Out[105]: C D sum mean std sum mean std A bar 0.392940 0.130980 0.181231 1.732707 0.577569 1.366330 foo -1.796421 -0.359284 0.912265 2.824590 0.564918 0.884785 The resulting aggregations are named after the functions themselves. If you need to rename, then you can add in a chained operation for a Series like this: In [106]: ( .....: grouped [ \"C\" ] .....: . agg ([ \"sum\" , \"mean\" , \"std\" ]) .....: . rename ( columns = { \"sum\" : \"foo\" , \"mean\" : \"bar\" , \"std\" : \"baz\" }) .....: ) .....: Out[106]: foo bar baz A bar 0.392940 0.130980 0.181231 foo -1.796421 -0.359284 0.912265 For a grouped DataFrame , you can rename in a similar manner: In [107]: ( .....: grouped [[ \"C\" , \"D\" ]] . agg ([ \"sum\" , \"mean\" , \"std\" ]) . rename ( .....: columns = { \"sum\" : \"foo\" , \"mean\" : \"bar\" , \"std\" : \"baz\" } .....: ) .....: ) .....: Out[107]: C D foo bar baz foo bar baz A bar 0.392940 0.130980 0.181231 1.732707 0.577569 1.366330 foo -1.796421 -0.359284 0.912265 2.824590 0.564918 0.884785 Note In general, the output column names should be unique, but pandas will allow you apply to the same function (or two functions with the same name) to the same column. In [108]: grouped [ \"C\" ] . agg ([ \"sum\" , \"sum\" ]) Out[108]: sum sum A bar 0.392940 0.392940 foo -1.796421 -1.796421 pandas also allows you to provide multiple lambdas. In this case, pandas will mangle the name of the (nameless) lambda functions, appending _<i> to each subsequent lambda. In [109]: grouped [ \"C\" ] . agg ([ lambda x : x . max () - x . min (), lambda x : x . median () - x . mean ()]) Out[109]: <lambda_0> <lambda_1> A bar 0.331279 0.084917 foo 2.337259 -0.215962 Named aggregation # To support column-specific aggregation with control over the output column names , pandas accepts the special syntax in DataFrameGroupBy.agg() and SeriesGroupBy.agg() , known as ânamed aggregationâ, where The keywords are the output column names The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. pandas provides the NamedAgg namedtuple with the fields ['column', 'aggfunc'] to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias. In [110]: animals Out[110]: kind height weight 0 cat 9.1 7.9 1 dog 6.0 7.5 2 cat 9.5 9.9 3 dog 34.0 198.0 In [111]: animals . groupby ( \"kind\" ) . agg ( .....: min_height = pd . NamedAgg ( column = \"height\" , aggfunc = \"min\" ), .....: max_height = pd . NamedAgg ( column = \"height\" , aggfunc = \"max\" ), .....: average_weight = pd . NamedAgg ( column = \"weight\" , aggfunc = \"mean\" ), .....: ) .....: Out[111]: min_height max_height average_weight kind cat 9.1 9.5 8.90 dog 6.0 34.0 102.75 NamedAgg is just a namedtuple . Plain tuples are allowed as well. In [112]: animals . groupby ( \"kind\" ) . agg ( .....: min_height = ( \"height\" , \"min\" ), .....: max_height = ( \"height\" , \"max\" ), .....: average_weight = ( \"weight\" , \"mean\" ), .....: ) .....: Out[112]: min_height max_height average_weight kind cat 9.1 9.5 8.90 dog 6.0 34.0 102.75 If the column names you want are not valid Python keywords, construct a dictionary and unpack the keyword arguments In [113]: animals . groupby ( \"kind\" ) . agg ( .....: ** { .....: \"total weight\" : pd . NamedAgg ( column = \"weight\" , aggfunc = \"sum\" ) .....: } .....: ) .....: Out[113]: total weight kind cat 17.8 dog 205.5 When using named aggregation, additional keyword arguments are not passed through to the aggregation functions; only pairs of (column, aggfunc) should be passed as **kwargs . If your aggregation functions require additional arguments, apply them partially with functools.partial() . Named aggregation is also valid for Series groupby aggregations. In this case thereâs no column selection, so the values are just the functions. In [114]: animals . groupby ( \"kind\" ) . height . agg ( .....: min_height = \"min\" , .....: max_height = \"max\" , .....: ) .....: Out[114]: min_height max_height kind cat 9.1 9.5 dog 6.0 34.0 Applying different functions to DataFrame columns # By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame: In [115]: grouped . agg ({ \"C\" : \"sum\" , \"D\" : lambda x : np . std ( x , ddof = 1 )}) Out[115]: C D A bar 0.392940 1.366330 foo -1.796421 0.884785 The function names can also be strings. In order for a string to be valid it must be implemented on GroupBy: In [116]: grouped . agg ({ \"C\" : \"sum\" , \"D\" : \"std\" }) Out[116]: C D A bar 0.392940 1.366330 foo -1.796421 0.884785 Transformation # A transformation is a GroupBy operation whose result is indexed the same as the one being grouped. Common examples include cumsum() and diff() . In [117]: speeds Out[117]: class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 In [118]: grouped = speeds . groupby ( \"class\" )[ \"max_speed\" ] In [119]: grouped . cumsum () Out[119]: falcon 389.0 parrot 413.0 lion 80.2 monkey NaN leopard 138.2 Name: max_speed, dtype: float64 In [120]: grouped . diff () Out[120]: falcon NaN parrot -365.0 lion NaN monkey NaN leopard NaN Name: max_speed, dtype: float64 Unlike aggregations, the groupings that are used to split the original object are not included in the result. Note Since transformations do not include the groupings that are used to split the result, the arguments as_index and sort in DataFrame.groupby() and Series.groupby() have no effect. A common use of a transformation is to add the result back into the original DataFrame. In [121]: result = speeds . copy () In [122]: result [ \"cumsum\" ] = grouped . cumsum () In [123]: result [ \"diff\" ] = grouped . diff () In [124]: result Out[124]: class order max_speed cumsum diff falcon bird Falconiformes 389.0 389.0 NaN parrot bird Psittaciformes 24.0 413.0 -365.0 lion mammal Carnivora 80.2 80.2 NaN monkey mammal Primates NaN NaN NaN leopard mammal Carnivora 58.0 138.2 NaN Built-in transformation methods # The following methods on GroupBy act as transformations. Method Description bfill() Back fill NA values within each group cumcount() Compute the cumulative count within each group cummax() Compute the cumulative max within each group cummin() Compute the cumulative min within each group cumprod() Compute the cumulative product within each group cumsum() Compute the cumulative sum within each group diff() Compute the difference between adjacent values within each group ffill() Forward fill NA values within each group pct_change() Compute the percent change between adjacent values within each group rank() Compute the rank of each value within each group shift() Shift values up or down within each group In addition, passing any built-in aggregation method as a string to transform() (see the next section) will broadcast the result across the group, producing a transformed result. If the aggregation method has an efficient implementation, this will be performant as well. The transform() method # Similar to the aggregation method , the transform() method can accept string aliases to the built-in transformation methods in the previous section. It can also accept string aliases to the built-in aggregation methods. When an aggregation method is provided, the result will be broadcast across the group. In [125]: speeds Out[125]: class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 In [126]: grouped = speeds . groupby ( \"class\" )[[ \"max_speed\" ]] In [127]: grouped . transform ( \"cumsum\" ) Out[127]: max_speed falcon 389.0 parrot 413.0 lion 80.2 monkey NaN leopard 138.2 In [128]: grouped . transform ( \"sum\" ) Out[128]: max_speed falcon 413.0 parrot 413.0 lion 138.2 monkey 138.2 leopard 138.2 In addition to string aliases, the transform() method can also accept User-Defined Functions (UDFs). The UDF must: Return a result that is either the same size as the group chunk or broadcastable to the size of the group chunk (e.g., a scalar, grouped.transform(lambda x: x.iloc[-1]) ). Operate column-by-column on the group chunk. The transform is applied to the first group chunk using chunk.apply. Not perform in-place operations on the group chunk. Group chunks should be treated as immutable, and changes to a group chunk may produce unexpected results. See Mutating with User Defined Function (UDF) methods for more information. (Optionally) operates on all columns of the entire group chunk at once. If this is supported, a fast path is used starting from the second chunk. Note Transforming by supplying transform with a UDF is often less performant than using the built-in methods on GroupBy. Consider breaking up a complex operation into a chain of operations that utilize the built-in methods. All of the examples in this section can be made more performant by calling built-in methods instead of using UDFs. See below for examples . Changed in version 2.0.0: When using .transform on a grouped DataFrame and the transformation function returns a DataFrame, pandas now aligns the resultâs index with the inputâs index. You can call .to_numpy() within the transformation function to avoid alignment. Similar to The aggregate() method , the resulting dtype will reflect that of the transformation function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction. Suppose we wish to standardize the data within each group: In [129]: index = pd . date_range ( \"10/1/1999\" , periods = 1100 ) In [130]: ts = pd . Series ( np . random . normal ( 0.5 , 2 , 1100 ), index ) In [131]: ts = ts . rolling ( window = 100 , min_periods = 100 ) . mean () . dropna () In [132]: ts . head () Out[132]: 2000-01-08 0.779333 2000-01-09 0.778852 2000-01-10 0.786476 2000-01-11 0.782797 2000-01-12 0.798110 Freq: D, dtype: float64 In [133]: ts . tail () Out[133]: 2002-09-30 0.660294 2002-10-01 0.631095 2002-10-02 0.673601 2002-10-03 0.709213 2002-10-04 0.719369 Freq: D, dtype: float64 In [134]: transformed = ts . groupby ( lambda x : x . year ) . transform ( .....: lambda x : ( x - x . mean ()) / x . std () .....: ) .....: We would expect the result to now have mean 0 and standard deviation 1 within each group (up to floating-point error), which we can easily check: # Original Data In [135]: grouped = ts . groupby ( lambda x : x . year ) In [136]: grouped . mean () Out[136]: 2000 0.442441 2001 0.526246 2002 0.459365 dtype: float64 In [137]: grouped . std () Out[137]: 2000 0.131752 2001 0.210945 2002 0.128753 dtype: float64 # Transformed Data In [138]: grouped_trans = transformed . groupby ( lambda x : x . year ) In [139]: grouped_trans . mean () Out[139]: 2000 -4.870756e-16 2001 -1.545187e-16 2002 4.136282e-16 dtype: float64 In [140]: grouped_trans . std () Out[140]: 2000 1.0 2001 1.0 2002 1.0 dtype: float64 We can also visually compare the original and transformed data sets. In [141]: compare = pd . DataFrame ({ \"Original\" : ts , \"Transformed\" : transformed }) In [142]: compare . plot () Out[142]: <Axes: > Transformation functions that have lower dimension outputs are broadcast to match the shape of the input array. In [143]: ts . groupby ( lambda x : x . year ) . transform ( lambda x : x . max () - x . min ()) Out[143]: 2000-01-08 0.623893 2000-01-09 0.623893 2000-01-10 0.623893 2000-01-11 0.623893 2000-01-12 0.623893 ... 2002-09-30 0.558275 2002-10-01 0.558275 2002-10-02 0.558275 2002-10-03 0.558275 2002-10-04 0.558275 Freq: D, Length: 1001, dtype: float64 Another common data transform is to replace missing data with the group mean. In [144]: cols = [ \"A\" , \"B\" , \"C\" ] In [145]: values = np . random . randn ( 1000 , 3 ) In [146]: values [ np . random . randint ( 0 , 1000 , 100 ), 0 ] = np . nan In [147]: values [ np . random . randint ( 0 , 1000 , 50 ), 1 ] = np . nan In [148]: values [ np . random . randint ( 0 , 1000 , 200 ), 2 ] = np . nan In [149]: data_df = pd . DataFrame ( values , columns = cols ) In [150]: data_df Out[150]: A B C 0 1.539708 -1.166480 0.533026 1 1.302092 -0.505754 NaN 2 -0.371983 1.104803 -0.651520 3 -1.309622 1.118697 -1.161657 4 -1.924296 0.396437 0.812436 .. ... ... ... 995 -0.093110 0.683847 -0.774753 996 -0.185043 1.438572 NaN 997 -0.394469 -0.642343 0.011374 998 -1.174126 1.857148 NaN 999 0.234564 0.517098 0.393534 [1000 rows x 3 columns] In [151]: countries = np . array ([ \"US\" , \"UK\" , \"GR\" , \"JP\" ]) In [152]: key = countries [ np . random . randint ( 0 , 4 , 1000 )] In [153]: grouped = data_df . groupby ( key ) # Non-NA count in each group In [154]: grouped . count () Out[154]: A B C GR 209 217 189 JP 240 255 217 UK 216 231 193 US 239 250 217 In [155]: transformed = grouped . transform ( lambda x : x . fillna ( x . mean ())) We can verify that the group means have not changed in the transformed data, and that the transformed data contains no NAs. In [156]: grouped_trans = transformed . groupby ( key ) In [157]: grouped . mean () # original group means Out[157]: A B C GR -0.098371 -0.015420 0.068053 JP 0.069025 0.023100 -0.077324 UK 0.034069 -0.052580 -0.116525 US 0.058664 -0.020399 0.028603 In [158]: grouped_trans . mean () # transformation did not change group means Out[158]: A B C GR -0.098371 -0.015420 0.068053 JP 0.069025 0.023100 -0.077324 UK 0.034069 -0.052580 -0.116525 US 0.058664 -0.020399 0.028603 In [159]: grouped . count () # original has some missing data points Out[159]: A B C GR 209 217 189 JP 240 255 217 UK 216 231 193 US 239 250 217 In [160]: grouped_trans . count () # counts after transformation Out[160]: A B C GR 228 228 228 JP 267 267 267 UK 247 247 247 US 258 258 258 In [161]: grouped_trans . size () # Verify non-NA count equals group size Out[161]: GR 228 JP 267 UK 247 US 258 dtype: int64 As mentioned in the note above, each of the examples in this section can be computed more efficiently using built-in methods. In the code below, the inefficient way using a UDF is commented out and the faster alternative appears below. # result = ts.groupby(lambda x: x.year).transform( # lambda x: (x - x.mean()) / x.std() # ) In [162]: grouped = ts . groupby ( lambda x : x . year ) In [163]: result = ( ts - grouped . transform ( \"mean\" )) / grouped . transform ( \"std\" ) # result = ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min()) In [164]: grouped = ts . groupby ( lambda x : x . year ) In [165]: result = grouped . transform ( \"max\" ) - grouped . transform ( \"min\" ) # grouped = data_df.groupby(key) # result = grouped.transform(lambda x: x.fillna(x.mean())) In [166]: grouped = data_df . groupby ( key ) In [167]: result = data_df . fillna ( grouped . transform ( \"mean\" )) Window and resample operations # It is possible to use resample() , expanding() and rolling() as methods on groupbys. The example below will apply the rolling() method on the samples of the column B, based on the groups of column A. In [168]: df_re = pd . DataFrame ({ \"A\" : [ 1 ] * 10 + [ 5 ] * 10 , \"B\" : np . arange ( 20 )}) In [169]: df_re Out[169]: A B 0 1 0 1 1 1 2 1 2 3 1 3 4 1 4 .. .. .. 15 5 15 16 5 16 17 5 17 18 5 18 19 5 19 [20 rows x 2 columns] In [170]: df_re . groupby ( \"A\" ) . rolling ( 4 ) . B . mean () Out[170]: A 1 0 NaN 1 NaN 2 NaN 3 1.5 4 2.5 ... 5 15 13.5 16 14.5 17 15.5 18 16.5 19 17.5 Name: B, Length: 20, dtype: float64 The expanding() method will accumulate a given operation ( sum() in the example) for all the members of each particular group. In [171]: df_re . groupby ( \"A\" ) . expanding () . sum () Out[171]: B A 1 0 0.0 1 1.0 2 3.0 3 6.0 4 10.0 ... ... 5 15 75.0 16 91.0 17 108.0 18 126.0 19 145.0 [20 rows x 1 columns] Suppose you want to use the resample() method to get a daily frequency in each group of your dataframe, and wish to complete the missing values with the ffill() method. In [172]: df_re = pd . DataFrame ( .....: { .....: \"date\" : pd . date_range ( start = \"2016-01-01\" , periods = 4 , freq = \"W\" ), .....: \"group\" : [ 1 , 1 , 2 , 2 ], .....: \"val\" : [ 5 , 6 , 7 , 8 ], .....: } .....: ) . set_index ( \"date\" ) .....: In [173]: df_re Out[173]: group val date 2016-01-03 1 5 2016-01-10 1 6 2016-01-17 2 7 2016-01-24 2 8 In [174]: df_re . groupby ( \"group\" ) . resample ( \"1D\" , include_groups = False ) . ffill () Out[174]: val group date 1 2016-01-03 5 2016-01-04 5 2016-01-05 5 2016-01-06 5 2016-01-07 5 ... ... 2 2016-01-20 7 2016-01-21 7 2016-01-22 7 2016-01-23 7 2016-01-24 8 [16 rows x 1 columns] Filtration # A filtration is a GroupBy operation that subsets the original grouping object. It may either filter out entire groups, part of groups, or both. Filtrations return a filtered version of the calling object, including the grouping columns when provided. In the following example, class is included in the result. In [175]: speeds Out[175]: class order max_speed falcon bird Falconiformes 389.0 parrot bird Psittaciformes 24.0 lion mammal Carnivora 80.2 monkey mammal Primates NaN leopard mammal Carnivora 58.0 In [176]: speeds . groupby ( \"class\" ) . nth ( 1 ) Out[176]: class order max_speed parrot bird Psittaciformes 24.0 monkey mammal Primates NaN Note Unlike aggregations, filtrations do not add the group keys to the index of the result. Because of this, passing as_index=False or sort=True will not affect these methods. Filtrations will respect subsetting the columns of the GroupBy object. In [177]: speeds . groupby ( \"class\" )[[ \"order\" , \"max_speed\" ]] . nth ( 1 ) Out[177]: order max_speed parrot Psittaciformes 24.0 monkey Primates NaN Built-in filtrations # The following methods on GroupBy act as filtrations. All these methods have an efficient, GroupBy-specific, implementation. Method Description head() Select the top row(s) of each group nth() Select the nth row(s) of each group tail() Select the bottom row(s) of each group Users can also use transformations along with Boolean indexing to construct complex filtrations within groups. For example, suppose we are given groups of products and their volumes, and we wish to subset the data to only the largest products capturing no more than 90% of the total volume within each group. In [178]: product_volumes = pd . DataFrame ( .....: { .....: \"group\" : list ( \"xxxxyyy\" ), .....: \"product\" : list ( \"abcdefg\" ), .....: \"volume\" : [ 10 , 30 , 20 , 15 , 40 , 10 , 20 ], .....: } .....: ) .....: In [179]: product_volumes Out[179]: group product volume 0 x a 10 1 x b 30 2 x c 20 3 x d 15 4 y e 40 5 y f 10 6 y g 20 # Sort by volume to select the largest products first In [180]: product_volumes = product_volumes . sort_values ( \"volume\" , ascending = False ) In [181]: grouped = product_volumes . groupby ( \"group\" )[ \"volume\" ] In [182]: cumpct = grouped . cumsum () / grouped . transform ( \"sum\" ) In [183]: cumpct Out[183]: 4 0.571429 1 0.400000 2 0.666667 6 0.857143 3 0.866667 0 1.000000 5 1.000000 Name: volume, dtype: float64 In [184]: significant_products = product_volumes [ cumpct <= 0.9 ] In [185]: significant_products . sort_values ([ \"group\" , \"product\" ]) Out[185]: group product volume 1 x b 30 2 x c 20 3 x d 15 4 y e 40 6 y g 20 The filter method # Note Filtering by supplying filter with a User-Defined Function (UDF) is often less performant than using the built-in methods on GroupBy. Consider breaking up a complex operation into a chain of operations that utilize the built-in methods. The filter method takes a User-Defined Function (UDF) that, when applied to an entire group, returns either True or False . The result of the filter method is then the subset of groups for which the UDF returned True . Suppose we want to take only elements that belong to groups with a group sum greater than 2. In [186]: sf = pd . Series ([ 1 , 1 , 2 , 3 , 3 , 3 ]) In [187]: sf . groupby ( sf ) . filter ( lambda x : x . sum () > 2 ) Out[187]: 3 3 4 3 5 3 dtype: int64 Another useful operation is filtering out elements that belong to groups with only a couple members. In [188]: dff = pd . DataFrame ({ \"A\" : np . arange ( 8 ), \"B\" : list ( \"aabbbbcc\" )}) In [189]: dff . groupby ( \"B\" ) . filter ( lambda x : len ( x ) > 2 ) Out[189]: A B 2 2 b 3 3 b 4 4 b 5 5 b Alternatively, instead of dropping the offending groups, we can return a like-indexed objects where the groups that do not pass the filter are filled with NaNs. In [190]: dff . groupby ( \"B\" ) . filter ( lambda x : len ( x ) > 2 , dropna = False ) Out[190]: A B 0 NaN NaN 1 NaN NaN 2 2.0 b 3 3.0 b 4 4.0 b 5 5.0 b 6 NaN NaN 7 NaN NaN For DataFrames with multiple columns, filters should explicitly specify a column as the filter criterion. In [191]: dff [ \"C\" ] = np . arange ( 8 ) In [192]: dff . groupby ( \"B\" ) . filter ( lambda x : len ( x [ \"C\" ]) > 2 ) Out[192]: A B C 2 2 b 2 3 3 b 3 4 4 b 4 5 5 b 5 Flexible apply # Some operations on the grouped data might not fit into the aggregation, transformation, or filtration categories. For these, you can use the apply function. Warning apply has to try to infer from the result whether it should act as a reducer, transformer, or filter, depending on exactly what is passed to it. Thus the grouped column(s) may be included in the output or not. While it tries to intelligently guess how to behave, it can sometimes guess wrong. Note All of the examples in this section can be more reliably, and more efficiently, computed using other pandas functionality. In [193]: df Out[193]: A B C D 0 foo one -0.575247 1.346061 1 bar one 0.254161 1.511763 2 foo two -1.143704 1.627081 3 bar three 0.215897 -0.990582 4 foo two 1.193555 -0.441652 5 bar two -0.077118 1.211526 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 In [194]: grouped = df . groupby ( \"A\" ) # could also just call .describe() In [195]: grouped [ \"C\" ] . apply ( lambda x : x . describe ()) Out[195]: A bar count 3.000000 mean 0.130980 std 0.181231 min -0.077118 25% 0.069390 ... foo min -1.143704 25% -0.862495 50% -0.575247 75% -0.408530 max 1.193555 Name: C, Length: 16, dtype: float64 The dimension of the returned result can also change: In [196]: grouped = df . groupby ( 'A' )[ 'C' ] In [197]: def f ( group ): .....: return pd . DataFrame ({ 'original' : group , .....: 'demeaned' : group - group . mean ()}) .....: In [198]: grouped . apply ( f ) Out[198]: original demeaned A bar 1 0.254161 0.123181 3 0.215897 0.084917 5 -0.077118 -0.208098 foo 0 -0.575247 -0.215962 2 -1.143704 -0.784420 4 1.193555 1.552839 6 -0.408530 -0.049245 7 -0.862495 -0.503211 apply on a Series can operate on a returned value from the applied function that is itself a series, and possibly upcast the result to a DataFrame: In [199]: def f ( x ): .....: return pd . Series ([ x , x ** 2 ], index = [ \"x\" , \"x^2\" ]) .....: In [200]: s = pd . Series ( np . random . rand ( 5 )) In [201]: s Out[201]: 0 0.582898 1 0.098352 2 0.001438 3 0.009420 4 0.815826 dtype: float64 In [202]: s . apply ( f ) Out[202]: x x^2 0 0.582898 0.339770 1 0.098352 0.009673 2 0.001438 0.000002 3 0.009420 0.000089 4 0.815826 0.665572 Similar to The aggregate() method , the resulting dtype will reflect that of the apply function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction. Control grouped column(s) placement with group_keys # To control whether the grouped column(s) are included in the indices, you can use the argument group_keys which defaults to True . Compare In [203]: df . groupby ( \"A\" , group_keys = True ) . apply ( lambda x : x , include_groups = False ) Out[203]: B C D A bar 1 one 0.254161 1.511763 3 three 0.215897 -0.990582 5 two -0.077118 1.211526 foo 0 one -0.575247 1.346061 2 two -1.143704 1.627081 4 two 1.193555 -0.441652 6 one -0.408530 0.268520 7 three -0.862495 0.024580 with In [204]: df . groupby ( \"A\" , group_keys = False ) . apply ( lambda x : x , include_groups = False ) Out[204]: B C D 0 one -0.575247 1.346061 1 one 0.254161 1.511763 2 two -1.143704 1.627081 3 three 0.215897 -0.990582 4 two 1.193555 -0.441652 5 two -0.077118 1.211526 6 one -0.408530 0.268520 7 three -0.862495 0.024580 Numba Accelerated Routines # Added in version 1.1. If Numba is installed as an optional dependency, the transform and aggregate methods support engine='numba' and engine_kwargs arguments. See enhancing performance with Numba for general usage of the arguments and performance considerations. The function signature must start with values, index exactly as the data belonging to each group will be passed into values , and the group index will be passed into index . Warning When using engine='numba' , there will be no âfall backâ behavior internally. The group data and group index will be passed as NumPy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Other useful features # Exclusion of non-numeric columns # Again consider the example DataFrame weâve been looking at: In [205]: df Out[205]: A B C D 0 foo one -0.575247 1.346061 1 bar one 0.254161 1.511763 2 foo two -1.143704 1.627081 3 bar three 0.215897 -0.990582 4 foo two 1.193555 -0.441652 5 bar two -0.077118 1.211526 6 foo one -0.408530 0.268520 7 foo three -0.862495 0.024580 Suppose we wish to compute the standard deviation grouped by the A column. There is a slight problem, namely that we donât care about the data in column B because it is not numeric. You can avoid non-numeric columns by specifying numeric_only=True : In [206]: df . groupby ( \"A\" ) . std ( numeric_only = True ) Out[206]: C D A bar 0.181231 1.366330 foo 0.912265 0.884785 Note that df.groupby('A').colname.std(). is more efficient than df.groupby('A').std().colname . So if the result of an aggregation function is only needed over one column (here colname ), it may be filtered before applying the aggregation function. In [207]: from decimal import Decimal In [208]: df_dec = pd . DataFrame ( .....: { .....: \"id\" : [ 1 , 2 , 1 , 2 ], .....: \"int_column\" : [ 1 , 2 , 3 , 4 ], .....: \"dec_column\" : [ .....: Decimal ( \"0.50\" ), .....: Decimal ( \"0.15\" ), .....: Decimal ( \"0.25\" ), .....: Decimal ( \"0.40\" ), .....: ], .....: } .....: ) .....: In [209]: df_dec . groupby ([ \"id\" ])[[ \"dec_column\" ]] . sum () Out[209]: dec_column id 1 0.75 2 0.55 Handling of (un)observed Categorical values # When using a Categorical grouper (as a single grouper, or as part of multiple groupers), the observed keyword controls whether to return a cartesian product of all possible groupers values ( observed=False ) or only those that are observed groupers ( observed=True ). Show all values: In [210]: pd . Series ([ 1 , 1 , 1 ]) . groupby ( .....: pd . Categorical ([ \"a\" , \"a\" , \"a\" ], categories = [ \"a\" , \"b\" ]), observed = False .....: ) . count () .....: Out[210]: a 3 b 0 dtype: int64 Show only the observed values: In [211]: pd . Series ([ 1 , 1 , 1 ]) . groupby ( .....: pd . Categorical ([ \"a\" , \"a\" , \"a\" ], categories = [ \"a\" , \"b\" ]), observed = True .....: ) . count () .....: Out[211]: a 3 dtype: int64 The returned dtype of the grouped will always include all of the categories that were grouped. In [212]: s = ( .....: pd . Series ([ 1 , 1 , 1 ]) .....: . groupby ( pd . Categorical ([ \"a\" , \"a\" , \"a\" ], categories = [ \"a\" , \"b\" ]), observed = True ) .....: . count () .....: ) .....: In [213]: s . index . dtype Out[213]: CategoricalDtype(categories=['a', 'b'], ordered=False, categories_dtype=object) NA group handling # By NA , we are referring to any NA values, including NA , NaN , NaT , and None . If there are any NA values in the grouping key, by default these will be excluded. In other words, any â NA groupâ will be dropped. You can include NA groups by specifying dropna=False . In [214]: df = pd . DataFrame ({ \"key\" : [ 1.0 , 1.0 , np . nan , 2.0 , np . nan ], \"A\" : [ 1 , 2 , 3 , 4 , 5 ]}) In [215]: df Out[215]: key A 0 1.0 1 1 1.0 2 2 NaN 3 3 2.0 4 4 NaN 5 In [216]: df . groupby ( \"key\" , dropna = True ) . sum () Out[216]: A key 1.0 3 2.0 4 In [217]: df . groupby ( \"key\" , dropna = False ) . sum () Out[217]: A key 1.0 3 2.0 4 NaN 8 Grouping with ordered factors # Categorical variables represented as instances of pandasâs Categorical class can be used as group keys. If so, the order of the levels will be preserved. When observed=False and sort=False , any unobserved categories will be at the end of the result in order. In [218]: days = pd . Categorical ( .....: values = [ \"Wed\" , \"Mon\" , \"Thu\" , \"Mon\" , \"Wed\" , \"Sat\" ], .....: categories = [ \"Mon\" , \"Tue\" , \"Wed\" , \"Thu\" , \"Fri\" , \"Sat\" , \"Sun\" ], .....: ) .....: In [219]: data = pd . DataFrame ( .....: { .....: \"day\" : days , .....: \"workers\" : [ 3 , 4 , 1 , 4 , 2 , 2 ], .....: } .....: ) .....: In [220]: data Out[220]: day workers 0 Wed 3 1 Mon 4 2 Thu 1 3 Mon 4 4 Wed 2 5 Sat 2 In [221]: data . groupby ( \"day\" , observed = False , sort = True ) . sum () Out[221]: workers day Mon 8 Tue 0 Wed 5 Thu 1 Fri 0 Sat 2 Sun 0 In [222]: data . groupby ( \"day\" , observed = False , sort = False ) . sum () Out[222]: workers day Wed 5 Mon 8 Thu 1 Sat 2 Tue 0 Fri 0 Sun 0 Grouping with a grouper specification # You may need to specify a bit more data to properly group. You can use the pd.Grouper to provide this local control. In [223]: import datetime In [224]: df = pd . DataFrame ( .....: { .....: \"Branch\" : \"A A A A A A A B\" . split (), .....: \"Buyer\" : \"Carl Mark Carl Carl Joe Joe Joe Carl\" . split (), .....: \"Quantity\" : [ 1 , 3 , 5 , 1 , 8 , 1 , 9 , 3 ], .....: \"Date\" : [ .....: datetime . datetime ( 2013 , 1 , 1 , 13 , 0 ), .....: datetime . datetime ( 2013 , 1 , 1 , 13 , 5 ), .....: datetime . datetime ( 2013 , 10 , 1 , 20 , 0 ), .....: datetime . datetime ( 2013 , 10 , 2 , 10 , 0 ), .....: datetime . datetime ( 2013 , 10 , 1 , 20 , 0 ), .....: datetime . datetime ( 2013 , 10 , 2 , 10 , 0 ), .....: datetime . datetime ( 2013 , 12 , 2 , 12 , 0 ), .....: datetime . datetime ( 2013 , 12 , 2 , 14 , 0 ), .....: ], .....: } .....: ) .....: In [225]: df Out[225]: Branch Buyer Quantity Date 0 A Carl 1 2013-01-01 13:00:00 1 A Mark 3 2013-01-01 13:05:00 2 A Carl 5 2013-10-01 20:00:00 3 A Carl 1 2013-10-02 10:00:00 4 A Joe 8 2013-10-01 20:00:00 5 A Joe 1 2013-10-02 10:00:00 6 A Joe 9 2013-12-02 12:00:00 7 B Carl 3 2013-12-02 14:00:00 Groupby a specific column with the desired frequency. This is like resampling. In [226]: df . groupby ([ pd . Grouper ( freq = \"1ME\" , key = \"Date\" ), \"Buyer\" ])[[ \"Quantity\" ]] . sum () Out[226]: Quantity Date Buyer 2013-01-31 Carl 1 Mark 3 2013-10-31 Carl 6 Joe 9 2013-12-31 Carl 3 Joe 9 When freq is specified, the object returned by pd.Grouper will be an instance of pandas.api.typing.TimeGrouper . When there is a column and index with the same name, you can use key to group by the column and level to group by the index. In [227]: df = df . set_index ( \"Date\" ) In [228]: df [ \"Date\" ] = df . index + pd . offsets . MonthEnd ( 2 ) In [229]: df . groupby ([ pd . Grouper ( freq = \"6ME\" , key = \"Date\" ), \"Buyer\" ])[[ \"Quantity\" ]] . sum () Out[229]: Quantity Date Buyer 2013-02-28 Carl 1 Mark 3 2014-02-28 Carl 9 Joe 18 In [230]: df . groupby ([ pd . Grouper ( freq = \"6ME\" , level = \"Date\" ), \"Buyer\" ])[[ \"Quantity\" ]] . sum () Out[230]: Quantity Date Buyer 2013-01-31 Carl 1 Mark 3 2014-01-31 Carl 9 Joe 18 Taking the first rows of each group # Just like for a DataFrame or Series you can call head and tail on a groupby: In [231]: df = pd . DataFrame ([[ 1 , 2 ], [ 1 , 4 ], [ 5 , 6 ]], columns = [ \"A\" , \"B\" ]) In [232]: df Out[232]: A B 0 1 2 1 1 4 2 5 6 In [233]: g = df . groupby ( \"A\" ) In [234]: g . head ( 1 ) Out[234]: A B 0 1 2 2 5 6 In [235]: g . tail ( 1 ) Out[235]: A B 1 1 4 2 5 6 This shows the first or last n rows from each group. Taking the nth row of each group # To select the nth item from each group, use DataFrameGroupBy.nth() or SeriesGroupBy.nth() . Arguments supplied can be any integer, lists of integers, slices, or lists of slices; see below for examples. When the nth element of a group does not exist an error is not raised; instead no corresponding rows are returned. In general this operation acts as a filtration. In certain cases it will also return one row per group, making it also a reduction. However because in general it can return zero or multiple rows per group, pandas treats it as a filtration in all cases. In [236]: df = pd . DataFrame ([[ 1 , np . nan ], [ 1 , 4 ], [ 5 , 6 ]], columns = [ \"A\" , \"B\" ]) In [237]: g = df . groupby ( \"A\" ) In [238]: g . nth ( 0 ) Out[238]: A B 0 1 NaN 2 5 6.0 In [239]: g . nth ( - 1 ) Out[239]: A B 1 1 4.0 2 5 6.0 In [240]: g . nth ( 1 ) Out[240]: A B 1 1 4.0 If the nth element of a group does not exist, then no corresponding row is included in the result. In particular, if the specified n is larger than any group, the result will be an empty DataFrame. In [241]: g . nth ( 5 ) Out[241]: Empty DataFrame Columns: [A, B] Index: [] If you want to select the nth not-null item, use the dropna kwarg. For a DataFrame this should be either 'any' or 'all' just like you would pass to dropna: # nth(0) is the same as g.first() In [242]: g . nth ( 0 , dropna = \"any\" ) Out[242]: A B 1 1 4.0 2 5 6.0 In [243]: g . first () Out[243]: B A 1 4.0 5 6.0 # nth(-1) is the same as g.last() In [244]: g . nth ( - 1 , dropna = \"any\" ) Out[244]: A B 1 1 4.0 2 5 6.0 In [245]: g . last () Out[245]: B A 1 4.0 5 6.0 In [246]: g . B . nth ( 0 , dropna = \"all\" ) Out[246]: 1 4.0 2 6.0 Name: B, dtype: float64 You can also select multiple rows from each group by specifying multiple nth values as a list of ints. In [247]: business_dates = pd . date_range ( start = \"4/1/2014\" , end = \"6/30/2014\" , freq = \"B\" ) In [248]: df = pd . DataFrame ( 1 , index = business_dates , columns = [ \"a\" , \"b\" ]) # get the first, 4th, and last date index for each month In [249]: df . groupby ([ df . index . year , df . index . month ]) . nth ([ 0 , 3 , - 1 ]) Out[249]: a b 2014-04-01 1 1 2014-04-04 1 1 2014-04-30 1 1 2014-05-01 1 1 2014-05-06 1 1 2014-05-30 1 1 2014-06-02 1 1 2014-06-05 1 1 2014-06-30 1 1 You may also use slices or lists of slices. In [250]: df . groupby ([ df . index . year , df . index . month ]) . nth [ 1 :] Out[250]: a b 2014-04-02 1 1 2014-04-03 1 1 2014-04-04 1 1 2014-04-07 1 1 2014-04-08 1 1 ... .. .. 2014-06-24 1 1 2014-06-25 1 1 2014-06-26 1 1 2014-06-27 1 1 2014-06-30 1 1 [62 rows x 2 columns] In [251]: df . groupby ([ df . index . year , df . index . month ]) . nth [ 1 :, : - 1 ] Out[251]: a b 2014-04-01 1 1 2014-04-02 1 1 2014-04-03 1 1 2014-04-04 1 1 2014-04-07 1 1 ... .. .. 2014-06-24 1 1 2014-06-25 1 1 2014-06-26 1 1 2014-06-27 1 1 2014-06-30 1 1 [65 rows x 2 columns] Enumerate group items # To see the order in which each row appears within its group, use the cumcount method: In [252]: dfg = pd . DataFrame ( list ( \"aaabba\" ), columns = [ \"A\" ]) In [253]: dfg Out[253]: A 0 a 1 a 2 a 3 b 4 b 5 a In [254]: dfg . groupby ( \"A\" ) . cumcount () Out[254]: 0 0 1 1 2 2 3 0 4 1 5 3 dtype: int64 In [255]: dfg . groupby ( \"A\" ) . cumcount ( ascending = False ) Out[255]: 0 3 1 2 2 1 3 1 4 0 5 0 dtype: int64 Enumerate groups # To see the ordering of the groups (as opposed to the order of rows within a group given by cumcount ) you can use DataFrameGroupBy.ngroup() . Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed. In [256]: dfg = pd . DataFrame ( list ( \"aaabba\" ), columns = [ \"A\" ]) In [257]: dfg Out[257]: A 0 a 1 a 2 a 3 b 4 b 5 a In [258]: dfg . groupby ( \"A\" ) . ngroup () Out[258]: 0 0 1 0 2 0 3 1 4 1 5 0 dtype: int64 In [259]: dfg . groupby ( \"A\" ) . ngroup ( ascending = False ) Out[259]: 0 1 1 1 2 1 3 0 4 0 5 1 dtype: int64 Plotting # Groupby also works with some plotting methods. In this case, suppose we suspect that the values in column 1 are 3 times higher on average in group âBâ. In [260]: np . random . seed ( 1234 ) In [261]: df = pd . DataFrame ( np . random . randn ( 50 , 2 )) In [262]: df [ \"g\" ] = np . random . choice ([ \"A\" , \"B\" ], size = 50 ) In [263]: df . loc [ df [ \"g\" ] == \"B\" , 1 ] += 3 We can easily visualize this with a boxplot: In [264]: df . groupby ( \"g\" ) . boxplot () Out[264]: A Axes(0.1,0.15;0.363636x0.75) B Axes(0.536364,0.15;0.363636x0.75) dtype: object The result of calling boxplot is a dictionary whose keys are the values of our grouping column g (âAâ and âBâ). The values of the resulting dictionary can be controlled by the return_type keyword of boxplot . See the visualization documentation for more. Warning For historical reasons, df.groupby(\"g\").boxplot() is not equivalent to df.boxplot(by=\"g\") . See here for an explanation. Piping function calls # Similar to the functionality provided by DataFrame and Series , functions that take GroupBy objects can be chained together using a pipe method to allow for a cleaner, more readable syntax. To read about .pipe in general terms, see here . Combining .groupby and .pipe is often useful when you need to reuse GroupBy objects. As an example, imagine having a DataFrame with columns for stores, products, revenue and quantity sold. Weâd like to do a groupwise calculation of prices (i.e. revenue/quantity) per store and per product. We could do this in a multi-step operation, but expressing it in terms of piping can make the code more readable. First we set the data: In [265]: n = 1000 In [266]: df = pd . DataFrame ( .....: { .....: \"Store\" : np . random . choice ([ \"Store_1\" , \"Store_2\" ], n ), .....: \"Product\" : np . random . choice ([ \"Product_1\" , \"Product_2\" ], n ), .....: \"Revenue\" : ( np . random . random ( n ) * 50 + 10 ) . round ( 2 ), .....: \"Quantity\" : np . random . randint ( 1 , 10 , size = n ), .....: } .....: ) .....: In [267]: df . head ( 2 ) Out[267]: Store Product Revenue Quantity 0 Store_2 Product_1 26.12 1 1 Store_2 Product_1 28.86 1 We now find the prices per store/product. In [268]: ( .....: df . groupby ([ \"Store\" , \"Product\" ]) .....: . pipe ( lambda grp : grp . Revenue . sum () / grp . Quantity . sum ()) .....: . unstack () .....: . round ( 2 ) .....: ) .....: Out[268]: Product Product_1 Product_2 Store Store_1 6.82 7.05 Store_2 6.30 6.64 Piping can also be expressive when you want to deliver a grouped object to some arbitrary function, for example: In [269]: def mean ( groupby ): .....: return groupby . mean () .....: In [270]: df . groupby ([ \"Store\" , \"Product\" ]) . pipe ( mean ) Out[270]: Revenue Quantity Store Product Store_1 Product_1 34.622727 5.075758 Product_2 35.482815 5.029630 Store_2 Product_1 32.972837 5.237589 Product_2 34.684360 5.224000 Here mean takes a GroupBy object and finds the mean of the Revenue and Quantity columns respectively for each Store-Product combination. The mean function can be any function that takes in a GroupBy object; the .pipe will pass the GroupBy object as a parameter into the function you specify. Examples # Multi-column factorization # By using DataFrameGroupBy.ngroup() , we can extract information about the groups in a way similar to factorize() (as described further in the reshaping API ) but which applies naturally to multiple columns of mixed type and different sources. This can be useful as an intermediate categorical-like step in processing, when the relationships between the group rows are more important than their content, or as input to an algorithm which only accepts the integer encoding. (For more information about support in pandas for full categorical data, see the Categorical introduction and the API documentation .) In [271]: dfg = pd . DataFrame ({ \"A\" : [ 1 , 1 , 2 , 3 , 2 ], \"B\" : list ( \"aaaba\" )}) In [272]: dfg Out[272]: A B 0 1 a 1 1 a 2 2 a 3 3 b 4 2 a In [273]: dfg . groupby ([ \"A\" , \"B\" ]) . ngroup () Out[273]: 0 0 1 0 2 1 3 2 4 1 dtype: int64 In [274]: dfg . groupby ([ \"A\" , [ 0 , 0 , 0 , 1 , 1 ]]) . ngroup () Out[274]: 0 0 1 0 2 1 3 3 4 2 dtype: int64 Groupby by indexer to âresampleâ data # Resampling produces new hypothetical samples (resamples) from already existing observed data or from a model that generates data. These new samples are similar to the pre-existing samples. In order for resample to work on indices that are non-datetimelike, the following procedure can be utilized. In the following examples, df.index // 5 returns an integer array which is used to determine what gets selected for the groupby operation. Note The example below shows how we can downsample by consolidation of samples into fewer ones. Here by using df.index // 5 , we are aggregating the samples in bins. By applying std() function, we aggregate the information contained in many samples into a small subset of values which is their standard deviation thereby reducing the number of samples. In [275]: df = pd . DataFrame ( np . random . randn ( 10 , 2 )) In [276]: df Out[276]: 0 1 0 -0.793893 0.321153 1 0.342250 1.618906 2 -0.975807 1.918201 3 -0.810847 -1.405919 4 -1.977759 0.461659 5 0.730057 -1.316938 6 -0.751328 0.528290 7 -0.257759 -1.081009 8 0.505895 -1.701948 9 -1.006349 0.020208 In [277]: df . index // 5 Out[277]: Index([0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype='int64') In [278]: df . groupby ( df . index // 5 ) . std () Out[278]: 0 1 0 0.823647 1.312912 1 0.760109 0.942941 Returning a Series to propagate names # Group DataFrame columns, compute a set of metrics and return a named Series. The Series name is used as the name for the column index. This is especially useful in conjunction with reshaping operations such as stacking, in which the column index name will be used as the name of the inserted column: In [279]: df = pd . DataFrame ( .....: { .....: \"a\" : [ 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 ], .....: \"b\" : [ 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 ], .....: \"c\" : [ 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 ], .....: \"d\" : [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 ], .....: } .....: ) .....: In [280]: def compute_metrics ( x ): .....: result = { \"b_sum\" : x [ \"b\" ] . sum (), \"c_mean\" : x [ \"c\" ] . mean ()} .....: return pd . Series ( result , name = \"metrics\" ) .....: In [281]: result = df . groupby ( \"a\" ) . apply ( compute_metrics , include_groups = False ) In [282]: result Out[282]: metrics b_sum c_mean a 0 2.0 0.5 1 2.0 0.5 2 2.0 0.5 In [283]: result . stack ( future_stack = True ) Out[283]: a metrics 0 b_sum 2.0 c_mean 0.5 1 b_sum 2.0 c_mean 0.5 2 b_sum 2.0 c_mean 0.5 dtype: float64 previous Table Visualization next Windowing operations On this page Splitting an object into groups GroupBy sorting GroupBy dropna GroupBy object attributes GroupBy with MultiIndex Grouping DataFrame with Index levels and columns DataFrame column selection in GroupBy Iterating through groups Selecting a group Aggregation Built-in aggregation methods The aggregate() method Aggregation with User-Defined Functions Applying multiple functions at once Named aggregation Applying different functions to DataFrame columns Transformation Built-in transformation methods The transform() method Window and resample operations Filtration Built-in filtrations The filter method Flexible apply Control grouped column(s) placement with group_keys Numba Accelerated Routines Other useful features Exclusion of non-numeric columns Handling of (un)observed Categorical values NA group handling Grouping with ordered factors Grouping with a grouper specification Taking the first rows of each group Taking the nth row of each group Enumerate group items Enumerate groups Plotting Piping function calls Examples Multi-column factorization Groupby by indexer to âresampleâ data Returning a Series to propagate names Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/groupby.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Extensions pandas.api.e... pandas.api.extensions.register_index_accessor # pandas.api.extensions. register_index_accessor ( name ) [source] # Register a custom accessor on Index objects. Parameters : name str Name under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute. Returns : callable A class decorator. See also register_dataframe_accessor Register a custom accessor on DataFrame objects. register_series_accessor Register a custom accessor on Series objects. register_index_accessor Register a custom accessor on Index objects. Notes When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be def __init__ ( self , pandas_object ): # noqa: E999 ... For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype. >>> pd . Series ([ 'a' , 'b' ]) . dt Traceback (most recent call last): ... AttributeError : Can only use .dt accessor with datetimelike values Examples In your library code: import pandas as pd @pd . api . extensions . register_dataframe_accessor ( \"geo\" ) class GeoAccessor : def __init__ ( self , pandas_obj ): self . _obj = pandas_obj @property def center ( self ): # return the geographic center point of this DataFrame lat = self . _obj . latitude lon = self . _obj . longitude return ( float ( lon . mean ()), float ( lat . mean ())) def plot ( self ): # plot this array's data on a map, e.g., using Cartopy pass Back in an interactive IPython session: In [1]: ds = pd . DataFrame ({ \"longitude\" : np . linspace ( 0 , 10 ), ...: \"latitude\" : np . linspace ( 0 , 20 )}) In [2]: ds . geo . center Out[2]: (5.0, 10.0) In [3]: ds . geo . plot () # plots data on a map previous pandas.api.extensions.register_series_accessor next pandas.api.extensions.ExtensionDtype On this page register_index_accessor() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_index_accessor.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Date offsets pandas.tseri... pandas.tseries.frequencies.to_offset # pandas.tseries.frequencies. to_offset ( freq , is_period = False ) # Return DateOffset object from string or datetime.timedelta object. Parameters : freq str, datetime.timedelta, BaseOffset or None Returns : BaseOffset subclass or None Raises : ValueError If freq is an invalid frequency See also BaseOffset Standard kind of date increment used for a date range. Examples >>> from pandas.tseries.frequencies import to_offset >>> to_offset ( \"5min\" ) <5 * Minutes> >>> to_offset ( \"1D1h\" ) <25 * Hours> >>> to_offset ( \"2W\" ) <2 * Weeks: weekday=6> >>> to_offset ( \"2B\" ) <2 * BusinessDays> >>> to_offset ( pd . Timedelta ( days = 1 )) <Day> >>> to_offset ( pd . offsets . Hour ()) <Hour> previous pandas.tseries.offsets.Nano.is_year_end next Window On this page to_offset() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.tseries.frequencies.to_offset.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Essential... Essential basic functionality # Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, letâs create some example objects like we did in the 10 minutes to pandas section: In [1]: index = pd . date_range ( \"1/1/2000\" , periods = 8 ) In [2]: s = pd . Series ( np . random . randn ( 5 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) In [3]: df = pd . DataFrame ( np . random . randn ( 8 , 3 ), index = index , columns = [ \"A\" , \"B\" , \"C\" ]) Head and tail # To view a small sample of a Series or DataFrame object, use the head() and tail() methods. The default number of elements to display is five, but you may pass a custom number. In [4]: long_series = pd . Series ( np . random . randn ( 1000 )) In [5]: long_series . head () Out[5]: 0 -1.157892 1 -1.344312 2 0.844885 3 1.075770 4 -0.109050 dtype: float64 In [6]: long_series . tail ( 3 ) Out[6]: 997 -0.289388 998 -1.020544 999 0.589993 dtype: float64 Attributes and underlying data # pandas objects have a number of attributes enabling you to access the metadata shape : gives the axis dimensions of the object, consistent with ndarray Axis labels Series : index (only axis) DataFrame : index (rows) and columns Note, these attributes can be safely assigned to ! In [7]: df [: 2 ] Out[7]: A B C 2000-01-01 -0.173215 0.119209 -1.044236 2000-01-02 -0.861849 -2.104569 -0.494929 In [8]: df . columns = [ x . lower () for x in df . columns ] In [9]: df Out[9]: a b c 2000-01-01 -0.173215 0.119209 -1.044236 2000-01-02 -0.861849 -2.104569 -0.494929 2000-01-03 1.071804 0.721555 -0.706771 2000-01-04 -1.039575 0.271860 -0.424972 2000-01-05 0.567020 0.276232 -1.087401 2000-01-06 -0.673690 0.113648 -1.478427 2000-01-07 0.524988 0.404705 0.577046 2000-01-08 -1.715002 -1.039268 -0.370647 pandas objects ( Index , Series , DataFrame ) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a numpy.ndarray . However, pandas and 3rd party libraries may extend NumPyâs type system to add support for custom arrays (see dtypes ). To get the actual data inside a Index or Series , use the .array property In [10]: s . array Out[10]: <NumpyExtensionArray> [ 0.4691122999071863, -0.2828633443286633, -1.5090585031735124, -1.1356323710171934, 1.2121120250208506] Length: 5, dtype: float64 In [11]: s . index . array Out[11]: <NumpyExtensionArray> ['a', 'b', 'c', 'd', 'e'] Length: 5, dtype: object array will always be an ExtensionArray . The exact details of what an ExtensionArray is and why pandas uses them are a bit beyond the scope of this introduction. See dtypes for more. If you know you need a NumPy array, use to_numpy() or numpy.asarray() . In [12]: s . to_numpy () Out[12]: array([ 0.4691, -0.2829, -1.5091, -1.1356, 1.2121]) In [13]: np . asarray ( s ) Out[13]: array([ 0.4691, -0.2829, -1.5091, -1.1356, 1.2121]) When the Series or Index is backed by an ExtensionArray , to_numpy() may involve copying data and coercing values. See dtypes for more. to_numpy() gives some control over the dtype of the resulting numpy.ndarray . For example, consider datetimes with timezones. NumPy doesnât have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations: An object-dtype numpy.ndarray with Timestamp objects, each with the correct tz A datetime64[ns] -dtype numpy.ndarray , where the values have been converted to UTC and the timezone discarded Timezones may be preserved with dtype=object In [14]: ser = pd . Series ( pd . date_range ( \"2000\" , periods = 2 , tz = \"CET\" )) In [15]: ser . to_numpy ( dtype = object ) Out[15]: array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'), Timestamp('2000-01-02 00:00:00+0100', tz='CET')], dtype=object) Or thrown away with dtype='datetime64[ns]' In [16]: ser . to_numpy ( dtype = \"datetime64[ns]\" ) Out[16]: array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'], dtype='datetime64[ns]') Getting the âraw dataâ inside a DataFrame is possibly a bit more complex. When your DataFrame only has a single data type for all the columns, DataFrame.to_numpy() will return the underlying data: In [17]: df . to_numpy () Out[17]: array([[-0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949], [ 1.0718, 0.7216, -0.7068], [-1.0396, 0.2719, -0.425 ], [ 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784], [ 0.525 , 0.4047, 0.577 ], [-1.715 , -1.0393, -0.3706]]) If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrameâs columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to. Note When working with heterogeneous data, the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype. In the past, pandas recommended Series.values or DataFrame.values for extracting the data from a Series or DataFrame. Youâll still find references to these in old code bases and online. Going forward, we recommend avoiding .values and using .array or .to_numpy() . .values has the following drawbacks: When your Series contains an extension type , itâs unclear whether Series.values returns a NumPy array or the extension array. Series.array will always return an ExtensionArray , and will never copy data. Series.to_numpy() will always return a NumPy array, potentially at the cost of copying / coercing values. When your DataFrame contains a mixture of data types, DataFrame.values may involve copying data and coercing values to a common dtype, a relatively expensive operation. DataFrame.to_numpy() , being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame. Accelerated operations # pandas has support for accelerating certain types of binary numerical and boolean operations using the numexpr library and the bottleneck libraries. These libraries are especially useful when dealing with large data sets, and provide large speedups. numexpr uses smart chunking, caching, and multiple cores. bottleneck is a set of specialized cython routines that are especially fast when dealing with arrays that have nans . Here is a sample (using 100 column x 100,000 row DataFrames ): Operation 0.11.0 (ms) Prior Version (ms) Ratio to Prior df1 > df2 13.32 125.35 0.1063 df1 * df2 21.71 36.63 0.5928 df1 + df2 22.04 36.50 0.6039 You are highly encouraged to install both libraries. See the section Recommended Dependencies for more installation info. These are both enabled to be used by default, you can control this by setting the options: pd . set_option ( \"compute.use_bottleneck\" , False ) pd . set_option ( \"compute.use_numexpr\" , False ) Flexible binary operations # With binary operations between pandas data structures, there are two key points of interest: Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects. Missing data in computations. We will demonstrate how to manage these issues independently, though they can be handled simultaneously. Matching / broadcasting behavior # DataFrame has the methods add() , sub() , mul() , div() and related functions radd() , rsub() , â¦ for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword: In [18]: df = pd . DataFrame ( ....: { ....: \"one\" : pd . Series ( np . random . randn ( 3 ), index = [ \"a\" , \"b\" , \"c\" ]), ....: \"two\" : pd . Series ( np . random . randn ( 4 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" ]), ....: \"three\" : pd . Series ( np . random . randn ( 3 ), index = [ \"b\" , \"c\" , \"d\" ]), ....: } ....: ) ....: In [19]: df Out[19]: one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [20]: row = df . iloc [ 1 ] In [21]: column = df [ \"two\" ] In [22]: df . sub ( row , axis = \"columns\" ) Out[22]: one two three a 1.051928 -0.139606 NaN b 0.000000 0.000000 0.000000 c 0.352192 -0.433754 1.277825 d NaN -1.632779 -0.562782 In [23]: df . sub ( row , axis = 1 ) Out[23]: one two three a 1.051928 -0.139606 NaN b 0.000000 0.000000 0.000000 c 0.352192 -0.433754 1.277825 d NaN -1.632779 -0.562782 In [24]: df . sub ( column , axis = \"index\" ) Out[24]: one two three a -0.377535 0.0 NaN b -1.569069 0.0 -1.962513 c -0.783123 0.0 -0.250933 d NaN 0.0 -0.892516 In [25]: df . sub ( column , axis = 0 ) Out[25]: one two three a -0.377535 0.0 NaN b -1.569069 0.0 -1.962513 c -0.783123 0.0 -0.250933 d NaN 0.0 -0.892516 Furthermore you can align a level of a MultiIndexed DataFrame with a Series. In [26]: dfmi = df . copy () In [27]: dfmi . index = pd . MultiIndex . from_tuples ( ....: [( 1 , \"a\" ), ( 1 , \"b\" ), ( 1 , \"c\" ), ( 2 , \"a\" )], names = [ \"first\" , \"second\" ] ....: ) ....: In [28]: dfmi . sub ( column , axis = 0 , level = \"second\" ) Out[28]: one two three first second 1 a -0.377535 0.000000 NaN b -1.569069 0.000000 -1.962513 c -0.783123 0.000000 -0.250933 2 a NaN -1.493173 -2.385688 Series and Index also support the divmod() builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example: In [29]: s = pd . Series ( np . arange ( 10 )) In [30]: s Out[30]: 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 dtype: int64 In [31]: div , rem = divmod ( s , 3 ) In [32]: div Out[32]: 0 0 1 0 2 0 3 1 4 1 5 1 6 2 7 2 8 2 9 3 dtype: int64 In [33]: rem Out[33]: 0 0 1 1 2 2 3 0 4 1 5 2 6 0 7 1 8 2 9 0 dtype: int64 In [34]: idx = pd . Index ( np . arange ( 10 )) In [35]: idx Out[35]: Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64') In [36]: div , rem = divmod ( idx , 3 ) In [37]: div Out[37]: Index([0, 0, 0, 1, 1, 1, 2, 2, 2, 3], dtype='int64') In [38]: rem Out[38]: Index([0, 1, 2, 0, 1, 2, 0, 1, 2, 0], dtype='int64') We can also do elementwise divmod() : In [39]: div , rem = divmod ( s , [ 2 , 2 , 3 , 3 , 4 , 4 , 5 , 5 , 6 , 6 ]) In [40]: div Out[40]: 0 0 1 0 2 0 3 1 4 1 5 1 6 1 7 1 8 1 9 1 dtype: int64 In [41]: rem Out[41]: 0 0 1 1 2 2 3 0 4 0 5 1 6 1 7 2 8 2 9 3 dtype: int64 Missing data / operations with fill values # In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value , namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using fillna if you wish). In [42]: df2 = df . copy () In [43]: df2 . loc [ \"a\" , \"three\" ] = 1.0 In [44]: df Out[44]: one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [45]: df2 Out[45]: one two three a 1.394981 1.772517 1.000000 b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [46]: df + df2 Out[46]: one two three a 2.789963 3.545034 NaN b 0.686107 3.824246 -0.100780 c 1.390491 2.956737 2.454870 d NaN 0.558688 -1.226343 In [47]: df . add ( df2 , fill_value = 0 ) Out[47]: one two three a 2.789963 3.545034 1.000000 b 0.686107 3.824246 -0.100780 c 1.390491 2.956737 2.454870 d NaN 0.558688 -1.226343 Flexible comparisons # Series and DataFrame have the binary comparison methods eq , ne , lt , gt , le , and ge whose behavior is analogous to the binary arithmetic operations described above: In [48]: df . gt ( df2 ) Out[48]: one two three a False False False b False False False c False False False d False False False In [49]: df2 . ne ( df ) Out[49]: one two three a False False True b False False False c False False False d True False False These operations produce a pandas object of the same type as the left-hand-side input that is of dtype bool . These boolean objects can be used in indexing operations, see the section on Boolean indexing . Boolean reductions # You can apply the reductions: empty , any() , all() , and bool() to provide a way to summarize a boolean result. In [50]: ( df > 0 ) . all () Out[50]: one False two True three False dtype: bool In [51]: ( df > 0 ) . any () Out[51]: one True two True three True dtype: bool You can reduce to a final boolean value. In [52]: ( df > 0 ) . any () . any () Out[52]: True You can test if a pandas object is empty, via the empty property. In [53]: df . empty Out[53]: False In [54]: pd . DataFrame ( columns = list ( \"ABC\" )) . empty Out[54]: True Warning Asserting the truthiness of a pandas object will raise an error, as the testing of the emptiness or values is ambiguous. In [55]: if df : ....: print ( True ) ....: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-55-318d08b2571a> in ? () ----> 1 if df : 2 print ( True ) ~/work/pandas/pandas/pandas/core/generic.py in ? (self) 1578 @final 1579 def __nonzero__ ( self ) -> NoReturn : -> 1580 raise ValueError ( 1581 f \"The truth value of a { type ( self ) . __name__ } is ambiguous. \" 1582 \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\" 1583 ) ValueError : The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [56]: df and df2 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-56-b241b64bb471> in ? () ----> 1 df and df2 ~/work/pandas/pandas/pandas/core/generic.py in ? (self) 1578 @final 1579 def __nonzero__ ( self ) -> NoReturn : -> 1580 raise ValueError ( 1581 f \"The truth value of a { type ( self ) . __name__ } is ambiguous. \" 1582 \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\" 1583 ) ValueError : The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). See gotchas for a more detailed discussion. Comparing if objects are equivalent # Often you may find that there is more than one way to compute the same result. As a simple example, consider df + df and df * 2 . To test that these two computations produce the same result, given the tools shown above, you might imagine using (df + df == df * 2).all() . But in fact, this expression is False: In [57]: df + df == df * 2 Out[57]: one two three a True True False b True True True c True True True d False True True In [58]: ( df + df == df * 2 ) . all () Out[58]: one False two True three False dtype: bool Notice that the boolean DataFrame df + df == df * 2 contains some False values! This is because NaNs do not compare as equals: In [59]: np . nan == np . nan Out[59]: False So, NDFrames (such as Series and DataFrames) have an equals() method for testing equality, with NaNs in corresponding locations treated as equal. In [60]: ( df + df ) . equals ( df * 2 ) Out[60]: True Note that the Series or DataFrame index needs to be in the same order for equality to be True: In [61]: df1 = pd . DataFrame ({ \"col\" : [ \"foo\" , 0 , np . nan ]}) In [62]: df2 = pd . DataFrame ({ \"col\" : [ np . nan , 0 , \"foo\" ]}, index = [ 2 , 1 , 0 ]) In [63]: df1 . equals ( df2 ) Out[63]: False In [64]: df1 . equals ( df2 . sort_index ()) Out[64]: True Comparing array-like objects # You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value: In [65]: pd . Series ([ \"foo\" , \"bar\" , \"baz\" ]) == \"foo\" Out[65]: 0 True 1 False 2 False dtype: bool In [66]: pd . Index ([ \"foo\" , \"bar\" , \"baz\" ]) == \"foo\" Out[66]: array([ True, False, False]) pandas also handles element-wise comparisons between different array-like objects of the same length: In [67]: pd . Series ([ \"foo\" , \"bar\" , \"baz\" ]) == pd . Index ([ \"foo\" , \"bar\" , \"qux\" ]) Out[67]: 0 True 1 True 2 False dtype: bool In [68]: pd . Series ([ \"foo\" , \"bar\" , \"baz\" ]) == np . array ([ \"foo\" , \"bar\" , \"qux\" ]) Out[68]: 0 True 1 True 2 False dtype: bool Trying to compare Index or Series objects of different lengths will raise a ValueError: In [69]: pd . Series ([ 'foo' , 'bar' , 'baz' ]) == pd . Series ([ 'foo' , 'bar' ]) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [ 69 ], line 1 ----> 1 pd . Series ([ 'foo' , 'bar' , 'baz' ]) == pd . Series ([ 'foo' , 'bar' ]) File ~/work/pandas/pandas/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method (self, other) 72 return NotImplemented 74 other = item_from_zerodim ( other ) ---> 76 return method ( self , other ) File ~/work/pandas/pandas/pandas/core/arraylike.py:40, in OpsMixin.__eq__ (self, other) 38 @unpack_zerodim_and_defer ( \"__eq__\" ) 39 def __eq__ ( self , other ): ---> 40 return self . _cmp_method ( other , operator . eq ) File ~/work/pandas/pandas/pandas/core/series.py:6133, in Series._cmp_method (self, other, op) 6130 res_name = ops . get_op_result_name ( self , other ) 6132 if isinstance ( other , Series ) and not self . _indexed_same ( other ): -> 6133 raise ValueError ( \"Can only compare identically-labeled Series objects\" ) 6135 lvalues = self . _values 6136 rvalues = extract_array ( other , extract_numpy = True , extract_range = True ) ValueError : Can only compare identically-labeled Series objects In [70]: pd . Series ([ 'foo' , 'bar' , 'baz' ]) == pd . Series ([ 'foo' ]) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [ 70 ], line 1 ----> 1 pd . Series ([ 'foo' , 'bar' , 'baz' ]) == pd . Series ([ 'foo' ]) File ~/work/pandas/pandas/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method (self, other) 72 return NotImplemented 74 other = item_from_zerodim ( other ) ---> 76 return method ( self , other ) File ~/work/pandas/pandas/pandas/core/arraylike.py:40, in OpsMixin.__eq__ (self, other) 38 @unpack_zerodim_and_defer ( \"__eq__\" ) 39 def __eq__ ( self , other ): ---> 40 return self . _cmp_method ( other , operator . eq ) File ~/work/pandas/pandas/pandas/core/series.py:6133, in Series._cmp_method (self, other, op) 6130 res_name = ops . get_op_result_name ( self , other ) 6132 if isinstance ( other , Series ) and not self . _indexed_same ( other ): -> 6133 raise ValueError ( \"Can only compare identically-labeled Series objects\" ) 6135 lvalues = self . _values 6136 rvalues = extract_array ( other , extract_numpy = True , extract_range = True ) ValueError : Can only compare identically-labeled Series objects Combining overlapping data sets # A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of âhigher qualityâ. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is combine_first() , which we illustrate: In [71]: df1 = pd . DataFrame ( ....: { \"A\" : [ 1.0 , np . nan , 3.0 , 5.0 , np . nan ], \"B\" : [ np . nan , 2.0 , 3.0 , np . nan , 6.0 ]} ....: ) ....: In [72]: df2 = pd . DataFrame ( ....: { ....: \"A\" : [ 5.0 , 2.0 , 4.0 , np . nan , 3.0 , 7.0 ], ....: \"B\" : [ np . nan , np . nan , 3.0 , 4.0 , 6.0 , 8.0 ], ....: } ....: ) ....: In [73]: df1 Out[73]: A B 0 1.0 NaN 1 NaN 2.0 2 3.0 3.0 3 5.0 NaN 4 NaN 6.0 In [74]: df2 Out[74]: A B 0 5.0 NaN 1 2.0 NaN 2 4.0 3.0 3 NaN 4.0 4 3.0 6.0 5 7.0 8.0 In [75]: df1 . combine_first ( df2 ) Out[75]: A B 0 1.0 NaN 1 2.0 2.0 2 3.0 3.0 3 5.0 4.0 4 3.0 6.0 5 7.0 8.0 General DataFrame combine # The combine_first() method above calls the more general DataFrame.combine() . This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same). So, for instance, to reproduce combine_first() as above: In [76]: def combiner ( x , y ): ....: return np . where ( pd . isna ( x ), y , x ) ....: In [77]: df1 . combine ( df2 , combiner ) Out[77]: A B 0 1.0 NaN 1 2.0 2.0 2 3.0 3.0 3 5.0 4.0 4 3.0 6.0 5 7.0 8.0 Descriptive statistics # There exists a large number of methods for computing descriptive statistics and other related operations on Series , DataFrame . Most of these are aggregations (hence producing a lower-dimensional result) like sum() , mean() , and quantile() , but some of them, like cumsum() and cumprod() , produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, â¦} , but the axis can be specified by name or integer: Series : no axis argument needed DataFrame : âindexâ (axis=0, default), âcolumnsâ (axis=1) For example: In [78]: df Out[78]: one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [79]: df . mean ( 0 ) Out[79]: one 0.811094 two 1.360588 three 0.187958 dtype: float64 In [80]: df . mean ( 1 ) Out[80]: a 1.583749 b 0.734929 c 1.133683 d -0.166914 dtype: float64 All such methods have a skipna option signaling whether to exclude missing data ( True by default): In [81]: df . sum ( 0 , skipna = False ) Out[81]: one NaN two 5.442353 three NaN dtype: float64 In [82]: df . sum ( axis = 1 , skipna = True ) Out[82]: a 3.167498 b 2.204786 c 3.401050 d -0.333828 dtype: float64 Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standardization (rendering data zero mean and standard deviation of 1), very concisely: In [83]: ts_stand = ( df - df . mean ()) / df . std () In [84]: ts_stand . std () Out[84]: one 1.0 two 1.0 three 1.0 dtype: float64 In [85]: xs_stand = df . sub ( df . mean ( 1 ), axis = 0 ) . div ( df . std ( 1 ), axis = 0 ) In [86]: xs_stand . std ( 1 ) Out[86]: a 1.0 b 1.0 c 1.0 d 1.0 dtype: float64 Note that methods like cumsum() and cumprod() preserve the location of NaN values. This is somewhat different from expanding() and rolling() since NaN behavior is furthermore dictated by a min_periods parameter. In [87]: df . cumsum () Out[87]: one two three a 1.394981 1.772517 NaN b 1.738035 3.684640 -0.050390 c 2.433281 5.163008 1.177045 d NaN 5.442353 0.563873 Here is a quick reference summary table of common functions. Each also takes an optional level parameter which applies only if the object has a hierarchical index . Function Description count Number of non-NA observations sum Sum of values mean Mean of values median Arithmetic median of values min Minimum max Maximum mode Mode abs Absolute Value prod Product of values std Bessel-corrected sample standard deviation var Unbiased variance sem Standard error of the mean skew Sample skewness (3rd moment) kurt Sample kurtosis (4th moment) quantile Sample quantile (value at %) cumsum Cumulative sum cumprod Cumulative product cummax Cumulative maximum cummin Cumulative minimum Note that by chance some NumPy methods, like mean , std , and sum , will exclude NAs on Series input by default: In [88]: np . mean ( df [ \"one\" ]) Out[88]: 0.8110935116651192 In [89]: np . mean ( df [ \"one\" ] . to_numpy ()) Out[89]: nan Series.nunique() will return the number of unique non-NA values in a Series: In [90]: series = pd . Series ( np . random . randn ( 500 )) In [91]: series [ 20 : 500 ] = np . nan In [92]: series [ 10 : 20 ] = 5 In [93]: series . nunique () Out[93]: 11 Summarizing data: describe # There is a convenient describe() function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course): In [94]: series = pd . Series ( np . random . randn ( 1000 )) In [95]: series [:: 2 ] = np . nan In [96]: series . describe () Out[96]: count 500.000000 mean -0.021292 std 1.015906 min -2.683763 25% -0.699070 50% -0.069718 75% 0.714483 max 3.160915 dtype: float64 In [97]: frame = pd . DataFrame ( np . random . randn ( 1000 , 5 ), columns = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) In [98]: frame . iloc [:: 2 ] = np . nan In [99]: frame . describe () Out[99]: a b c d e count 500.000000 500.000000 500.000000 500.000000 500.000000 mean 0.033387 0.030045 -0.043719 -0.051686 0.005979 std 1.017152 0.978743 1.025270 1.015988 1.006695 min -3.000951 -2.637901 -3.303099 -3.159200 -3.188821 25% -0.647623 -0.576449 -0.712369 -0.691338 -0.691115 50% 0.047578 -0.021499 -0.023888 -0.032652 -0.025363 75% 0.729907 0.775880 0.618896 0.670047 0.649748 max 2.740139 2.752332 3.004229 2.728702 3.240991 You can select specific percentiles to include in the output: In [100]: series . describe ( percentiles = [ 0.05 , 0.25 , 0.75 , 0.95 ]) Out[100]: count 500.000000 mean -0.021292 std 1.015906 min -2.683763 5% -1.645423 25% -0.699070 50% -0.069718 75% 0.714483 95% 1.711409 max 3.160915 dtype: float64 By default, the median is always included. For a non-numerical Series object, describe() will give a simple summary of the number of unique values and most frequently occurring values: In [101]: s = pd . Series ([ \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"a\" , np . nan , \"c\" , \"d\" , \"a\" ]) In [102]: s . describe () Out[102]: count 9 unique 4 top a freq 5 dtype: object Note that on a mixed-type DataFrame object, describe() will restrict the summary to include only numerical columns or, if none are, only categorical columns: In [103]: frame = pd . DataFrame ({ \"a\" : [ \"Yes\" , \"Yes\" , \"No\" , \"No\" ], \"b\" : range ( 4 )}) In [104]: frame . describe () Out[104]: b count 4.000000 mean 1.500000 std 1.290994 min 0.000000 25% 0.750000 50% 1.500000 75% 2.250000 max 3.000000 This behavior can be controlled by providing a list of types as include / exclude arguments. The special value all can also be used: In [105]: frame . describe ( include = [ \"object\" ]) Out[105]: a count 4 unique 2 top Yes freq 2 In [106]: frame . describe ( include = [ \"number\" ]) Out[106]: b count 4.000000 mean 1.500000 std 1.290994 min 0.000000 25% 0.750000 50% 1.500000 75% 2.250000 max 3.000000 In [107]: frame . describe ( include = \"all\" ) Out[107]: a b count 4 4.000000 unique 2 NaN top Yes NaN freq 2 NaN mean NaN 1.500000 std NaN 1.290994 min NaN 0.000000 25% NaN 0.750000 50% NaN 1.500000 75% NaN 2.250000 max NaN 3.000000 That feature relies on select_dtypes . Refer to there for details about accepted inputs. Index of min/max values # The idxmin() and idxmax() functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values: In [108]: s1 = pd . Series ( np . random . randn ( 5 )) In [109]: s1 Out[109]: 0 1.118076 1 -0.352051 2 -1.242883 3 -1.277155 4 -0.641184 dtype: float64 In [110]: s1 . idxmin (), s1 . idxmax () Out[110]: (3, 0) In [111]: df1 = pd . DataFrame ( np . random . randn ( 5 , 3 ), columns = [ \"A\" , \"B\" , \"C\" ]) In [112]: df1 Out[112]: A B C 0 -0.327863 -0.946180 -0.137570 1 -0.186235 -0.257213 -0.486567 2 -0.507027 -0.871259 -0.111110 3 2.000339 -2.430505 0.089759 4 -0.321434 -0.033695 0.096271 In [113]: df1 . idxmin ( axis = 0 ) Out[113]: A 2 B 3 C 1 dtype: int64 In [114]: df1 . idxmax ( axis = 1 ) Out[114]: 0 C 1 A 2 C 3 A 4 C dtype: object When there are multiple rows (or columns) matching the minimum or maximum value, idxmin() and idxmax() return the first matching index: In [115]: df3 = pd . DataFrame ([ 2 , 1 , 1 , 3 , np . nan ], columns = [ \"A\" ], index = list ( \"edcba\" )) In [116]: df3 Out[116]: A e 2.0 d 1.0 c 1.0 b 3.0 a NaN In [117]: df3 [ \"A\" ] . idxmin () Out[117]: 'd' Note idxmin and idxmax are called argmin and argmax in NumPy. Value counts (histogramming) / mode # The value_counts() Series method computes a histogram of a 1D array of values. It can also be used as a function on regular arrays: In [118]: data = np . random . randint ( 0 , 7 , size = 50 ) In [119]: data Out[119]: array([6, 6, 2, 3, 5, 3, 2, 5, 4, 5, 4, 3, 4, 5, 0, 2, 0, 4, 2, 0, 3, 2, 2, 5, 6, 5, 3, 4, 6, 4, 3, 5, 6, 4, 3, 6, 2, 6, 6, 2, 3, 4, 2, 1, 6, 2, 6, 1, 5, 4]) In [120]: s = pd . Series ( data ) In [121]: s . value_counts () Out[121]: 6 10 2 10 4 9 3 8 5 8 0 3 1 2 Name: count, dtype: int64 The value_counts() method can be used to count combinations across multiple columns. By default all columns are used but a subset can be selected using the subset argument. In [122]: data = { \"a\" : [ 1 , 2 , 3 , 4 ], \"b\" : [ \"x\" , \"x\" , \"y\" , \"y\" ]} In [123]: frame = pd . DataFrame ( data ) In [124]: frame . value_counts () Out[124]: a b 1 x 1 2 x 1 3 y 1 4 y 1 Name: count, dtype: int64 Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame: In [125]: s5 = pd . Series ([ 1 , 1 , 3 , 3 , 3 , 5 , 5 , 7 , 7 , 7 ]) In [126]: s5 . mode () Out[126]: 0 3 1 7 dtype: int64 In [127]: df5 = pd . DataFrame ( .....: { .....: \"A\" : np . random . randint ( 0 , 7 , size = 50 ), .....: \"B\" : np . random . randint ( - 10 , 15 , size = 50 ), .....: } .....: ) .....: In [128]: df5 . mode () Out[128]: A B 0 1.0 -9 1 NaN 10 2 NaN 13 Discretization and quantiling # Continuous values can be discretized using the cut() (bins based on values) and qcut() (bins based on sample quantiles) functions: In [129]: arr = np . random . randn ( 20 ) In [130]: factor = pd . cut ( arr , 4 ) In [131]: factor Out[131]: [(-0.251, 0.464], (-0.968, -0.251], (0.464, 1.179], (-0.251, 0.464], (-0.968, -0.251], ..., (-0.251, 0.464], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251]] Length: 20 Categories (4, interval[float64, right]): [(-0.968, -0.251] < (-0.251, 0.464] < (0.464, 1.179] < (1.179, 1.893]] In [132]: factor = pd . cut ( arr , [ - 5 , - 1 , 0 , 1 , 5 ]) In [133]: factor Out[133]: [(0, 1], (-1, 0], (0, 1], (0, 1], (-1, 0], ..., (-1, 0], (-1, 0], (-1, 0], (-1, 0], (-1, 0]] Length: 20 Categories (4, interval[int64, right]): [(-5, -1] < (-1, 0] < (0, 1] < (1, 5]] qcut() computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so: In [134]: arr = np . random . randn ( 30 ) In [135]: factor = pd . qcut ( arr , [ 0 , 0.25 , 0.5 , 0.75 , 1 ]) In [136]: factor Out[136]: [(0.569, 1.184], (-2.278, -0.301], (-2.278, -0.301], (0.569, 1.184], (0.569, 1.184], ..., (-0.301, 0.569], (1.184, 2.346], (1.184, 2.346], (-0.301, 0.569], (-2.278, -0.301]] Length: 30 Categories (4, interval[float64, right]): [(-2.278, -0.301] < (-0.301, 0.569] < (0.569, 1.184] < (1.184, 2.346]] We can also pass infinite values to define the bins: In [137]: arr = np . random . randn ( 20 ) In [138]: factor = pd . cut ( arr , [ - np . inf , 0 , np . inf ]) In [139]: factor Out[139]: [(-inf, 0.0], (0.0, inf], (0.0, inf], (-inf, 0.0], (-inf, 0.0], ..., (-inf, 0.0], (-inf, 0.0], (-inf, 0.0], (0.0, inf], (0.0, inf]] Length: 20 Categories (2, interval[float64, right]): [(-inf, 0.0] < (0.0, inf]] Function application # To apply your own or another libraryâs functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame or Series , row- or column-wise, or elementwise. Tablewise Function Application : pipe() Row or Column-wise Function Application : apply() Aggregation API : agg() and transform() Applying Elementwise Functions : map() Tablewise function application # DataFrames and Series can be passed into functions. However, if the function needs to be called in a chain, consider using the pipe() method. First some setup: In [140]: def extract_city_name ( df ): .....: \"\"\" .....: Chicago, IL -> Chicago for city_name column .....: \"\"\" .....: df [ \"city_name\" ] = df [ \"city_and_code\" ] . str . split ( \",\" ) . str . get ( 0 ) .....: return df .....: In [141]: def add_country_name ( df , country_name = None ): .....: \"\"\" .....: Chicago -> Chicago-US for city_name column .....: \"\"\" .....: col = \"city_name\" .....: df [ \"city_and_country\" ] = df [ col ] + country_name .....: return df .....: In [142]: df_p = pd . DataFrame ({ \"city_and_code\" : [ \"Chicago, IL\" ]}) extract_city_name and add_country_name are functions taking and returning DataFrames . Now compare the following: In [143]: add_country_name ( extract_city_name ( df_p ), country_name = \"US\" ) Out[143]: city_and_code city_name city_and_country 0 Chicago, IL Chicago ChicagoUS Is equivalent to: In [144]: df_p . pipe ( extract_city_name ) . pipe ( add_country_name , country_name = \"US\" ) Out[144]: city_and_code city_name city_and_country 0 Chicago, IL Chicago ChicagoUS pandas encourages the second style, which is known as method chaining. pipe makes it easy to use your own or another libraryâs functions in method chains, alongside pandasâ methods. In the example above, the functions extract_city_name and add_country_name each expected a DataFrame as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide pipe with a tuple of (callable, data_keyword) . .pipe will route the DataFrame to the argument specified in the tuple. For example, we can fit a regression using statsmodels. Their API expects a formula first and a DataFrame as the second argument, data . We pass in the function, keyword pair (sm.ols, 'data') to pipe : In [147]: import statsmodels.formula.api as sm In [148]: bb = pd . read_csv ( \"data/baseball.csv\" , index_col = \"id\" ) In [149]: ( .....: bb . query ( \"h > 0\" ) .....: . assign ( ln_h = lambda df : np . log ( df . h )) .....: . pipe (( sm . ols , \"data\" ), \"hr ~ ln_h + year + g + C(lg)\" ) .....: . fit () .....: . summary () .....: ) .....: Out[149]: <class 'statsmodels.iolib.summary.Summary'> \"\"\" OLS Regression Results ============================================================================== Dep. Variable: hr R-squared: 0.685 Model: OLS Adj. R-squared: 0.665 Method: Least Squares F-statistic: 34.28 Date: Tue, 22 Nov 2022 Prob (F-statistic): 3.48e-15 Time: 05:34:17 Log-Likelihood: -205.92 No. Observations: 68 AIC: 421.8 Df Residuals: 63 BIC: 432.9 Df Model: 4 Covariance Type: nonrobust =============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------- Intercept - 8484.7720 4664.146 - 1.819 0.074 - 1.78e+04 835.780 C ( lg )[ T . NL ] - 2.2736 1.325 - 1.716 0.091 - 4.922 0.375 ln_h - 1.3542 0.875 - 1.547 0.127 - 3.103 0.395 year 4.2277 2.324 1.819 0.074 - 0.417 8.872 g 0.1841 0.029 6.258 0.000 0.125 0.243 ============================================================================== Omnibus : 10.875 Durbin-Watson: 1.999 Prob ( Omnibus ): 0.004 Jarque - Bera ( JB ): 17.298 Skew : 0.537 Prob(JB): 0.000175 Kurtosis : 5.225 Cond. No. 1.49e+07 ============================================================================== Notes : [ 1 ] Standard Errors assume that the covariance matrix of the errors is correctly specified . [ 2 ] The condition number is large , 1.49e+07 . This might indicate that there are strong multicollinearity or other numerical problems . \"\"\" The pipe method is inspired by unix pipes and more recently dplyr and magrittr , which have introduced the popular (%>%) (read pipe) operator for R . The implementation of pipe here is quite clean and feels right at home in Python. We encourage you to view the source code of pipe() . Row or column-wise function application # Arbitrary functions can be applied along the axes of a DataFrame using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument: In [145]: df . apply ( lambda x : np . mean ( x )) Out[145]: one 0.811094 two 1.360588 three 0.187958 dtype: float64 In [146]: df . apply ( lambda x : np . mean ( x ), axis = 1 ) Out[146]: a 1.583749 b 0.734929 c 1.133683 d -0.166914 dtype: float64 In [147]: df . apply ( lambda x : x . max () - x . min ()) Out[147]: one 1.051928 two 1.632779 three 1.840607 dtype: float64 In [148]: df . apply ( np . cumsum ) Out[148]: one two three a 1.394981 1.772517 NaN b 1.738035 3.684640 -0.050390 c 2.433281 5.163008 1.177045 d NaN 5.442353 0.563873 In [149]: df . apply ( np . exp ) Out[149]: one two three a 4.034899 5.885648 NaN b 1.409244 6.767440 0.950858 c 2.004201 4.385785 3.412466 d NaN 1.322262 0.541630 The apply() method will also dispatch on a string method name. In [150]: df . apply ( \"mean\" ) Out[150]: one 0.811094 two 1.360588 three 0.187958 dtype: float64 In [151]: df . apply ( \"mean\" , axis = 1 ) Out[151]: a 1.583749 b 0.734929 c 1.133683 d -0.166914 dtype: float64 The return type of the function passed to apply() affects the type of the final output from DataFrame.apply for the default behaviour: If the applied function returns a Series , the final output is a DataFrame . The columns match the index of the Series returned by the applied function. If the applied function returns any other type, the final output is a Series . This default behaviour can be overridden using the result_type , which accepts three options: reduce , broadcast , and expand . These will determine how list-likes return values expand (or not) to a DataFrame . apply() combined with some cleverness can be used to answer many questions about a data set. For example, suppose we wanted to extract the date where the maximum value for each column occurred: In [152]: tsdf = pd . DataFrame ( .....: np . random . randn ( 1000 , 3 ), .....: columns = [ \"A\" , \"B\" , \"C\" ], .....: index = pd . date_range ( \"1/1/2000\" , periods = 1000 ), .....: ) .....: In [153]: tsdf . apply ( lambda x : x . idxmax ()) Out[153]: A 2000-08-06 B 2001-01-18 C 2001-07-18 dtype: datetime64[ns] You may also pass additional arguments and keyword arguments to the apply() method. In [154]: def subtract_and_divide ( x , sub , divide = 1 ): .....: return ( x - sub ) / divide .....: In [155]: df_udf = pd . DataFrame ( np . ones (( 2 , 2 ))) In [156]: df_udf . apply ( subtract_and_divide , args = ( 5 ,), divide = 3 ) Out[156]: 0 1 0 -1.333333 -1.333333 1 -1.333333 -1.333333 Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row: In [157]: tsdf = pd . DataFrame ( .....: np . random . randn ( 10 , 3 ), .....: columns = [ \"A\" , \"B\" , \"C\" ], .....: index = pd . date_range ( \"1/1/2000\" , periods = 10 ), .....: ) .....: In [158]: tsdf . iloc [ 3 : 7 ] = np . nan In [159]: tsdf Out[159]: A B C 2000-01-01 -0.158131 -0.232466 0.321604 2000-01-02 -1.810340 -3.105758 0.433834 2000-01-03 -1.209847 -1.156793 -0.136794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 -0.653602 0.178875 1.008298 2000-01-09 1.007996 0.462824 0.254472 2000-01-10 0.307473 0.600337 1.643950 In [160]: tsdf . apply ( pd . Series . interpolate ) Out[160]: A B C 2000-01-01 -0.158131 -0.232466 0.321604 2000-01-02 -1.810340 -3.105758 0.433834 2000-01-03 -1.209847 -1.156793 -0.136794 2000-01-04 -1.098598 -0.889659 0.092225 2000-01-05 -0.987349 -0.622526 0.321243 2000-01-06 -0.876100 -0.355392 0.550262 2000-01-07 -0.764851 -0.088259 0.779280 2000-01-08 -0.653602 0.178875 1.008298 2000-01-09 1.007996 0.462824 0.254472 2000-01-10 0.307473 0.600337 1.643950 Finally, apply() takes an argument raw which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality. Aggregation API # The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API , the window API , and the resample API . The entry point for aggregation is DataFrame.aggregate() , or the alias DataFrame.agg() . We will use a similar starting frame from above: In [161]: tsdf = pd . DataFrame ( .....: np . random . randn ( 10 , 3 ), .....: columns = [ \"A\" , \"B\" , \"C\" ], .....: index = pd . date_range ( \"1/1/2000\" , periods = 10 ), .....: ) .....: In [162]: tsdf . iloc [ 3 : 7 ] = np . nan In [163]: tsdf Out[163]: A B C 2000-01-01 1.257606 1.004194 0.167574 2000-01-02 -0.749892 0.288112 -0.757304 2000-01-03 -0.207550 -0.298599 0.116018 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.814347 -0.257623 0.869226 2000-01-09 -0.250663 -1.206601 0.896839 2000-01-10 2.169758 -1.333363 0.283157 Using a single function is equivalent to apply() . You can also pass named methods as strings. These will return a Series of the aggregated output: In [164]: tsdf . agg ( lambda x : np . sum ( x )) Out[164]: A 3.033606 B -1.803879 C 1.575510 dtype: float64 In [165]: tsdf . agg ( \"sum\" ) Out[165]: A 3.033606 B -1.803879 C 1.575510 dtype: float64 # these are equivalent to a ``.sum()`` because we are aggregating # on a single function In [166]: tsdf . sum () Out[166]: A 3.033606 B -1.803879 C 1.575510 dtype: float64 Single aggregations on a Series this will return a scalar value: In [167]: tsdf [ \"A\" ] . agg ( \"sum\" ) Out[167]: 3.033606102414146 Aggregating with multiple functions # You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting DataFrame . These are naturally named from the aggregation function. In [168]: tsdf . agg ([ \"sum\" ]) Out[168]: A B C sum 3.033606 -1.803879 1.57551 Multiple functions yield multiple rows: In [169]: tsdf . agg ([ \"sum\" , \"mean\" ]) Out[169]: A B C sum 3.033606 -1.803879 1.575510 mean 0.505601 -0.300647 0.262585 On a Series , multiple functions return a Series , indexed by the function names: In [170]: tsdf [ \"A\" ] . agg ([ \"sum\" , \"mean\" ]) Out[170]: sum 3.033606 mean 0.505601 Name: A, dtype: float64 Passing a lambda function will yield a <lambda> named row: In [171]: tsdf [ \"A\" ] . agg ([ \"sum\" , lambda x : x . mean ()]) Out[171]: sum 3.033606 <lambda> 0.505601 Name: A, dtype: float64 Passing a named function will yield that name for the row: In [172]: def mymean ( x ): .....: return x . mean () .....: In [173]: tsdf [ \"A\" ] . agg ([ \"sum\" , mymean ]) Out[173]: sum 3.033606 mymean 0.505601 Name: A, dtype: float64 Aggregating with a dict # Passing a dictionary of column names to a scalar or a list of scalars, to DataFrame.agg allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an OrderedDict instead to guarantee ordering. In [174]: tsdf . agg ({ \"A\" : \"mean\" , \"B\" : \"sum\" }) Out[174]: A 0.505601 B -1.803879 dtype: float64 Passing a list-like will generate a DataFrame output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be NaN : In [175]: tsdf . agg ({ \"A\" : [ \"mean\" , \"min\" ], \"B\" : \"sum\" }) Out[175]: A B mean 0.505601 NaN min -0.749892 NaN sum NaN -1.803879 Custom describe # With .agg() it is possible to easily create a custom describe function, similar to the built in describe function . In [176]: from functools import partial In [177]: q_25 = partial ( pd . Series . quantile , q = 0.25 ) In [178]: q_25 . __name__ = \"25%\" In [179]: q_75 = partial ( pd . Series . quantile , q = 0.75 ) In [180]: q_75 . __name__ = \"75%\" In [181]: tsdf . agg ([ \"count\" , \"mean\" , \"std\" , \"min\" , q_25 , \"median\" , q_75 , \"max\" ]) Out[181]: A B C count 6.000000 6.000000 6.000000 mean 0.505601 -0.300647 0.262585 std 1.103362 0.887508 0.606860 min -0.749892 -1.333363 -0.757304 25% -0.239885 -0.979600 0.128907 median 0.303398 -0.278111 0.225365 75% 1.146791 0.151678 0.722709 max 2.169758 1.004194 0.896839 Transform API # The transform() method returns an object that is indexed the same (same size) as the original. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the .agg API. We create a frame similar to the one used in the above sections. In [182]: tsdf = pd . DataFrame ( .....: np . random . randn ( 10 , 3 ), .....: columns = [ \"A\" , \"B\" , \"C\" ], .....: index = pd . date_range ( \"1/1/2000\" , periods = 10 ), .....: ) .....: In [183]: tsdf . iloc [ 3 : 7 ] = np . nan In [184]: tsdf Out[184]: A B C 2000-01-01 -0.428759 -0.864890 -0.675341 2000-01-02 -0.168731 1.338144 -1.279321 2000-01-03 -1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 -1.240447 -0.201052 2000-01-09 -0.157795 0.791197 -1.144209 2000-01-10 -0.030876 0.371900 0.061932 Transform the entire frame. .transform() allows input functions as: a NumPy function, a string function name or a user defined function. In [185]: tsdf . transform ( np . abs ) Out[185]: A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 In [186]: tsdf . transform ( \"abs\" ) Out[186]: A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 In [187]: tsdf . transform ( lambda x : x . abs ()) Out[187]: A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 Here transform() received a single function; this is equivalent to a ufunc application. In [188]: np . abs ( tsdf ) Out[188]: A B C 2000-01-01 0.428759 0.864890 0.675341 2000-01-02 0.168731 1.338144 1.279321 2000-01-03 1.621034 0.438107 0.903794 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 1.240447 0.201052 2000-01-09 0.157795 0.791197 1.144209 2000-01-10 0.030876 0.371900 0.061932 Passing a single function to .transform() with a Series will yield a single Series in return. In [189]: tsdf [ \"A\" ] . transform ( np . abs ) Out[189]: 2000-01-01 0.428759 2000-01-02 0.168731 2000-01-03 1.621034 2000-01-04 NaN 2000-01-05 NaN 2000-01-06 NaN 2000-01-07 NaN 2000-01-08 0.254374 2000-01-09 0.157795 2000-01-10 0.030876 Freq: D, Name: A, dtype: float64 Transform with multiple functions # Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions. In [190]: tsdf . transform ([ np . abs , lambda x : x + 1 ]) Out[190]: A B C absolute <lambda> absolute <lambda> absolute <lambda> 2000-01-01 0.428759 0.571241 0.864890 0.135110 0.675341 0.324659 2000-01-02 0.168731 0.831269 1.338144 2.338144 1.279321 -0.279321 2000-01-03 1.621034 -0.621034 0.438107 1.438107 0.903794 1.903794 2000-01-04 NaN NaN NaN NaN NaN NaN 2000-01-05 NaN NaN NaN NaN NaN NaN 2000-01-06 NaN NaN NaN NaN NaN NaN 2000-01-07 NaN NaN NaN NaN NaN NaN 2000-01-08 0.254374 1.254374 1.240447 -0.240447 0.201052 0.798948 2000-01-09 0.157795 0.842205 0.791197 1.791197 1.144209 -0.144209 2000-01-10 0.030876 0.969124 0.371900 1.371900 0.061932 1.061932 Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions. In [191]: tsdf [ \"A\" ] . transform ([ np . abs , lambda x : x + 1 ]) Out[191]: absolute <lambda> 2000-01-01 0.428759 0.571241 2000-01-02 0.168731 0.831269 2000-01-03 1.621034 -0.621034 2000-01-04 NaN NaN 2000-01-05 NaN NaN 2000-01-06 NaN NaN 2000-01-07 NaN NaN 2000-01-08 0.254374 1.254374 2000-01-09 0.157795 0.842205 2000-01-10 0.030876 0.969124 Transforming with a dict # Passing a dict of functions will allow selective transforming per column. In [192]: tsdf . transform ({ \"A\" : np . abs , \"B\" : lambda x : x + 1 }) Out[192]: A B 2000-01-01 0.428759 0.135110 2000-01-02 0.168731 2.338144 2000-01-03 1.621034 1.438107 2000-01-04 NaN NaN 2000-01-05 NaN NaN 2000-01-06 NaN NaN 2000-01-07 NaN NaN 2000-01-08 0.254374 -0.240447 2000-01-09 0.157795 1.791197 2000-01-10 0.030876 1.371900 Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms. In [193]: tsdf . transform ({ \"A\" : np . abs , \"B\" : [ lambda x : x + 1 , \"sqrt\" ]}) Out[193]: A B absolute <lambda> sqrt 2000-01-01 0.428759 0.135110 NaN 2000-01-02 0.168731 2.338144 1.156782 2000-01-03 1.621034 1.438107 0.661897 2000-01-04 NaN NaN NaN 2000-01-05 NaN NaN NaN 2000-01-06 NaN NaN NaN 2000-01-07 NaN NaN NaN 2000-01-08 0.254374 -0.240447 NaN 2000-01-09 0.157795 1.791197 0.889493 2000-01-10 0.030876 1.371900 0.609836 Applying elementwise functions # Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods map() on DataFrame and analogously map() on Series accept any Python function taking a single value and returning a single value. For example: In [194]: df4 = df . copy () In [195]: df4 Out[195]: one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [196]: def f ( x ): .....: return len ( str ( x )) .....: In [197]: df4 [ \"one\" ] . map ( f ) Out[197]: a 18 b 19 c 18 d 3 Name: one, dtype: int64 In [198]: df4 . map ( f ) Out[198]: one two three a 18 17 3 b 19 18 20 c 18 18 16 d 3 19 19 Series.map() has an additional feature; it can be used to easily âlinkâ or âmapâ values defined by a secondary series. This is closely related to merging/joining functionality : In [199]: s = pd . Series ( .....: [ \"six\" , \"seven\" , \"six\" , \"seven\" , \"six\" ], index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ] .....: ) .....: In [200]: t = pd . Series ({ \"six\" : 6.0 , \"seven\" : 7.0 }) In [201]: s Out[201]: a six b seven c six d seven e six dtype: object In [202]: s . map ( t ) Out[202]: a 6.0 b 7.0 c 6.0 d 7.0 e 6.0 dtype: float64 Reindexing and altering labels # reindex() is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things: Reorders the existing data to match a new set of labels Inserts missing value (NA) markers in label locations where no data for that label existed If specified, fill data for missing labels using logic (highly relevant to working with time series data) Here is a simple example: In [203]: s = pd . Series ( np . random . randn ( 5 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) In [204]: s Out[204]: a 1.695148 b 1.328614 c 1.234686 d -0.385845 e -1.326508 dtype: float64 In [205]: s . reindex ([ \"e\" , \"b\" , \"f\" , \"d\" ]) Out[205]: e -1.326508 b 1.328614 f NaN d -0.385845 dtype: float64 Here, the f label was not contained in the Series and hence appears as NaN in the result. With a DataFrame, you can simultaneously reindex the index and columns: In [206]: df Out[206]: one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [207]: df . reindex ( index = [ \"c\" , \"f\" , \"b\" ], columns = [ \"three\" , \"two\" , \"one\" ]) Out[207]: three two one c 1.227435 1.478369 0.695246 f NaN NaN NaN b -0.050390 1.912123 0.343054 Note that the Index objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done: In [208]: rs = s . reindex ( df . index ) In [209]: rs Out[209]: a 1.695148 b 1.328614 c 1.234686 d -0.385845 dtype: float64 In [210]: rs . index is df . index Out[210]: True This means that the reindexed Seriesâs index is the same Python object as the DataFrameâs index. DataFrame.reindex() also supports an âaxis-styleâ calling convention, where you specify a single labels argument and the axis it applies to. In [211]: df . reindex ([ \"c\" , \"f\" , \"b\" ], axis = \"index\" ) Out[211]: one two three c 0.695246 1.478369 1.227435 f NaN NaN NaN b 0.343054 1.912123 -0.050390 In [212]: df . reindex ([ \"three\" , \"two\" , \"one\" ], axis = \"columns\" ) Out[212]: three two one a NaN 1.772517 1.394981 b -0.050390 1.912123 0.343054 c 1.227435 1.478369 0.695246 d -0.613172 0.279344 NaN See also MultiIndex / Advanced Indexing is an even more concise way of doing reindexing. Note When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: many operations are faster on pre-aligned data . Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because reindex has been heavily optimized), but when CPU cycles matter sprinkling a few explicit reindex calls here and there can have an impact. Reindexing to align with another object # You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the reindex_like() method is available to make this simpler: In [213]: df2 = df . reindex ([ \"a\" , \"b\" , \"c\" ], columns = [ \"one\" , \"two\" ]) In [214]: df3 = df2 - df2 . mean () In [215]: df2 Out[215]: one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369 In [216]: df3 Out[216]: one two a 0.583888 0.051514 b -0.468040 0.191120 c -0.115848 -0.242634 In [217]: df . reindex_like ( df2 ) Out[217]: one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369 Aligning objects with each other with align # The align() method is the fastest way to simultaneously align two objects. It supports a join argument (related to joining and merging ): join='outer' : take the union of the indexes (default) join='left' : use the calling objectâs index join='right' : use the passed objectâs index join='inner' : intersect the indexes It returns a tuple with both of the reindexed Series: In [218]: s = pd . Series ( np . random . randn ( 5 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ]) In [219]: s1 = s [: 4 ] In [220]: s2 = s [ 1 :] In [221]: s1 . align ( s2 ) Out[221]: (a -0.186646 b -1.692424 c -0.303893 d -1.425662 e NaN dtype: float64, a NaN b -1.692424 c -0.303893 d -1.425662 e 1.114285 dtype: float64) In [222]: s1 . align ( s2 , join = \"inner\" ) Out[222]: (b -1.692424 c -0.303893 d -1.425662 dtype: float64, b -1.692424 c -0.303893 d -1.425662 dtype: float64) In [223]: s1 . align ( s2 , join = \"left\" ) Out[223]: (a -0.186646 b -1.692424 c -0.303893 d -1.425662 dtype: float64, a NaN b -1.692424 c -0.303893 d -1.425662 dtype: float64) For DataFrames, the join method will be applied to both the index and the columns by default: In [224]: df . align ( df2 , join = \"inner\" ) Out[224]: ( one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369, one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369) You can also pass an axis option to only align on the specified axis: In [225]: df . align ( df2 , join = \"inner\" , axis = 0 ) Out[225]: ( one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435, one two a 1.394981 1.772517 b 0.343054 1.912123 c 0.695246 1.478369) If you pass a Series to DataFrame.align() , you can choose to align both objects either on the DataFrameâs index or columns using the axis argument: In [226]: df . align ( df2 . iloc [ 0 ], axis = 1 ) Out[226]: ( one three two a 1.394981 NaN 1.772517 b 0.343054 -0.050390 1.912123 c 0.695246 1.227435 1.478369 d NaN -0.613172 0.279344, one 1.394981 three NaN two 1.772517 Name: a, dtype: float64) Filling while reindexing # reindex() takes an optional parameter method which is a filling method chosen from the following table: Method Action pad / ffill Fill values forward bfill / backfill Fill values backward nearest Fill from the nearest index value We illustrate these fill methods on a simple Series: In [227]: rng = pd . date_range ( \"1/3/2000\" , periods = 8 ) In [228]: ts = pd . Series ( np . random . randn ( 8 ), index = rng ) In [229]: ts2 = ts . iloc [[ 0 , 3 , 6 ]] In [230]: ts Out[230]: 2000-01-03 0.183051 2000-01-04 0.400528 2000-01-05 -0.015083 2000-01-06 2.395489 2000-01-07 1.414806 2000-01-08 0.118428 2000-01-09 0.733639 2000-01-10 -0.936077 Freq: D, dtype: float64 In [231]: ts2 Out[231]: 2000-01-03 0.183051 2000-01-06 2.395489 2000-01-09 0.733639 Freq: 3D, dtype: float64 In [232]: ts2 . reindex ( ts . index ) Out[232]: 2000-01-03 0.183051 2000-01-04 NaN 2000-01-05 NaN 2000-01-06 2.395489 2000-01-07 NaN 2000-01-08 NaN 2000-01-09 0.733639 2000-01-10 NaN Freq: D, dtype: float64 In [233]: ts2 . reindex ( ts . index , method = \"ffill\" ) Out[233]: 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 0.183051 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 2.395489 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 In [234]: ts2 . reindex ( ts . index , method = \"bfill\" ) Out[234]: 2000-01-03 0.183051 2000-01-04 2.395489 2000-01-05 2.395489 2000-01-06 2.395489 2000-01-07 0.733639 2000-01-08 0.733639 2000-01-09 0.733639 2000-01-10 NaN Freq: D, dtype: float64 In [235]: ts2 . reindex ( ts . index , method = \"nearest\" ) Out[235]: 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 2.395489 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 0.733639 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 These methods require that the indexes are ordered increasing or decreasing. Note that the same result could have been achieved using ffill (except for method='nearest' ) or interpolate : In [236]: ts2 . reindex ( ts . index ) . ffill () Out[236]: 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 0.183051 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 2.395489 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 reindex() will raise a ValueError if the index is not monotonically increasing or decreasing. fillna() and interpolate() will not perform any checks on the order of the index. Limits on filling while reindexing # The limit and tolerance arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches: In [237]: ts2 . reindex ( ts . index , method = \"ffill\" , limit = 1 ) Out[237]: 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 NaN 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 NaN 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 In contrast, tolerance specifies the maximum distance between the index and indexer values: In [238]: ts2 . reindex ( ts . index , method = \"ffill\" , tolerance = \"1 day\" ) Out[238]: 2000-01-03 0.183051 2000-01-04 0.183051 2000-01-05 NaN 2000-01-06 2.395489 2000-01-07 2.395489 2000-01-08 NaN 2000-01-09 0.733639 2000-01-10 0.733639 Freq: D, dtype: float64 Notice that when used on a DatetimeIndex , TimedeltaIndex or PeriodIndex , tolerance will coerced into a Timedelta if possible. This allows you to specify tolerance with appropriate strings. Dropping labels from an axis # A method closely related to reindex is the drop() function. It removes a set of labels from an axis: In [239]: df Out[239]: one two three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [240]: df . drop ([ \"a\" , \"d\" ], axis = 0 ) Out[240]: one two three b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 In [241]: df . drop ([ \"one\" ], axis = 1 ) Out[241]: two three a 1.772517 NaN b 1.912123 -0.050390 c 1.478369 1.227435 d 0.279344 -0.613172 Note that the following also works, but is a bit less obvious / clean: In [242]: df . reindex ( df . index . difference ([ \"a\" , \"d\" ])) Out[242]: one two three b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 Renaming / mapping labels # The rename() method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function. In [243]: s Out[243]: a -0.186646 b -1.692424 c -0.303893 d -1.425662 e 1.114285 dtype: float64 In [244]: s . rename ( str . upper ) Out[244]: A -0.186646 B -1.692424 C -0.303893 D -1.425662 E 1.114285 dtype: float64 If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used: In [245]: df . rename ( .....: columns = { \"one\" : \"foo\" , \"two\" : \"bar\" }, .....: index = { \"a\" : \"apple\" , \"b\" : \"banana\" , \"d\" : \"durian\" }, .....: ) .....: Out[245]: foo bar three apple 1.394981 1.772517 NaN banana 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 durian NaN 0.279344 -0.613172 If the mapping doesnât include a column/index label, it isnât renamed. Note that extra labels in the mapping donât throw an error. DataFrame.rename() also supports an âaxis-styleâ calling convention, where you specify a single mapper and the axis to apply that mapping to. In [246]: df . rename ({ \"one\" : \"foo\" , \"two\" : \"bar\" }, axis = \"columns\" ) Out[246]: foo bar three a 1.394981 1.772517 NaN b 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 d NaN 0.279344 -0.613172 In [247]: df . rename ({ \"a\" : \"apple\" , \"b\" : \"banana\" , \"d\" : \"durian\" }, axis = \"index\" ) Out[247]: one two three apple 1.394981 1.772517 NaN banana 0.343054 1.912123 -0.050390 c 0.695246 1.478369 1.227435 durian NaN 0.279344 -0.613172 Finally, rename() also accepts a scalar or list-like for altering the Series.name attribute. In [248]: s . rename ( \"scalar-name\" ) Out[248]: a -0.186646 b -1.692424 c -0.303893 d -1.425662 e 1.114285 Name: scalar-name, dtype: float64 The methods DataFrame.rename_axis() and Series.rename_axis() allow specific names of a MultiIndex to be changed (as opposed to the labels). In [249]: df = pd . DataFrame ( .....: { \"x\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], \"y\" : [ 10 , 20 , 30 , 40 , 50 , 60 ]}, .....: index = pd . MultiIndex . from_product ( .....: [[ \"a\" , \"b\" , \"c\" ], [ 1 , 2 ]], names = [ \"let\" , \"num\" ] .....: ), .....: ) .....: In [250]: df Out[250]: x y let num a 1 1 10 2 2 20 b 1 3 30 2 4 40 c 1 5 50 2 6 60 In [251]: df . rename_axis ( index = { \"let\" : \"abc\" }) Out[251]: x y abc num a 1 1 10 2 2 20 b 1 3 30 2 4 40 c 1 5 50 2 6 60 In [252]: df . rename_axis ( index = str . upper ) Out[252]: x y LET NUM a 1 1 10 2 2 20 b 1 3 30 2 4 40 c 1 5 50 2 6 60 Iteration # The behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the âkeysâ of the objects. In short, basic iteration ( for i in object ) produces: Series : values DataFrame : column labels Thus, for example, iterating over a DataFrame gives you the column names: In [253]: df = pd . DataFrame ( .....: { \"col1\" : np . random . randn ( 3 ), \"col2\" : np . random . randn ( 3 )}, index = [ \"a\" , \"b\" , \"c\" ] .....: ) .....: In [254]: for col in df : .....: print ( col ) .....: col1 col2 pandas objects also have the dict-like items() method to iterate over the (key, value) pairs. To iterate over the rows of a DataFrame, you can use the following methods: iterrows() : Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications. itertuples() : Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster than iterrows() , and is in most cases preferable to use to iterate over the values of a DataFrame. Warning Iterating through pandas objects is generally slow . In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches: Look for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, â¦ When you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application . If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach. Warning You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect! For example, in the following case setting the value has no effect: In [255]: df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ \"a\" , \"b\" , \"c\" ]}) In [256]: for index , row in df . iterrows (): .....: row [ \"a\" ] = 10 .....: In [257]: df Out[257]: a b 0 1 a 1 2 b 2 3 c items # Consistent with the dict-like interface, items() iterates through key-value pairs: Series : (index, scalar value) pairs DataFrame : (column, Series) pairs For example: In [258]: for label , ser in df . items (): .....: print ( label ) .....: print ( ser ) .....: a 0 1 1 2 2 3 Name: a, dtype: int64 b 0 a 1 b 2 c Name: b, dtype: object iterrows # iterrows() allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row: In [259]: for row_index , row in df . iterrows (): .....: print ( row_index , row , sep = \" \\n \" ) .....: 0 a 1 b a Name: 0, dtype: object 1 a 2 b b Name: 1, dtype: object 2 a 3 b c Name: 2, dtype: object Note Because iterrows() returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example, In [260]: df_orig = pd . DataFrame ([[ 1 , 1.5 ]], columns = [ \"int\" , \"float\" ]) In [261]: df_orig . dtypes Out[261]: int int64 float float64 dtype: object In [262]: row = next ( df_orig . iterrows ())[ 1 ] In [263]: row Out[263]: int 1.0 float 1.5 Name: 0, dtype: float64 All values in row , returned as a Series, are now upcasted to floats, also the original integer value in column x : In [264]: row [ \"int\" ] . dtype Out[264]: dtype('float64') In [265]: df_orig [ \"int\" ] . dtype Out[265]: dtype('int64') To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows() . For instance, a contrived way to transpose the DataFrame would be: In [266]: df2 = pd . DataFrame ({ \"x\" : [ 1 , 2 , 3 ], \"y\" : [ 4 , 5 , 6 ]}) In [267]: print ( df2 ) x y 0 1 4 1 2 5 2 3 6 In [268]: print ( df2 . T ) 0 1 2 x 1 2 3 y 4 5 6 In [269]: df2_t = pd . DataFrame ({ idx : values for idx , values in df2 . iterrows ()}) In [270]: print ( df2_t ) 0 1 2 x 1 2 3 y 4 5 6 itertuples # The itertuples() method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the rowâs corresponding index value, while the remaining values are the row values. For instance: In [271]: for row in df . itertuples (): .....: print ( row ) .....: Pandas(Index=0, a=1, b='a') Pandas(Index=1, a=2, b='b') Pandas(Index=2, a=3, b='c') This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, itertuples() preserves the data type of the values and is generally faster as iterrows() . Note The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned. .dt accessor # Series has an accessor to succinctly return datetime like properties for the values of the Series, if it is a datetime/period like Series. This will return a Series, indexed like the existing Series. # datetime In [272]: s = pd . Series ( pd . date_range ( \"20130101 09:10:12\" , periods = 4 )) In [273]: s Out[273]: 0 2013-01-01 09:10:12 1 2013-01-02 09:10:12 2 2013-01-03 09:10:12 3 2013-01-04 09:10:12 dtype: datetime64[ns] In [274]: s . dt . hour Out[274]: 0 9 1 9 2 9 3 9 dtype: int32 In [275]: s . dt . second Out[275]: 0 12 1 12 2 12 3 12 dtype: int32 In [276]: s . dt . day Out[276]: 0 1 1 2 2 3 3 4 dtype: int32 This enables nice expressions like this: In [277]: s [ s . dt . day == 2 ] Out[277]: 1 2013-01-02 09:10:12 dtype: datetime64[ns] You can easily produces tz aware transformations: In [278]: stz = s . dt . tz_localize ( \"US/Eastern\" ) In [279]: stz Out[279]: 0 2013-01-01 09:10:12-05:00 1 2013-01-02 09:10:12-05:00 2 2013-01-03 09:10:12-05:00 3 2013-01-04 09:10:12-05:00 dtype: datetime64[ns, US/Eastern] In [280]: stz . dt . tz Out[280]: <DstTzInfo 'US/Eastern' LMT-1 day, 19:04:00 STD> You can also chain these types of operations: In [281]: s . dt . tz_localize ( \"UTC\" ) . dt . tz_convert ( \"US/Eastern\" ) Out[281]: 0 2013-01-01 04:10:12-05:00 1 2013-01-02 04:10:12-05:00 2 2013-01-03 04:10:12-05:00 3 2013-01-04 04:10:12-05:00 dtype: datetime64[ns, US/Eastern] You can also format datetime values as strings with Series.dt.strftime() which supports the same format as the standard strftime() . # DatetimeIndex In [282]: s = pd . Series ( pd . date_range ( \"20130101\" , periods = 4 )) In [283]: s Out[283]: 0 2013-01-01 1 2013-01-02 2 2013-01-03 3 2013-01-04 dtype: datetime64[ns] In [284]: s . dt . strftime ( \"%Y/%m/ %d \" ) Out[284]: 0 2013/01/01 1 2013/01/02 2 2013/01/03 3 2013/01/04 dtype: object # PeriodIndex In [285]: s = pd . Series ( pd . period_range ( \"20130101\" , periods = 4 )) In [286]: s Out[286]: 0 2013-01-01 1 2013-01-02 2 2013-01-03 3 2013-01-04 dtype: period[D] In [287]: s . dt . strftime ( \"%Y/%m/ %d \" ) Out[287]: 0 2013/01/01 1 2013/01/02 2 2013/01/03 3 2013/01/04 dtype: object The .dt accessor works for period and timedelta dtypes. # period In [288]: s = pd . Series ( pd . period_range ( \"20130101\" , periods = 4 , freq = \"D\" )) In [289]: s Out[289]: 0 2013-01-01 1 2013-01-02 2 2013-01-03 3 2013-01-04 dtype: period[D] In [290]: s . dt . year Out[290]: 0 2013 1 2013 2 2013 3 2013 dtype: int64 In [291]: s . dt . day Out[291]: 0 1 1 2 2 3 3 4 dtype: int64 # timedelta In [292]: s = pd . Series ( pd . timedelta_range ( \"1 day 00:00:05\" , periods = 4 , freq = \"s\" )) In [293]: s Out[293]: 0 1 days 00:00:05 1 1 days 00:00:06 2 1 days 00:00:07 3 1 days 00:00:08 dtype: timedelta64[ns] In [294]: s . dt . days Out[294]: 0 1 1 1 2 1 3 1 dtype: int64 In [295]: s . dt . seconds Out[295]: 0 5 1 6 2 7 3 8 dtype: int32 In [296]: s . dt . components Out[296]: days hours minutes seconds milliseconds microseconds nanoseconds 0 1 0 0 5 0 0 0 1 1 0 0 6 0 0 0 2 1 0 0 7 0 0 0 3 1 0 0 8 0 0 0 Note Series.dt will raise a TypeError if you access with a non-datetime-like values. Vectorized string methods # Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the Seriesâs str attribute and generally have names matching the equivalent (scalar) built-in string methods. For example: In [297]: s = pd . Series ( .....: [ \"A\" , \"B\" , \"C\" , \"Aaba\" , \"Baca\" , np . nan , \"CABA\" , \"dog\" , \"cat\" ], dtype = \"string\" .....: ) .....: In [298]: s . str . lower () Out[298]: 0 a 1 b 2 c 3 aaba 4 baca 5 <NA> 6 caba 7 dog 8 cat dtype: string Powerful pattern-matching methods are provided as well, but note that pattern-matching generally uses regular expressions by default (and in some cases always uses them). Note Prior to pandas 1.0, string methods were only available on object -dtype Series . pandas 1.0 added the StringDtype which is dedicated to strings. See Text data types for more. Please see Vectorized String Methods for a complete description. Sorting # pandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both. By index # The Series.sort_index() and DataFrame.sort_index() methods are used to sort a pandas object by its index levels. In [299]: df = pd . DataFrame ( .....: { .....: \"one\" : pd . Series ( np . random . randn ( 3 ), index = [ \"a\" , \"b\" , \"c\" ]), .....: \"two\" : pd . Series ( np . random . randn ( 4 ), index = [ \"a\" , \"b\" , \"c\" , \"d\" ]), .....: \"three\" : pd . Series ( np . random . randn ( 3 ), index = [ \"b\" , \"c\" , \"d\" ]), .....: } .....: ) .....: In [300]: unsorted_df = df . reindex ( .....: index = [ \"a\" , \"d\" , \"c\" , \"b\" ], columns = [ \"three\" , \"two\" , \"one\" ] .....: ) .....: In [301]: unsorted_df Out[301]: three two one a NaN -1.152244 0.562973 d -0.252916 -0.109597 NaN c 1.273388 -0.167123 0.640382 b -0.098217 0.009797 -1.299504 # DataFrame In [302]: unsorted_df . sort_index () Out[302]: three two one a NaN -1.152244 0.562973 b -0.098217 0.009797 -1.299504 c 1.273388 -0.167123 0.640382 d -0.252916 -0.109597 NaN In [303]: unsorted_df . sort_index ( ascending = False ) Out[303]: three two one d -0.252916 -0.109597 NaN c 1.273388 -0.167123 0.640382 b -0.098217 0.009797 -1.299504 a NaN -1.152244 0.562973 In [304]: unsorted_df . sort_index ( axis = 1 ) Out[304]: one three two a 0.562973 NaN -1.152244 d NaN -0.252916 -0.109597 c 0.640382 1.273388 -0.167123 b -1.299504 -0.098217 0.009797 # Series In [305]: unsorted_df [ \"three\" ] . sort_index () Out[305]: a NaN b -0.098217 c 1.273388 d -0.252916 Name: three, dtype: float64 Sorting by index also supports a key parameter that takes a callable function to apply to the index being sorted. For MultiIndex objects, the key is applied per-level to the levels specified by level . In [306]: s1 = pd . DataFrame ({ \"a\" : [ \"B\" , \"a\" , \"C\" ], \"b\" : [ 1 , 2 , 3 ], \"c\" : [ 2 , 3 , 4 ]}) . set_index ( .....: list ( \"ab\" ) .....: ) .....: In [307]: s1 Out[307]: c a b B 1 2 a 2 3 C 3 4 In [308]: s1 . sort_index ( level = \"a\" ) Out[308]: c a b B 1 2 C 3 4 a 2 3 In [309]: s1 . sort_index ( level = \"a\" , key = lambda idx : idx . str . lower ()) Out[309]: c a b a 2 3 B 1 2 C 3 4 For information on key sorting by value, see value sorting . By values # The Series.sort_values() method is used to sort a Series by its values. The DataFrame.sort_values() method is used to sort a DataFrame by its column or row values. The optional by parameter to DataFrame.sort_values() may used to specify one or more columns to use to determine the sorted order. In [310]: df1 = pd . DataFrame ( .....: { \"one\" : [ 2 , 1 , 1 , 1 ], \"two\" : [ 1 , 3 , 2 , 4 ], \"three\" : [ 5 , 4 , 3 , 2 ]} .....: ) .....: In [311]: df1 . sort_values ( by = \"two\" ) Out[311]: one two three 0 2 1 5 2 1 2 3 1 1 3 4 3 1 4 2 The by parameter can take a list of column names, e.g.: In [312]: df1 [[ \"one\" , \"two\" , \"three\" ]] . sort_values ( by = [ \"one\" , \"two\" ]) Out[312]: one two three 2 1 2 3 1 1 3 4 3 1 4 2 0 2 1 5 These methods have special treatment of NA values via the na_position argument: In [313]: s [ 2 ] = np . nan In [314]: s . sort_values () Out[314]: 0 A 3 Aaba 1 B 4 Baca 6 CABA 8 cat 7 dog 2 <NA> 5 <NA> dtype: string In [315]: s . sort_values ( na_position = \"first\" ) Out[315]: 2 <NA> 5 <NA> 0 A 3 Aaba 1 B 4 Baca 6 CABA 8 cat 7 dog dtype: string Sorting also supports a key parameter that takes a callable function to apply to the values being sorted. In [316]: s1 = pd . Series ([ \"B\" , \"a\" , \"C\" ]) In [317]: s1 . sort_values () Out[317]: 0 B 2 C 1 a dtype: object In [318]: s1 . sort_values ( key = lambda x : x . str . lower ()) Out[318]: 1 a 0 B 2 C dtype: object key will be given the Series of values and should return a Series or array of the same shape with the transformed values. For DataFrame objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g. In [319]: df = pd . DataFrame ({ \"a\" : [ \"B\" , \"a\" , \"C\" ], \"b\" : [ 1 , 2 , 3 ]}) In [320]: df . sort_values ( by = \"a\" ) Out[320]: a b 0 B 1 2 C 3 1 a 2 In [321]: df . sort_values ( by = \"a\" , key = lambda col : col . str . lower ()) Out[321]: a b 1 a 2 0 B 1 2 C 3 The name or type of each column can be used to apply different functions to different columns. By indexes and values # Strings passed as the by parameter to DataFrame.sort_values() may refer to either columns or index level names. # Build MultiIndex In [322]: idx = pd . MultiIndex . from_tuples ( .....: [( \"a\" , 1 ), ( \"a\" , 2 ), ( \"a\" , 2 ), ( \"b\" , 2 ), ( \"b\" , 1 ), ( \"b\" , 1 )] .....: ) .....: In [323]: idx . names = [ \"first\" , \"second\" ] # Build DataFrame In [324]: df_multi = pd . DataFrame ({ \"A\" : np . arange ( 6 , 0 , - 1 )}, index = idx ) In [325]: df_multi Out[325]: A first second a 1 6 2 5 2 4 b 2 3 1 2 1 1 Sort by âsecondâ (index) and âAâ (column) In [326]: df_multi . sort_values ( by = [ \"second\" , \"A\" ]) Out[326]: A first second b 1 1 1 2 a 1 6 b 2 3 a 2 4 2 5 Note If a string matches both a column name and an index level name then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version. searchsorted # Series has the searchsorted() method, which works similarly to numpy.ndarray.searchsorted() . In [327]: ser = pd . Series ([ 1 , 2 , 3 ]) In [328]: ser . searchsorted ([ 0 , 3 ]) Out[328]: array([0, 2]) In [329]: ser . searchsorted ([ 0 , 4 ]) Out[329]: array([0, 3]) In [330]: ser . searchsorted ([ 1 , 3 ], side = \"right\" ) Out[330]: array([1, 3]) In [331]: ser . searchsorted ([ 1 , 3 ], side = \"left\" ) Out[331]: array([0, 2]) In [332]: ser = pd . Series ([ 3 , 1 , 2 ]) In [333]: ser . searchsorted ([ 0 , 3 ], sorter = np . argsort ( ser )) Out[333]: array([0, 2]) smallest / largest values # Series has the nsmallest() and nlargest() methods which return the smallest or largest \\(n\\) values. For a large Series this can be much faster than sorting the entire Series and calling head(n) on the result. In [334]: s = pd . Series ( np . random . permutation ( 10 )) In [335]: s Out[335]: 0 2 1 0 2 3 3 7 4 1 5 5 6 9 7 6 8 8 9 4 dtype: int64 In [336]: s . sort_values () Out[336]: 1 0 4 1 0 2 2 3 9 4 5 5 7 6 3 7 8 8 6 9 dtype: int64 In [337]: s . nsmallest ( 3 ) Out[337]: 1 0 4 1 0 2 dtype: int64 In [338]: s . nlargest ( 3 ) Out[338]: 6 9 8 8 3 7 dtype: int64 DataFrame also has the nlargest and nsmallest methods. In [339]: df = pd . DataFrame ( .....: { .....: \"a\" : [ - 2 , - 1 , 1 , 10 , 8 , 11 , - 1 ], .....: \"b\" : list ( \"abdceff\" ), .....: \"c\" : [ 1.0 , 2.0 , 4.0 , 3.2 , np . nan , 3.0 , 4.0 ], .....: } .....: ) .....: In [340]: df . nlargest ( 3 , \"a\" ) Out[340]: a b c 5 11 f 3.0 3 10 c 3.2 4 8 e NaN In [341]: df . nlargest ( 5 , [ \"a\" , \"c\" ]) Out[341]: a b c 5 11 f 3.0 3 10 c 3.2 4 8 e NaN 2 1 d 4.0 6 -1 f 4.0 In [342]: df . nsmallest ( 3 , \"a\" ) Out[342]: a b c 0 -2 a 1.0 1 -1 b 2.0 6 -1 f 4.0 In [343]: df . nsmallest ( 5 , [ \"a\" , \"c\" ]) Out[343]: a b c 0 -2 a 1.0 1 -1 b 2.0 6 -1 f 4.0 2 1 d 4.0 4 8 e NaN Sorting by a MultiIndex column # You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to by . In [344]: df1 . columns = pd . MultiIndex . from_tuples ( .....: [( \"a\" , \"one\" ), ( \"a\" , \"two\" ), ( \"b\" , \"three\" )] .....: ) .....: In [345]: df1 . sort_values ( by = ( \"a\" , \"two\" )) Out[345]: a b one two three 0 2 1 5 2 1 2 3 1 1 3 4 3 1 4 2 Copying # The copy() method on pandas objects copies the underlying data (though not the axis indexes, since they are immutable) and returns a new object. Note that it is seldom necessary to copy objects . For example, there are only a handful of ways to alter a DataFrame in-place : Inserting, deleting, or modifying a column. Assigning to the index or columns attributes. For homogeneous data, directly modifying the values via the values attribute or advanced indexing. To be clear, no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly. dtypes # For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for float , int , bool , timedelta64[ns] and datetime64[ns] (note that NumPy does not support timezone-aware datetimes). pandas and third-party libraries extend NumPyâs type system in a few places. This section describes the extensions pandas has made internally. See Extension types for how to write your own extension that works with pandas. See the ecosystem page for a list of third-party libraries that have implemented an extension. The following table lists all of pandas extension types. For methods requiring dtype arguments, strings can be specified as indicated. See the respective documentation sections for more on each type. Kind of Data Data Type Scalar Array String Aliases tz-aware datetime DatetimeTZDtype Timestamp arrays.DatetimeArray 'datetime64[ns, <tz>]' Categorical CategoricalDtype (none) Categorical 'category' period (time spans) PeriodDtype Period arrays.PeriodArray 'Period[<freq>]' 'period[<freq>]' , sparse SparseDtype (none) arrays.SparseArray 'Sparse' , 'Sparse[int]' , 'Sparse[float]' intervals IntervalDtype Interval arrays.IntervalArray 'interval' , 'Interval' , 'Interval[<numpy_dtype>]' , 'Interval[datetime64[ns, <tz>]]' , 'Interval[timedelta64[<freq>]]' nullable integer Int64Dtype , â¦ (none) arrays.IntegerArray 'Int8' , 'Int16' , 'Int32' , 'Int64' , 'UInt8' , 'UInt16' , 'UInt32' , 'UInt64' nullable float Float64Dtype , â¦ (none) arrays.FloatingArray 'Float32' , 'Float64' Strings StringDtype str arrays.StringArray 'string' Boolean (with NA) BooleanDtype bool arrays.BooleanArray 'boolean' pandas has two ways to store strings. object dtype, which can hold any Python object, including strings. StringDtype , which is dedicated to strings. Generally, we recommend using StringDtype . See Text data types for more. Finally, arbitrary objects may be stored using the object dtype, but should be avoided to the extent possible (for performance and interoperability with other libraries and methods. See object conversion ). A convenient dtypes attribute for DataFrame returns a Series with the data type of each column. In [346]: dft = pd . DataFrame ( .....: { .....: \"A\" : np . random . rand ( 3 ), .....: \"B\" : 1 , .....: \"C\" : \"foo\" , .....: \"D\" : pd . Timestamp ( \"20010102\" ), .....: \"E\" : pd . Series ([ 1.0 ] * 3 ) . astype ( \"float32\" ), .....: \"F\" : False , .....: \"G\" : pd . Series ([ 1 ] * 3 , dtype = \"int8\" ), .....: } .....: ) .....: In [347]: dft Out[347]: A B C D E F G 0 0.035962 1 foo 2001-01-02 1.0 False 1 1 0.701379 1 foo 2001-01-02 1.0 False 1 2 0.281885 1 foo 2001-01-02 1.0 False 1 In [348]: dft . dtypes Out[348]: A float64 B int64 C object D datetime64[s] E float32 F bool G int8 dtype: object On a Series object, use the dtype attribute. In [349]: dft [ \"A\" ] . dtype Out[349]: dtype('float64') If a pandas object contains data with multiple dtypes in a single column , the dtype of the column will be chosen to accommodate all of the data types ( object is the most general). # these ints are coerced to floats In [350]: pd . Series ([ 1 , 2 , 3 , 4 , 5 , 6.0 ]) Out[350]: 0 1.0 1 2.0 2 3.0 3 4.0 4 5.0 5 6.0 dtype: float64 # string data forces an ``object`` dtype In [351]: pd . Series ([ 1 , 2 , 3 , 6.0 , \"foo\" ]) Out[351]: 0 1 1 2 2 3 3 6.0 4 foo dtype: object The number of columns of each type in a DataFrame can be found by calling DataFrame.dtypes.value_counts() . In [352]: dft . dtypes . value_counts () Out[352]: float64 1 int64 1 object 1 datetime64[s] 1 float32 1 bool 1 int8 1 Name: count, dtype: int64 Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the dtype keyword, a passed ndarray , or a passed Series ), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will NOT be combined. The following example will give you a taste. In [353]: df1 = pd . DataFrame ( np . random . randn ( 8 , 1 ), columns = [ \"A\" ], dtype = \"float32\" ) In [354]: df1 Out[354]: A 0 0.224364 1 1.890546 2 0.182879 3 0.787847 4 -0.188449 5 0.667715 6 -0.011736 7 -0.399073 In [355]: df1 . dtypes Out[355]: A float32 dtype: object In [356]: df2 = pd . DataFrame ( .....: { .....: \"A\" : pd . Series ( np . random . randn ( 8 ), dtype = \"float16\" ), .....: \"B\" : pd . Series ( np . random . randn ( 8 )), .....: \"C\" : pd . Series ( np . random . randint ( 0 , 255 , size = 8 ), dtype = \"uint8\" ), # [0,255] (range of uint8) .....: } .....: ) .....: In [357]: df2 Out[357]: A B C 0 0.823242 0.256090 26 1 1.607422 1.426469 86 2 -0.333740 -0.416203 46 3 -0.063477 1.139976 212 4 -1.014648 -1.193477 26 5 0.678711 0.096706 7 6 -0.040863 -1.956850 184 7 -0.357422 -0.714337 206 In [358]: df2 . dtypes Out[358]: A float16 B float64 C uint8 dtype: object defaults # By default integer types are int64 and float types are float64 , regardless of platform (32-bit or 64-bit). The following will all result in int64 dtypes. In [359]: pd . DataFrame ([ 1 , 2 ], columns = [ \"a\" ]) . dtypes Out[359]: a int64 dtype: object In [360]: pd . DataFrame ({ \"a\" : [ 1 , 2 ]}) . dtypes Out[360]: a int64 dtype: object In [361]: pd . DataFrame ({ \"a\" : 1 }, index = list ( range ( 2 ))) . dtypes Out[361]: a int64 dtype: object Note that Numpy will choose platform-dependent types when creating arrays. The following WILL result in int32 on 32-bit platform. In [362]: frame = pd . DataFrame ( np . array ([ 1 , 2 ])) upcasting # Types can potentially be upcasted when combined with other types, meaning they are promoted from the current type (e.g. int to float ). In [363]: df3 = df1 . reindex_like ( df2 ) . fillna ( value = 0.0 ) + df2 In [364]: df3 Out[364]: A B C 0 1.047606 0.256090 26.0 1 3.497968 1.426469 86.0 2 -0.150862 -0.416203 46.0 3 0.724370 1.139976 212.0 4 -1.203098 -1.193477 26.0 5 1.346426 0.096706 7.0 6 -0.052599 -1.956850 184.0 7 -0.756495 -0.714337 206.0 In [365]: df3 . dtypes Out[365]: A float32 B float64 C float64 dtype: object DataFrame.to_numpy() will return the lower-common-denominator of the dtypes, meaning the dtype that can accommodate ALL of the types in the resulting homogeneous dtyped NumPy array. This can force some upcasting . In [366]: df3 . to_numpy () . dtype Out[366]: dtype('float64') astype # You can use the astype() method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass copy=False to change this behavior). In addition, they will raise an exception if the astype operation is invalid. Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation. In [367]: df3 Out[367]: A B C 0 1.047606 0.256090 26.0 1 3.497968 1.426469 86.0 2 -0.150862 -0.416203 46.0 3 0.724370 1.139976 212.0 4 -1.203098 -1.193477 26.0 5 1.346426 0.096706 7.0 6 -0.052599 -1.956850 184.0 7 -0.756495 -0.714337 206.0 In [368]: df3 . dtypes Out[368]: A float32 B float64 C float64 dtype: object # conversion of dtypes In [369]: df3 . astype ( \"float32\" ) . dtypes Out[369]: A float32 B float32 C float32 dtype: object Convert a subset of columns to a specified type using astype() . In [370]: dft = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ], \"c\" : [ 7 , 8 , 9 ]}) In [371]: dft [[ \"a\" , \"b\" ]] = dft [[ \"a\" , \"b\" ]] . astype ( np . uint8 ) In [372]: dft Out[372]: a b c 0 1 4 7 1 2 5 8 2 3 6 9 In [373]: dft . dtypes Out[373]: a uint8 b uint8 c int64 dtype: object Convert certain columns to a specific dtype by passing a dict to astype() . In [374]: dft1 = pd . DataFrame ({ \"a\" : [ 1 , 0 , 1 ], \"b\" : [ 4 , 5 , 6 ], \"c\" : [ 7 , 8 , 9 ]}) In [375]: dft1 = dft1 . astype ({ \"a\" : np . bool_ , \"c\" : np . float64 }) In [376]: dft1 Out[376]: a b c 0 True 4 7.0 1 False 5 8.0 2 True 6 9.0 In [377]: dft1 . dtypes Out[377]: a bool b int64 c float64 dtype: object Note When trying to convert a subset of columns to a specified type using astype() and loc() , upcasting occurs. loc() tries to fit in what we are assigning to the current dtypes, while [] will overwrite them taking the dtype from the right hand side. Therefore the following piece of code produces the unintended result. In [378]: dft = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ], \"c\" : [ 7 , 8 , 9 ]}) In [379]: dft . loc [:, [ \"a\" , \"b\" ]] . astype ( np . uint8 ) . dtypes Out[379]: a uint8 b uint8 dtype: object In [380]: dft . loc [:, [ \"a\" , \"b\" ]] = dft . loc [:, [ \"a\" , \"b\" ]] . astype ( np . uint8 ) In [381]: dft . dtypes Out[381]: a int64 b int64 c int64 dtype: object object conversion # pandas offers various functions to try to force conversion of types from the object dtype to other types. In cases where the data is already of the correct type, but stored in an object array, the DataFrame.infer_objects() and Series.infer_objects() methods can be used to soft convert to the correct type. In [382]: import datetime In [383]: df = pd . DataFrame ( .....: [ .....: [ 1 , 2 ], .....: [ \"a\" , \"b\" ], .....: [ datetime . datetime ( 2016 , 3 , 2 ), datetime . datetime ( 2016 , 3 , 2 )], .....: ] .....: ) .....: In [384]: df = df . T In [385]: df Out[385]: 0 1 2 0 1 a 2016-03-02 00:00:00 1 2 b 2016-03-02 00:00:00 In [386]: df . dtypes Out[386]: 0 object 1 object 2 object dtype: object Because the data was transposed the original inference stored all columns as object, which infer_objects will correct. In [387]: df . infer_objects () . dtypes Out[387]: 0 int64 1 object 2 datetime64[ns] dtype: object The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type: to_numeric() (conversion to numeric dtypes) In [388]: m = [ \"1.1\" , 2 , 3 ] In [389]: pd . to_numeric ( m ) Out[389]: array([1.1, 2. , 3. ]) to_datetime() (conversion to datetime objects) In [390]: import datetime In [391]: m = [ \"2016-07-09\" , datetime . datetime ( 2016 , 3 , 2 )] In [392]: pd . to_datetime ( m ) Out[392]: DatetimeIndex(['2016-07-09', '2016-03-02'], dtype='datetime64[ns]', freq=None) to_timedelta() (conversion to timedelta objects) In [393]: m = [ \"5us\" , pd . Timedelta ( \"1day\" )] In [394]: pd . to_timedelta ( m ) Out[394]: TimedeltaIndex(['0 days 00:00:00.000005', '1 days 00:00:00'], dtype='timedelta64[ns]', freq=None) To force a conversion, we can pass in an errors argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, errors='raise' , meaning that any errors encountered will be raised during the conversion process. However, if errors='coerce' , these errors will be ignored and pandas will convert problematic elements to pd.NaT (for datetime and timedelta) or np.nan (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing: In [395]: import datetime In [396]: m = [ \"apple\" , datetime . datetime ( 2016 , 3 , 2 )] In [397]: pd . to_datetime ( m , errors = \"coerce\" ) Out[397]: DatetimeIndex(['NaT', '2016-03-02'], dtype='datetime64[ns]', freq=None) In [398]: m = [ \"apple\" , 2 , 3 ] In [399]: pd . to_numeric ( m , errors = \"coerce\" ) Out[399]: array([nan, 2., 3.]) In [400]: m = [ \"apple\" , pd . Timedelta ( \"1day\" )] In [401]: pd . to_timedelta ( m , errors = \"coerce\" ) Out[401]: TimedeltaIndex([NaT, '1 days'], dtype='timedelta64[ns]', freq=None) In addition to object conversion, to_numeric() provides another argument downcast , which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory: In [402]: m = [ \"1\" , 2 , 3 ] In [403]: pd . to_numeric ( m , downcast = \"integer\" ) # smallest signed int dtype Out[403]: array([1, 2, 3], dtype=int8) In [404]: pd . to_numeric ( m , downcast = \"signed\" ) # same as 'integer' Out[404]: array([1, 2, 3], dtype=int8) In [405]: pd . to_numeric ( m , downcast = \"unsigned\" ) # smallest unsigned int dtype Out[405]: array([1, 2, 3], dtype=uint8) In [406]: pd . to_numeric ( m , downcast = \"float\" ) # smallest float dtype Out[406]: array([1., 2., 3.], dtype=float32) As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such as DataFrames. However, with apply() , we can âapplyâ the function over each column efficiently: In [407]: import datetime In [408]: df = pd . DataFrame ([[ \"2016-07-09\" , datetime . datetime ( 2016 , 3 , 2 )]] * 2 , dtype = \"O\" ) In [409]: df Out[409]: 0 1 0 2016-07-09 2016-03-02 00:00:00 1 2016-07-09 2016-03-02 00:00:00 In [410]: df . apply ( pd . to_datetime ) Out[410]: 0 1 0 2016-07-09 2016-03-02 1 2016-07-09 2016-03-02 In [411]: df = pd . DataFrame ([[ \"1.1\" , 2 , 3 ]] * 2 , dtype = \"O\" ) In [412]: df Out[412]: 0 1 2 0 1.1 2 3 1 1.1 2 3 In [413]: df . apply ( pd . to_numeric ) Out[413]: 0 1 2 0 1.1 2 3 1 1.1 2 3 In [414]: df = pd . DataFrame ([[ \"5us\" , pd . Timedelta ( \"1day\" )]] * 2 , dtype = \"O\" ) In [415]: df Out[415]: 0 1 0 5us 1 days 00:00:00 1 5us 1 days 00:00:00 In [416]: df . apply ( pd . to_timedelta ) Out[416]: 0 1 0 0 days 00:00:00.000005 1 days 1 0 days 00:00:00.000005 1 days gotchas # Performing selection operations on integer type data can easily upcast the data to floating . The dtype of the input data will be preserved in cases where nans are not introduced. See also Support for integer NA . In [417]: dfi = df3 . astype ( \"int32\" ) In [418]: dfi [ \"E\" ] = 1 In [419]: dfi Out[419]: A B C E 0 1 0 26 1 1 3 1 86 1 2 0 0 46 1 3 0 1 212 1 4 -1 -1 26 1 5 1 0 7 1 6 0 -1 184 1 7 0 0 206 1 In [420]: dfi . dtypes Out[420]: A int32 B int32 C int32 E int64 dtype: object In [421]: casted = dfi [ dfi > 0 ] In [422]: casted Out[422]: A B C E 0 1.0 NaN 26 1 1 3.0 1.0 86 1 2 NaN NaN 46 1 3 NaN 1.0 212 1 4 NaN NaN 26 1 5 1.0 NaN 7 1 6 NaN NaN 184 1 7 NaN NaN 206 1 In [423]: casted . dtypes Out[423]: A float64 B float64 C int32 E int64 dtype: object While float dtypes are unchanged. In [424]: dfa = df3 . copy () In [425]: dfa [ \"A\" ] = dfa [ \"A\" ] . astype ( \"float32\" ) In [426]: dfa . dtypes Out[426]: A float32 B float64 C float64 dtype: object In [427]: casted = dfa [ df2 > 0 ] In [428]: casted Out[428]: A B C 0 1.047606 0.256090 26.0 1 3.497968 1.426469 86.0 2 NaN NaN 46.0 3 NaN 1.139976 212.0 4 NaN NaN 26.0 5 1.346426 0.096706 7.0 6 NaN NaN 184.0 7 NaN NaN 206.0 In [429]: casted . dtypes Out[429]: A float32 B float64 C float64 dtype: object Selecting columns based on dtype # The select_dtypes() method implements subsetting of columns based on their dtype . First, letâs create a DataFrame with a slew of different dtypes: In [430]: df = pd . DataFrame ( .....: { .....: \"string\" : list ( \"abc\" ), .....: \"int64\" : list ( range ( 1 , 4 )), .....: \"uint8\" : np . arange ( 3 , 6 ) . astype ( \"u1\" ), .....: \"float64\" : np . arange ( 4.0 , 7.0 ), .....: \"bool1\" : [ True , False , True ], .....: \"bool2\" : [ False , True , False ], .....: \"dates\" : pd . date_range ( \"now\" , periods = 3 ), .....: \"category\" : pd . Series ( list ( \"ABC\" )) . astype ( \"category\" ), .....: } .....: ) .....: In [431]: df [ \"tdeltas\" ] = df . dates . diff () In [432]: df [ \"uint64\" ] = np . arange ( 3 , 6 ) . astype ( \"u8\" ) In [433]: df [ \"other_dates\" ] = pd . date_range ( \"20130101\" , periods = 3 ) In [434]: df [ \"tz_aware_dates\" ] = pd . date_range ( \"20130101\" , periods = 3 , tz = \"US/Eastern\" ) In [435]: df Out[435]: string int64 uint8 ... uint64 other_dates tz_aware_dates 0 a 1 3 ... 3 2013-01-01 2013-01-01 00:00:00-05:00 1 b 2 4 ... 4 2013-01-02 2013-01-02 00:00:00-05:00 2 c 3 5 ... 5 2013-01-03 2013-01-03 00:00:00-05:00 [3 rows x 12 columns] And the dtypes: In [436]: df . dtypes Out[436]: string object int64 int64 uint8 uint8 float64 float64 bool1 bool bool2 bool dates datetime64[ns] category category tdeltas timedelta64[ns] uint64 uint64 other_dates datetime64[ns] tz_aware_dates datetime64[ns, US/Eastern] dtype: object select_dtypes() has two parameters include and exclude that allow you to say âgive me the columns with these dtypesâ ( include ) and/or âgive the columns without these dtypesâ ( exclude ). For example, to select bool columns: In [437]: df . select_dtypes ( include = [ bool ]) Out[437]: bool1 bool2 0 True False 1 False True 2 True False You can also pass the name of a dtype in the NumPy dtype hierarchy : In [438]: df . select_dtypes ( include = [ \"bool\" ]) Out[438]: bool1 bool2 0 True False 1 False True 2 True False select_dtypes() also works with generic dtypes as well. For example, to select all numeric and boolean columns while excluding unsigned integers: In [439]: df . select_dtypes ( include = [ \"number\" , \"bool\" ], exclude = [ \"unsignedinteger\" ]) Out[439]: int64 float64 bool1 bool2 tdeltas 0 1 4.0 True False NaT 1 2 5.0 False True 1 days 2 3 6.0 True False 1 days To select string columns you must use the object dtype: In [440]: df . select_dtypes ( include = [ \"object\" ]) Out[440]: string 0 a 1 b 2 c To see all the child dtypes of a generic dtype like numpy.number you can define a function that returns a tree of child dtypes: In [441]: def subdtypes ( dtype ): .....: subs = dtype . __subclasses__ () .....: if not subs : .....: return dtype .....: return [ dtype , [ subdtypes ( dt ) for dt in subs ]] .....: All NumPy dtypes are subclasses of numpy.generic : In [442]: subdtypes ( np . generic ) Out[442]: [numpy.generic, [[numpy.number, [[numpy.integer, [[numpy.signedinteger, [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.longlong, numpy.timedelta64]], [numpy.unsignedinteger, [numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, numpy.ulonglong]]]], [numpy.inexact, [[numpy.floating, [numpy.float16, numpy.float32, numpy.float64, numpy.longdouble]], [numpy.complexfloating, [numpy.complex64, numpy.complex128, numpy.clongdouble]]]]]], [numpy.flexible, [[numpy.character, [numpy.bytes_, numpy.str_]], [numpy.void, [numpy.record]]]], numpy.bool_, numpy.datetime64, numpy.object_]] Note pandas also defines the types category , and datetime64[ns, tz] , which are not integrated into the normal NumPy hierarchy and wonât show up with the above function. previous Intro to data structures next IO tools (text, CSV, HDF5, â¦) On this page Head and tail Attributes and underlying data Accelerated operations Flexible binary operations Matching / broadcasting behavior Missing data / operations with fill values Flexible comparisons Boolean reductions Comparing if objects are equivalent Comparing array-like objects Combining overlapping data sets General DataFrame combine Descriptive statistics Summarizing data: describe Index of min/max values Value counts (histogramming) / mode Discretization and quantiling Function application Tablewise function application Row or column-wise function application Aggregation API Aggregating with multiple functions Aggregating with a dict Custom describe Transform API Transform with multiple functions Transforming with a dict Applying elementwise functions Reindexing and altering labels Reindexing to align with another object Aligning objects with each other with align Filling while reindexing Limits on filling while reindexing Dropping labels from an axis Renaming / mapping labels Iteration items iterrows itertuples .dt accessor Vectorized string methods Sorting By index By values By indexes and values searchsorted smallest / largest values Sorting by a MultiIndex column Copying dtypes defaults upcasting astype object conversion gotchas Selecting columns based on dtype Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/basics.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.andrews_curves # pandas.plotting. andrews_curves ( frame , class_column , ax = None , samples = 200 , color = None , colormap = None , ** kwargs ) [source] # Generate a matplotlib plot for visualizing clusters of multivariate data. Andrews curves have the functional form: \\[f(t) = \\frac{x_1}{\\sqrt{2}} + x_2 \\sin(t) + x_3 \\cos(t) + x_4 \\sin(2t) + x_5 \\cos(2t) + \\cdots\\] Where \\(x\\) coefficients correspond to the values of each dimension and \\(t\\) is linearly spaced between \\(-\\pi\\) and \\(+\\pi\\) . Each row of frame then corresponds to a single curve. Parameters : frame DataFrame Data to be plotted, preferably normalized to (0.0, 1.0). class_column label Name of the column containing class names. ax axes object, default None Axes to use. samples int Number of points to plot in each curve. color str, list[str] or tuple[str], optional Colors to use for the different classes. Colors can be strings or 3-element floating point RGB values. colormap str or matplotlib colormap object, default None Colormap to select colors from. If a string, load colormap with that name from matplotlib. **kwargs Options to pass to matplotlib plotting method. Returns : matplotlib.axes.Axes Examples >>> df = pd . read_csv ( ... 'https://raw.githubusercontent.com/pandas-dev/' ... 'pandas/main/pandas/tests/io/data/csv/iris.csv' ... ) >>> pd . plotting . andrews_curves ( df , 'Name' ) previous Plotting next pandas.plotting.autocorrelation_plot On this page andrews_curves() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.andrews_curves.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Resampling Resampling # pandas.api.typing.Resampler instances are returned by resample calls: pandas.DataFrame.resample() , pandas.Series.resample() . Indexing, iteration # Resampler.__iter__ () Groupby iterator. Resampler.groups Dict {group name -> group labels}. Resampler.indices Dict {group name -> group indices}. Resampler.get_group (name[,Â obj]) Construct DataFrame from group with provided name. Function application # Resampler.apply ([func]) Aggregate using one or more operations over the specified axis. Resampler.aggregate ([func]) Aggregate using one or more operations over the specified axis. Resampler.transform (arg,Â *args,Â **kwargs) Call function producing a like-indexed Series on each group. Resampler.pipe (func,Â *args,Â **kwargs) Apply a func with arguments to this Resampler object and return its result. Upsampling # Resampler.ffill ([limit]) Forward fill the values. Resampler.bfill ([limit]) Backward fill the new missing values in the resampled data. Resampler.nearest ([limit]) Resample by using the nearest value. Resampler.fillna (method[,Â limit]) Fill missing values introduced by upsampling. Resampler.asfreq ([fill_value]) Return the values at the new freq, essentially a reindex. Resampler.interpolate ([method,Â axis,Â limit,Â ...]) Interpolate values between target timestamps according to different methods. Computations / descriptive stats # Resampler.count () Compute count of group, excluding missing values. Resampler.nunique (*args,Â **kwargs) Return number of unique elements in the group. Resampler.first ([numeric_only,Â min_count,Â ...]) Compute the first entry of each column within each group. Resampler.last ([numeric_only,Â min_count,Â skipna]) Compute the last entry of each column within each group. Resampler.max ([numeric_only,Â min_count]) Compute max value of group. Resampler.mean ([numeric_only]) Compute mean of groups, excluding missing values. Resampler.median ([numeric_only]) Compute median of groups, excluding missing values. Resampler.min ([numeric_only,Â min_count]) Compute min value of group. Resampler.ohlc (*args,Â **kwargs) Compute open, high, low and close values of a group, excluding missing values. Resampler.prod ([numeric_only,Â min_count]) Compute prod of group values. Resampler.size () Compute group sizes. Resampler.sem ([ddof,Â numeric_only]) Compute standard error of the mean of groups, excluding missing values. Resampler.std ([ddof,Â numeric_only]) Compute standard deviation of groups, excluding missing values. Resampler.sum ([numeric_only,Â min_count]) Compute sum of group values. Resampler.var ([ddof,Â numeric_only]) Compute variance of groups, excluding missing values. Resampler.quantile ([q]) Return value at the given quantile. previous pandas.core.groupby.SeriesGroupBy.plot next pandas.core.resample.Resampler.__iter__ On this page Indexing, iteration Function application Upsampling Computations / descriptive stats Show Source",
    "url": "https://pandas.pydata.org/docs/reference/resampling.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.table # pandas.plotting. table ( ax , data , ** kwargs ) [source] # Helper function to convert DataFrame and Series to matplotlib.table. Parameters : ax Matplotlib axes object data DataFrame or Series Data for table contents. **kwargs Keyword arguments to be passed to matplotlib.table.table. If rowLabels or colLabels is not specified, data index or column name will be used. Returns : matplotlib table object Examples >>> import matplotlib.pyplot as plt >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 ], 'B' : [ 3 , 4 ]}) >>> fix , ax = plt . subplots () >>> ax . axis ( 'off' ) (0.0, 1.0, 0.0, 1.0) >>> table = pd . plotting . table ( ax , df , loc = 'center' , ... cellLoc = 'center' , colWidths = list ([ .2 , .2 ])) previous pandas.plotting.scatter_matrix next Options and settings On this page table() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.table.html"
  },
  {
    "library": "pandas",
    "source": "tutorial",
    "text": "User Guide Nullable... Nullable integer data type # Note IntegerArray is currently experimental. Its API or implementation may change without warning. Uses pandas.NA as the missing value. In Working with missing data , we saw that pandas primarily uses NaN to represent missing data. Because NaN is a float, this forces an array of integers with any missing values to become floating point. In some cases, this may not matter much. But if your integer column is, say, an identifier, casting to float can be problematic. Some integers cannot even be represented as floating point numbers. Construction # pandas can represent integer data with possibly missing values using arrays.IntegerArray . This is an extension type implemented within pandas. In [1]: arr = pd . array ([ 1 , 2 , None ], dtype = pd . Int64Dtype ()) In [2]: arr Out[2]: <IntegerArray> [1, 2, <NA>] Length: 3, dtype: Int64 Or the string alias \"Int64\" (note the capital \"I\" ) to differentiate from NumPyâs 'int64' dtype: In [3]: pd . array ([ 1 , 2 , np . nan ], dtype = \"Int64\" ) Out[3]: <IntegerArray> [1, 2, <NA>] Length: 3, dtype: Int64 All NA-like values are replaced with pandas.NA . In [4]: pd . array ([ 1 , 2 , np . nan , None , pd . NA ], dtype = \"Int64\" ) Out[4]: <IntegerArray> [1, 2, <NA>, <NA>, <NA>] Length: 5, dtype: Int64 This array can be stored in a DataFrame or Series like any NumPy array. In [5]: pd . Series ( arr ) Out[5]: 0 1 1 2 2 <NA> dtype: Int64 You can also pass the list-like object to the Series constructor with the dtype. Warning Currently pandas.array() and pandas.Series() use different rules for dtype inference. pandas.array() will infer a nullable-integer dtype In [6]: pd . array ([ 1 , None ]) Out[6]: <IntegerArray> [1, <NA>] Length: 2, dtype: Int64 In [7]: pd . array ([ 1 , 2 ]) Out[7]: <IntegerArray> [1, 2] Length: 2, dtype: Int64 For backwards-compatibility, Series infers these as either integer or float dtype. In [8]: pd . Series ([ 1 , None ]) Out[8]: 0 1.0 1 NaN dtype: float64 In [9]: pd . Series ([ 1 , 2 ]) Out[9]: 0 1 1 2 dtype: int64 We recommend explicitly providing the dtype to avoid confusion. In [10]: pd . array ([ 1 , None ], dtype = \"Int64\" ) Out[10]: <IntegerArray> [1, <NA>] Length: 2, dtype: Int64 In [11]: pd . Series ([ 1 , None ], dtype = \"Int64\" ) Out[11]: 0 1 1 <NA> dtype: Int64 In the future, we may provide an option for Series to infer a nullable-integer dtype. Operations # Operations involving an integer array will behave similar to NumPy arrays. Missing values will be propagated, and the data will be coerced to another dtype if needed. In [12]: s = pd . Series ([ 1 , 2 , None ], dtype = \"Int64\" ) # arithmetic In [13]: s + 1 Out[13]: 0 2 1 3 2 <NA> dtype: Int64 # comparison In [14]: s == 1 Out[14]: 0 True 1 False 2 <NA> dtype: boolean # slicing operation In [15]: s . iloc [ 1 : 3 ] Out[15]: 1 2 2 <NA> dtype: Int64 # operate with other dtypes In [16]: s + s . iloc [ 1 : 3 ] . astype ( \"Int8\" ) Out[16]: 0 <NA> 1 4 2 <NA> dtype: Int64 # coerce when needed In [17]: s + 0.01 Out[17]: 0 1.01 1 2.01 2 <NA> dtype: Float64 These dtypes can operate as part of a DataFrame . In [18]: df = pd . DataFrame ({ \"A\" : s , \"B\" : [ 1 , 1 , 3 ], \"C\" : list ( \"aab\" )}) In [19]: df Out[19]: A B C 0 1 1 a 1 2 1 a 2 <NA> 3 b In [20]: df . dtypes Out[20]: A Int64 B int64 C object dtype: object These dtypes can be merged, reshaped & casted. In [21]: pd . concat ([ df [[ \"A\" ]], df [[ \"B\" , \"C\" ]]], axis = 1 ) . dtypes Out[21]: A Int64 B int64 C object dtype: object In [22]: df [ \"A\" ] . astype ( float ) Out[22]: 0 1.0 1 2.0 2 NaN Name: A, dtype: float64 Reduction and groupby operations such as sum() work as well. In [23]: df . sum ( numeric_only = True ) Out[23]: A 3 B 5 dtype: Int64 In [24]: df . sum () Out[24]: A 3 B 5 C aab dtype: object In [25]: df . groupby ( \"B\" ) . A . sum () Out[25]: B 1 3 3 0 Name: A, dtype: Int64 Scalar NA Value # arrays.IntegerArray uses pandas.NA as its scalar missing value. Slicing a single element thatâs missing will return pandas.NA In [26]: a = pd . array ([ 1 , None ], dtype = \"Int64\" ) In [27]: a [ 1 ] Out[27]: <NA> previous Categorical data next Nullable Boolean data type On this page Construction Operations Scalar NA Value Show Source",
    "url": "https://pandas.pydata.org/docs/user_guide/integer_na.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Extensions pandas.api.e... pandas.api.extensions.register_series_accessor # pandas.api.extensions. register_series_accessor ( name ) [source] # Register a custom accessor on Series objects. Parameters : name str Name under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute. Returns : callable A class decorator. See also register_dataframe_accessor Register a custom accessor on DataFrame objects. register_series_accessor Register a custom accessor on Series objects. register_index_accessor Register a custom accessor on Index objects. Notes When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be def __init__ ( self , pandas_object ): # noqa: E999 ... For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype. >>> pd . Series ([ 'a' , 'b' ]) . dt Traceback (most recent call last): ... AttributeError : Can only use .dt accessor with datetimelike values Examples In your library code: import pandas as pd @pd . api . extensions . register_dataframe_accessor ( \"geo\" ) class GeoAccessor : def __init__ ( self , pandas_obj ): self . _obj = pandas_obj @property def center ( self ): # return the geographic center point of this DataFrame lat = self . _obj . latitude lon = self . _obj . longitude return ( float ( lon . mean ()), float ( lat . mean ())) def plot ( self ): # plot this array's data on a map, e.g., using Cartopy pass Back in an interactive IPython session: In [1]: ds = pd . DataFrame ({ \"longitude\" : np . linspace ( 0 , 10 ), ...: \"latitude\" : np . linspace ( 0 , 20 )}) In [2]: ds . geo . center Out[2]: (5.0, 10.0) In [3]: ds . geo . plot () # plots data on a map previous pandas.api.extensions.register_dataframe_accessor next pandas.api.extensions.register_index_accessor On this page register_series_accessor() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.register_series_accessor.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Extensions pandas.api.i... pandas.api.indexers.check_array_indexer # pandas.api.indexers. check_array_indexer ( array , indexer ) [source] # Check if indexer is a valid array indexer for array . For a boolean mask, array and indexer are checked to have the same length. The dtype is validated, and if it is an integer or boolean ExtensionArray, it is checked if there are missing values present, and it is converted to the appropriate numpy array. Other dtypes will raise an error. Non-array indexers (integer, slice, Ellipsis, tuples, ..) are passed through as is. Parameters : array array-like The array that is being indexed (only used for the length). indexer array-like or list-like The array-like thatâs used to index. List-like input that is not yet a numpy array or an ExtensionArray is converted to one. Other input types are passed through as is. Returns : numpy.ndarray The validated indexer as a numpy array that can be used to index. Raises : IndexError When the lengths donât match. ValueError When indexer cannot be converted to a numpy ndarray to index (e.g. presence of missing values). See also api.types.is_bool_dtype Check if key is of boolean dtype. Examples When checking a boolean mask, a boolean ndarray is returned when the arguments are all valid. >>> mask = pd . array ([ True , False ]) >>> arr = pd . array ([ 1 , 2 ]) >>> pd . api . indexers . check_array_indexer ( arr , mask ) array([ True, False]) An IndexError is raised when the lengths donât match. >>> mask = pd . array ([ True , False , True ]) >>> pd . api . indexers . check_array_indexer ( arr , mask ) Traceback (most recent call last): ... IndexError : Boolean index has wrong length: 3 instead of 2. NA values in a boolean array are treated as False. >>> mask = pd . array ([ True , pd . NA ]) >>> pd . api . indexers . check_array_indexer ( arr , mask ) array([ True, False]) A numpy boolean mask will get passed through (if the length is correct): >>> mask = np . array ([ True , False ]) >>> pd . api . indexers . check_array_indexer ( arr , mask ) array([ True, False]) Similarly for integer indexers, an integer ndarray is returned when it is a valid indexer, otherwise an error is (for integer indexers, a matching length is not required): >>> indexer = pd . array ([ 0 , 2 ], dtype = \"Int64\" ) >>> arr = pd . array ([ 1 , 2 , 3 ]) >>> pd . api . indexers . check_array_indexer ( arr , indexer ) array([0, 2]) >>> indexer = pd . array ([ 0 , pd . NA ], dtype = \"Int64\" ) >>> pd . api . indexers . check_array_indexer ( arr , indexer ) Traceback (most recent call last): ... ValueError : Cannot index with an integer indexer containing NA values For non-integer/boolean dtypes, an appropriate error is raised: >>> indexer = np . array ([ 0. , 2. ], dtype = \"float64\" ) >>> pd . api . indexers . check_array_indexer ( arr , indexer ) Traceback (most recent call last): ... IndexError : arrays used as indices must be of integer or boolean type previous pandas.arrays.NumpyExtensionArray next Testing On this page check_array_indexer() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.api.indexers.check_array_indexer.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Input/output Input/output # Pickling # read_pickle (filepath_or_buffer[,Â ...]) Load pickled pandas object (or any object) from file. DataFrame.to_pickle (path,Â *[,Â compression,Â ...]) Pickle (serialize) object to file. Flat file # read_table (filepath_or_buffer,Â *[,Â sep,Â ...]) Read general delimited file into DataFrame. read_csv (filepath_or_buffer,Â *[,Â sep,Â ...]) Read a comma-separated values (csv) file into DataFrame. DataFrame.to_csv ([path_or_buf,Â sep,Â na_rep,Â ...]) Write object to a comma-separated values (csv) file. read_fwf (filepath_or_buffer,Â *[,Â colspecs,Â ...]) Read a table of fixed-width formatted lines into DataFrame. Clipboard # read_clipboard ([sep,Â dtype_backend]) Read text from clipboard and pass to read_csv() . DataFrame.to_clipboard (*[,Â excel,Â sep]) Copy object to the system clipboard. Excel # read_excel (io[,Â sheet_name,Â header,Â names,Â ...]) Read an Excel file into a pandas DataFrame . DataFrame.to_excel (excel_writer,Â *[,Â ...]) Write object to an Excel sheet. ExcelFile (path_or_buffer[,Â engine,Â ...]) Class for parsing tabular Excel sheets into DataFrame objects. ExcelFile.book ExcelFile.sheet_names ExcelFile.parse ([sheet_name,Â header,Â names,Â ...]) Parse specified sheet(s) into a DataFrame. Styler.to_excel (excel_writer[,Â sheet_name,Â ...]) Write Styler to an Excel sheet. ExcelWriter (path[,Â engine,Â date_format,Â ...]) Class for writing DataFrame objects into excel sheets. JSON # read_json (path_or_buf,Â *[,Â orient,Â typ,Â ...]) Convert a JSON string to pandas object. json_normalize (data[,Â record_path,Â meta,Â ...]) Normalize semi-structured JSON data into a flat table. DataFrame.to_json ([path_or_buf,Â orient,Â ...]) Convert the object to a JSON string. build_table_schema (data[,Â index,Â ...]) Create a Table schema from data . HTML # read_html (io,Â *[,Â match,Â flavor,Â header,Â ...]) Read HTML tables into a list of DataFrame objects. DataFrame.to_html ([buf,Â columns,Â col_space,Â ...]) Render a DataFrame as an HTML table. Styler.to_html ([buf,Â table_uuid,Â ...]) Write Styler to a file, buffer or string in HTML-CSS format. XML # read_xml (path_or_buffer,Â *[,Â xpath,Â ...]) Read XML document into a DataFrame object. DataFrame.to_xml ([path_or_buffer,Â index,Â ...]) Render a DataFrame to an XML document. Latex # DataFrame.to_latex ([buf,Â columns,Â header,Â ...]) Render object to a LaTeX tabular, longtable, or nested table. Styler.to_latex ([buf,Â column_format,Â ...]) Write Styler to a file, buffer or string in LaTeX format. HDFStore: PyTables (HDF5) # read_hdf (path_or_buf[,Â key,Â mode,Â errors,Â ...]) Read from the store, close it if we opened it. HDFStore.put (key,Â value[,Â format,Â index,Â ...]) Store object in HDFStore. HDFStore.append (key,Â value[,Â format,Â axes,Â ...]) Append to Table in file. HDFStore.get (key) Retrieve pandas object stored in file. HDFStore.select (key[,Â where,Â start,Â stop,Â ...]) Retrieve pandas object stored in file, optionally based on where criteria. HDFStore.info () Print detailed information on the store. HDFStore.keys ([include]) Return a list of keys corresponding to objects stored in HDFStore. HDFStore.groups () Return a list of all the top-level nodes. HDFStore.walk ([where]) Walk the pytables group hierarchy for pandas objects. Warning One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing. Feather # read_feather (path[,Â columns,Â use_threads,Â ...]) Load a feather-format object from the file path. DataFrame.to_feather (path,Â **kwargs) Write a DataFrame to the binary Feather format. Parquet # read_parquet (path[,Â engine,Â columns,Â ...]) Load a parquet object from the file path, returning a DataFrame. DataFrame.to_parquet ([path,Â engine,Â ...]) Write a DataFrame to the binary parquet format. ORC # read_orc (path[,Â columns,Â dtype_backend,Â ...]) Load an ORC object from the file path, returning a DataFrame. DataFrame.to_orc ([path,Â engine,Â index,Â ...]) Write a DataFrame to the ORC format. SAS # read_sas (filepath_or_buffer,Â *[,Â format,Â ...]) Read SAS files stored as either XPORT or SAS7BDAT format files. SPSS # read_spss (path[,Â usecols,Â ...]) Load an SPSS file from the file path, returning a DataFrame. SQL # read_sql_table (table_name,Â con[,Â schema,Â ...]) Read SQL database table into a DataFrame. read_sql_query (sql,Â con[,Â index_col,Â ...]) Read SQL query into a DataFrame. read_sql (sql,Â con[,Â index_col,Â ...]) Read SQL query or database table into a DataFrame. DataFrame.to_sql (name,Â con,Â *[,Â schema,Â ...]) Write records stored in a DataFrame to a SQL database. Google BigQuery # read_gbq (query[,Â project_id,Â index_col,Â ...]) (DEPRECATED) Load data from Google BigQuery. STATA # read_stata (filepath_or_buffer,Â *[,Â ...]) Read Stata file into DataFrame. DataFrame.to_stata (path,Â *[,Â convert_dates,Â ...]) Export DataFrame object to Stata dta format. StataReader.data_label Return data label of Stata file. StataReader.value_labels () Return a nested dict associating each variable name to its value and label. StataReader.variable_labels () Return a dict associating each variable name with corresponding label. StataWriter.write_file () Export DataFrame object to Stata dta format. previous API reference next pandas.read_pickle On this page Pickling Flat file Clipboard Excel JSON HTML XML Latex HDFStore: PyTables (HDF5) Feather Parquet ORC SAS SPSS SQL Google BigQuery STATA Show Source",
    "url": "https://pandas.pydata.org/docs/reference/io.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Date offsets Date offsets # DateOffset # DateOffset Standard kind of date increment used for a date range. Properties # DateOffset.freqstr Return a string representing the frequency. DateOffset.kwds Return a dict of extra parameters for the offset. DateOffset.name Return a string representing the base frequency. DateOffset.nanos DateOffset.normalize DateOffset.rule_code DateOffset.n DateOffset.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. DateOffset.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Methods # DateOffset.copy () Return a copy of the frequency. DateOffset.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). DateOffset.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. DateOffset.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. DateOffset.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. DateOffset.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. DateOffset.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. DateOffset.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. DateOffset.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BusinessDay # BusinessDay DateOffset subclass representing possibly n business days. Alias: BDay alias of BusinessDay Properties # BusinessDay.freqstr Return a string representing the frequency. BusinessDay.kwds Return a dict of extra parameters for the offset. BusinessDay.name Return a string representing the base frequency. BusinessDay.nanos BusinessDay.normalize BusinessDay.rule_code BusinessDay.n BusinessDay.weekmask BusinessDay.holidays BusinessDay.calendar Methods # BusinessDay.copy () Return a copy of the frequency. BusinessDay.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). BusinessDay.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BusinessDay.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BusinessDay.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BusinessDay.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BusinessDay.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BusinessDay.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BusinessDay.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BusinessHour # BusinessHour DateOffset subclass representing possibly n business hours. Properties # BusinessHour.freqstr Return a string representing the frequency. BusinessHour.kwds Return a dict of extra parameters for the offset. BusinessHour.name Return a string representing the base frequency. BusinessHour.nanos BusinessHour.normalize BusinessHour.rule_code BusinessHour.n BusinessHour.start BusinessHour.end BusinessHour.weekmask BusinessHour.holidays BusinessHour.calendar Methods # BusinessHour.copy () Return a copy of the frequency. BusinessHour.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). BusinessHour.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BusinessHour.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BusinessHour.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BusinessHour.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BusinessHour.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BusinessHour.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BusinessHour.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. CustomBusinessDay # CustomBusinessDay DateOffset subclass representing possibly n custom business days. Alias: CDay alias of CustomBusinessDay Properties # CustomBusinessDay.freqstr Return a string representing the frequency. CustomBusinessDay.kwds Return a dict of extra parameters for the offset. CustomBusinessDay.name Return a string representing the base frequency. CustomBusinessDay.nanos CustomBusinessDay.normalize CustomBusinessDay.rule_code CustomBusinessDay.n CustomBusinessDay.weekmask CustomBusinessDay.calendar CustomBusinessDay.holidays Methods # CustomBusinessDay.copy () Return a copy of the frequency. CustomBusinessDay.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). CustomBusinessDay.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. CustomBusinessDay.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. CustomBusinessDay.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. CustomBusinessDay.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. CustomBusinessDay.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. CustomBusinessDay.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. CustomBusinessDay.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. CustomBusinessHour # CustomBusinessHour DateOffset subclass representing possibly n custom business days. Properties # CustomBusinessHour.freqstr Return a string representing the frequency. CustomBusinessHour.kwds Return a dict of extra parameters for the offset. CustomBusinessHour.name Return a string representing the base frequency. CustomBusinessHour.nanos CustomBusinessHour.normalize CustomBusinessHour.rule_code CustomBusinessHour.n CustomBusinessHour.weekmask CustomBusinessHour.calendar CustomBusinessHour.holidays CustomBusinessHour.start CustomBusinessHour.end Methods # CustomBusinessHour.copy () Return a copy of the frequency. CustomBusinessHour.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). CustomBusinessHour.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. CustomBusinessHour.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. CustomBusinessHour.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. CustomBusinessHour.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. CustomBusinessHour.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. CustomBusinessHour.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. CustomBusinessHour.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. MonthEnd # MonthEnd DateOffset of one month end. Properties # MonthEnd.freqstr Return a string representing the frequency. MonthEnd.kwds Return a dict of extra parameters for the offset. MonthEnd.name Return a string representing the base frequency. MonthEnd.nanos MonthEnd.normalize MonthEnd.rule_code MonthEnd.n Methods # MonthEnd.copy () Return a copy of the frequency. MonthEnd.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). MonthEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. MonthEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. MonthEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. MonthEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. MonthEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. MonthEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. MonthEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. MonthBegin # MonthBegin DateOffset of one month at beginning. Properties # MonthBegin.freqstr Return a string representing the frequency. MonthBegin.kwds Return a dict of extra parameters for the offset. MonthBegin.name Return a string representing the base frequency. MonthBegin.nanos MonthBegin.normalize MonthBegin.rule_code MonthBegin.n Methods # MonthBegin.copy () Return a copy of the frequency. MonthBegin.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). MonthBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. MonthBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. MonthBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. MonthBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. MonthBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. MonthBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. MonthBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BusinessMonthEnd # BusinessMonthEnd DateOffset increments between the last business day of the month. Alias: BMonthEnd alias of BusinessMonthEnd Properties # BusinessMonthEnd.freqstr Return a string representing the frequency. BusinessMonthEnd.kwds Return a dict of extra parameters for the offset. BusinessMonthEnd.name Return a string representing the base frequency. BusinessMonthEnd.nanos BusinessMonthEnd.normalize BusinessMonthEnd.rule_code BusinessMonthEnd.n Methods # BusinessMonthEnd.copy () Return a copy of the frequency. BusinessMonthEnd.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). BusinessMonthEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BusinessMonthEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BusinessMonthEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BusinessMonthEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BusinessMonthEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BusinessMonthEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BusinessMonthEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BusinessMonthBegin # BusinessMonthBegin DateOffset of one month at the first business day. Alias: BMonthBegin alias of BusinessMonthBegin Properties # BusinessMonthBegin.freqstr Return a string representing the frequency. BusinessMonthBegin.kwds Return a dict of extra parameters for the offset. BusinessMonthBegin.name Return a string representing the base frequency. BusinessMonthBegin.nanos BusinessMonthBegin.normalize BusinessMonthBegin.rule_code BusinessMonthBegin.n Methods # BusinessMonthBegin.copy () Return a copy of the frequency. BusinessMonthBegin.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). BusinessMonthBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BusinessMonthBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BusinessMonthBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BusinessMonthBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BusinessMonthBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BusinessMonthBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BusinessMonthBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. CustomBusinessMonthEnd # CustomBusinessMonthEnd DateOffset subclass representing custom business month(s). Alias: CBMonthEnd alias of CustomBusinessMonthEnd Properties # CustomBusinessMonthEnd.freqstr Return a string representing the frequency. CustomBusinessMonthEnd.kwds Return a dict of extra parameters for the offset. CustomBusinessMonthEnd.m_offset CustomBusinessMonthEnd.name Return a string representing the base frequency. CustomBusinessMonthEnd.nanos CustomBusinessMonthEnd.normalize CustomBusinessMonthEnd.rule_code CustomBusinessMonthEnd.n CustomBusinessMonthEnd.weekmask CustomBusinessMonthEnd.calendar CustomBusinessMonthEnd.holidays Methods # CustomBusinessMonthEnd.copy () Return a copy of the frequency. CustomBusinessMonthEnd.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). CustomBusinessMonthEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. CustomBusinessMonthEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. CustomBusinessMonthEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. CustomBusinessMonthEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. CustomBusinessMonthEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. CustomBusinessMonthEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. CustomBusinessMonthEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. CustomBusinessMonthBegin # CustomBusinessMonthBegin DateOffset subclass representing custom business month(s). Alias: CBMonthBegin alias of CustomBusinessMonthBegin Properties # CustomBusinessMonthBegin.freqstr Return a string representing the frequency. CustomBusinessMonthBegin.kwds Return a dict of extra parameters for the offset. CustomBusinessMonthBegin.m_offset CustomBusinessMonthBegin.name Return a string representing the base frequency. CustomBusinessMonthBegin.nanos CustomBusinessMonthBegin.normalize CustomBusinessMonthBegin.rule_code CustomBusinessMonthBegin.n CustomBusinessMonthBegin.weekmask CustomBusinessMonthBegin.calendar CustomBusinessMonthBegin.holidays Methods # CustomBusinessMonthBegin.copy () Return a copy of the frequency. CustomBusinessMonthBegin.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). CustomBusinessMonthBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. CustomBusinessMonthBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. CustomBusinessMonthBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. CustomBusinessMonthBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. CustomBusinessMonthBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. CustomBusinessMonthBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. CustomBusinessMonthBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. SemiMonthEnd # SemiMonthEnd Two DateOffset's per month repeating on the last day of the month & day_of_month. Properties # SemiMonthEnd.freqstr Return a string representing the frequency. SemiMonthEnd.kwds Return a dict of extra parameters for the offset. SemiMonthEnd.name Return a string representing the base frequency. SemiMonthEnd.nanos SemiMonthEnd.normalize SemiMonthEnd.rule_code SemiMonthEnd.n SemiMonthEnd.day_of_month Methods # SemiMonthEnd.copy () Return a copy of the frequency. SemiMonthEnd.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). SemiMonthEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. SemiMonthEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. SemiMonthEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. SemiMonthEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. SemiMonthEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. SemiMonthEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. SemiMonthEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. SemiMonthBegin # SemiMonthBegin Two DateOffset's per month repeating on the first day of the month & day_of_month. Properties # SemiMonthBegin.freqstr Return a string representing the frequency. SemiMonthBegin.kwds Return a dict of extra parameters for the offset. SemiMonthBegin.name Return a string representing the base frequency. SemiMonthBegin.nanos SemiMonthBegin.normalize SemiMonthBegin.rule_code SemiMonthBegin.n SemiMonthBegin.day_of_month Methods # SemiMonthBegin.copy () Return a copy of the frequency. SemiMonthBegin.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). SemiMonthBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. SemiMonthBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. SemiMonthBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. SemiMonthBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. SemiMonthBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. SemiMonthBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. SemiMonthBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Week # Week Weekly offset. Properties # Week.freqstr Return a string representing the frequency. Week.kwds Return a dict of extra parameters for the offset. Week.name Return a string representing the base frequency. Week.nanos Week.normalize Week.rule_code Week.n Week.weekday Methods # Week.copy () Return a copy of the frequency. Week.is_anchored () Return boolean whether the frequency is a unit frequency (n=1). Week.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Week.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Week.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Week.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Week.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Week.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Week.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. WeekOfMonth # WeekOfMonth Describes monthly dates like \"the Tuesday of the 2nd week of each month\". Properties # WeekOfMonth.freqstr Return a string representing the frequency. WeekOfMonth.kwds Return a dict of extra parameters for the offset. WeekOfMonth.name Return a string representing the base frequency. WeekOfMonth.nanos WeekOfMonth.normalize WeekOfMonth.rule_code WeekOfMonth.n WeekOfMonth.week Methods # WeekOfMonth.copy () Return a copy of the frequency. WeekOfMonth.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). WeekOfMonth.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. WeekOfMonth.weekday WeekOfMonth.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. WeekOfMonth.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. WeekOfMonth.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. WeekOfMonth.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. WeekOfMonth.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. WeekOfMonth.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. LastWeekOfMonth # LastWeekOfMonth Describes monthly dates in last week of month. Properties # LastWeekOfMonth.freqstr Return a string representing the frequency. LastWeekOfMonth.kwds Return a dict of extra parameters for the offset. LastWeekOfMonth.name Return a string representing the base frequency. LastWeekOfMonth.nanos LastWeekOfMonth.normalize LastWeekOfMonth.rule_code LastWeekOfMonth.n LastWeekOfMonth.weekday LastWeekOfMonth.week Methods # LastWeekOfMonth.copy () Return a copy of the frequency. LastWeekOfMonth.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). LastWeekOfMonth.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. LastWeekOfMonth.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. LastWeekOfMonth.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. LastWeekOfMonth.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. LastWeekOfMonth.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. LastWeekOfMonth.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. LastWeekOfMonth.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BQuarterEnd # BQuarterEnd DateOffset increments between the last business day of each Quarter. Properties # BQuarterEnd.freqstr Return a string representing the frequency. BQuarterEnd.kwds Return a dict of extra parameters for the offset. BQuarterEnd.name Return a string representing the base frequency. BQuarterEnd.nanos BQuarterEnd.normalize BQuarterEnd.rule_code BQuarterEnd.n BQuarterEnd.startingMonth Methods # BQuarterEnd.copy () Return a copy of the frequency. BQuarterEnd.is_anchored () Return boolean whether the frequency is a unit frequency (n=1). BQuarterEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BQuarterEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BQuarterEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BQuarterEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BQuarterEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BQuarterEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BQuarterEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BQuarterBegin # BQuarterBegin DateOffset increments between the first business day of each Quarter. Properties # BQuarterBegin.freqstr Return a string representing the frequency. BQuarterBegin.kwds Return a dict of extra parameters for the offset. BQuarterBegin.name Return a string representing the base frequency. BQuarterBegin.nanos BQuarterBegin.normalize BQuarterBegin.rule_code BQuarterBegin.n BQuarterBegin.startingMonth Methods # BQuarterBegin.copy () Return a copy of the frequency. BQuarterBegin.is_anchored () Return boolean whether the frequency is a unit frequency (n=1). BQuarterBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BQuarterBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BQuarterBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BQuarterBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BQuarterBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BQuarterBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BQuarterBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. QuarterEnd # QuarterEnd DateOffset increments between Quarter end dates. Properties # QuarterEnd.freqstr Return a string representing the frequency. QuarterEnd.kwds Return a dict of extra parameters for the offset. QuarterEnd.name Return a string representing the base frequency. QuarterEnd.nanos QuarterEnd.normalize QuarterEnd.rule_code QuarterEnd.n QuarterEnd.startingMonth Methods # QuarterEnd.copy () Return a copy of the frequency. QuarterEnd.is_anchored () Return boolean whether the frequency is a unit frequency (n=1). QuarterEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. QuarterEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. QuarterEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. QuarterEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. QuarterEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. QuarterEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. QuarterEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. QuarterBegin # QuarterBegin DateOffset increments between Quarter start dates. Properties # QuarterBegin.freqstr Return a string representing the frequency. QuarterBegin.kwds Return a dict of extra parameters for the offset. QuarterBegin.name Return a string representing the base frequency. QuarterBegin.nanos QuarterBegin.normalize QuarterBegin.rule_code QuarterBegin.n QuarterBegin.startingMonth Methods # QuarterBegin.copy () Return a copy of the frequency. QuarterBegin.is_anchored () Return boolean whether the frequency is a unit frequency (n=1). QuarterBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. QuarterBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. QuarterBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. QuarterBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. QuarterBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. QuarterBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. QuarterBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BYearEnd # BYearEnd DateOffset increments between the last business day of the year. Properties # BYearEnd.freqstr Return a string representing the frequency. BYearEnd.kwds Return a dict of extra parameters for the offset. BYearEnd.name Return a string representing the base frequency. BYearEnd.nanos BYearEnd.normalize BYearEnd.rule_code BYearEnd.n BYearEnd.month Methods # BYearEnd.copy () Return a copy of the frequency. BYearEnd.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). BYearEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BYearEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BYearEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BYearEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BYearEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BYearEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BYearEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. BYearBegin # BYearBegin DateOffset increments between the first business day of the year. Properties # BYearBegin.freqstr Return a string representing the frequency. BYearBegin.kwds Return a dict of extra parameters for the offset. BYearBegin.name Return a string representing the base frequency. BYearBegin.nanos BYearBegin.normalize BYearBegin.rule_code BYearBegin.n BYearBegin.month Methods # BYearBegin.copy () Return a copy of the frequency. BYearBegin.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). BYearBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. BYearBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. BYearBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. BYearBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. BYearBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. BYearBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. BYearBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. YearEnd # YearEnd DateOffset increments between calendar year end dates. Properties # YearEnd.freqstr Return a string representing the frequency. YearEnd.kwds Return a dict of extra parameters for the offset. YearEnd.name Return a string representing the base frequency. YearEnd.nanos YearEnd.normalize YearEnd.rule_code YearEnd.n YearEnd.month Methods # YearEnd.copy () Return a copy of the frequency. YearEnd.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). YearEnd.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. YearEnd.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. YearEnd.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. YearEnd.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. YearEnd.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. YearEnd.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. YearEnd.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. YearBegin # YearBegin DateOffset increments between calendar year begin dates. Properties # YearBegin.freqstr Return a string representing the frequency. YearBegin.kwds Return a dict of extra parameters for the offset. YearBegin.name Return a string representing the base frequency. YearBegin.nanos YearBegin.normalize YearBegin.rule_code YearBegin.n YearBegin.month Methods # YearBegin.copy () Return a copy of the frequency. YearBegin.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). YearBegin.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. YearBegin.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. YearBegin.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. YearBegin.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. YearBegin.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. YearBegin.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. YearBegin.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. FY5253 # FY5253 Describes 52-53 week fiscal year. Properties # FY5253.freqstr Return a string representing the frequency. FY5253.kwds Return a dict of extra parameters for the offset. FY5253.name Return a string representing the base frequency. FY5253.nanos FY5253.normalize FY5253.rule_code FY5253.n FY5253.startingMonth FY5253.variation FY5253.weekday Methods # FY5253.copy () Return a copy of the frequency. FY5253.get_rule_code_suffix () FY5253.get_year_end (dt) FY5253.is_anchored () Return boolean whether the frequency is a unit frequency (n=1). FY5253.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. FY5253.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. FY5253.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. FY5253.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. FY5253.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. FY5253.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. FY5253.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. FY5253Quarter # FY5253Quarter DateOffset increments between business quarter dates for 52-53 week fiscal year. Properties # FY5253Quarter.freqstr Return a string representing the frequency. FY5253Quarter.kwds Return a dict of extra parameters for the offset. FY5253Quarter.name Return a string representing the base frequency. FY5253Quarter.nanos FY5253Quarter.normalize FY5253Quarter.rule_code FY5253Quarter.n FY5253Quarter.qtr_with_extra_week FY5253Quarter.startingMonth FY5253Quarter.variation FY5253Quarter.weekday Methods # FY5253Quarter.copy () Return a copy of the frequency. FY5253Quarter.get_rule_code_suffix () FY5253Quarter.get_weeks (dt) FY5253Quarter.is_anchored () Return boolean whether the frequency is a unit frequency (n=1). FY5253Quarter.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. FY5253Quarter.year_has_extra_week (dt) FY5253Quarter.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. FY5253Quarter.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. FY5253Quarter.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. FY5253Quarter.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. FY5253Quarter.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. FY5253Quarter.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Easter # Easter DateOffset for the Easter holiday using logic defined in dateutil. Properties # Easter.freqstr Return a string representing the frequency. Easter.kwds Return a dict of extra parameters for the offset. Easter.name Return a string representing the base frequency. Easter.nanos Easter.normalize Easter.rule_code Easter.n Methods # Easter.copy () Return a copy of the frequency. Easter.is_anchored () (DEPRECATED) Return boolean whether the frequency is a unit frequency (n=1). Easter.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Easter.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Easter.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Easter.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Easter.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Easter.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Easter.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Tick # Tick Properties # Tick.delta Tick.freqstr Return a string representing the frequency. Tick.kwds Return a dict of extra parameters for the offset. Tick.name Return a string representing the base frequency. Tick.nanos Return an integer of the total number of nanoseconds. Tick.normalize Tick.rule_code Tick.n Methods # Tick.copy () Return a copy of the frequency. Tick.is_anchored () (DEPRECATED) Return False. Tick.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Tick.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Tick.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Tick.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Tick.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Tick.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Tick.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Day # Day Offset n days. Properties # Day.delta Day.freqstr Return a string representing the frequency. Day.kwds Return a dict of extra parameters for the offset. Day.name Return a string representing the base frequency. Day.nanos Return an integer of the total number of nanoseconds. Day.normalize Day.rule_code Day.n Methods # Day.copy () Return a copy of the frequency. Day.is_anchored () (DEPRECATED) Return False. Day.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Day.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Day.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Day.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Day.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Day.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Day.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Hour # Hour Offset n hours. Properties # Hour.delta Hour.freqstr Return a string representing the frequency. Hour.kwds Return a dict of extra parameters for the offset. Hour.name Return a string representing the base frequency. Hour.nanos Return an integer of the total number of nanoseconds. Hour.normalize Hour.rule_code Hour.n Methods # Hour.copy () Return a copy of the frequency. Hour.is_anchored () (DEPRECATED) Return False. Hour.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Hour.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Hour.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Hour.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Hour.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Hour.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Hour.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Minute # Minute Offset n minutes. Properties # Minute.delta Minute.freqstr Return a string representing the frequency. Minute.kwds Return a dict of extra parameters for the offset. Minute.name Return a string representing the base frequency. Minute.nanos Return an integer of the total number of nanoseconds. Minute.normalize Minute.rule_code Minute.n Methods # Minute.copy () Return a copy of the frequency. Minute.is_anchored () (DEPRECATED) Return False. Minute.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Minute.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Minute.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Minute.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Minute.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Minute.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Minute.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Second # Second Offset n seconds. Properties # Second.delta Second.freqstr Return a string representing the frequency. Second.kwds Return a dict of extra parameters for the offset. Second.name Return a string representing the base frequency. Second.nanos Return an integer of the total number of nanoseconds. Second.normalize Second.rule_code Second.n Methods # Second.copy () Return a copy of the frequency. Second.is_anchored () (DEPRECATED) Return False. Second.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Second.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Second.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Second.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Second.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Second.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Second.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Milli # Milli Offset n milliseconds. Properties # Milli.delta Milli.freqstr Return a string representing the frequency. Milli.kwds Return a dict of extra parameters for the offset. Milli.name Return a string representing the base frequency. Milli.nanos Return an integer of the total number of nanoseconds. Milli.normalize Milli.rule_code Milli.n Methods # Milli.copy () Return a copy of the frequency. Milli.is_anchored () (DEPRECATED) Return False. Milli.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Milli.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Milli.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Milli.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Milli.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Milli.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Milli.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Micro # Micro Offset n microseconds. Properties # Micro.delta Micro.freqstr Return a string representing the frequency. Micro.kwds Return a dict of extra parameters for the offset. Micro.name Return a string representing the base frequency. Micro.nanos Return an integer of the total number of nanoseconds. Micro.normalize Micro.rule_code Micro.n Methods # Micro.copy () Return a copy of the frequency. Micro.is_anchored () (DEPRECATED) Return False. Micro.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Micro.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Micro.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Micro.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Micro.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Micro.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Micro.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Nano # Nano Offset n nanoseconds. Properties # Nano.delta Nano.freqstr Return a string representing the frequency. Nano.kwds Return a dict of extra parameters for the offset. Nano.name Return a string representing the base frequency. Nano.nanos Return an integer of the total number of nanoseconds. Nano.normalize Nano.rule_code Nano.n Methods # Nano.copy () Return a copy of the frequency. Nano.is_anchored () (DEPRECATED) Return False. Nano.is_on_offset (dt) Return boolean whether a timestamp intersects with this frequency. Nano.is_month_start (ts) Return boolean whether a timestamp occurs on the month start. Nano.is_month_end (ts) Return boolean whether a timestamp occurs on the month end. Nano.is_quarter_start (ts) Return boolean whether a timestamp occurs on the quarter start. Nano.is_quarter_end (ts) Return boolean whether a timestamp occurs on the quarter end. Nano.is_year_start (ts) Return boolean whether a timestamp occurs on the year start. Nano.is_year_end (ts) Return boolean whether a timestamp occurs on the year end. Frequencies # to_offset (freq[,Â is_period]) Return DateOffset object from string or datetime.timedelta object. previous pandas.PeriodIndex.from_ordinals next pandas.tseries.offsets.DateOffset On this page Date offsets DateOffset Properties Methods BusinessDay Properties Methods BusinessHour Properties Methods CustomBusinessDay Properties Methods CustomBusinessHour Properties Methods MonthEnd Properties Methods MonthBegin Properties Methods BusinessMonthEnd Properties Methods BusinessMonthBegin Properties Methods CustomBusinessMonthEnd Properties Methods CustomBusinessMonthBegin Properties Methods SemiMonthEnd Properties Methods SemiMonthBegin Properties Methods Week Properties Methods WeekOfMonth Properties Methods LastWeekOfMonth Properties Methods BQuarterEnd Properties Methods BQuarterBegin Properties Methods QuarterEnd Properties Methods QuarterBegin Properties Methods BYearEnd Properties Methods BYearBegin Properties Methods YearEnd Properties Methods YearBegin Properties Methods FY5253 Properties Methods FY5253Quarter Properties Methods Easter Properties Methods Tick Properties Methods Day Properties Methods Hour Properties Methods Minute Properties Methods Second Properties Methods Milli Properties Methods Micro Properties Methods Nano Properties Methods Frequencies Show Source",
    "url": "https://pandas.pydata.org/docs/reference/offset_frequency.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Extensions Extensions # These are primarily intended for library authors looking to extend pandas objects. api.extensions.register_extension_dtype (cls) Register an ExtensionType with pandas as class decorator. api.extensions.register_dataframe_accessor (name) Register a custom accessor on DataFrame objects. api.extensions.register_series_accessor (name) Register a custom accessor on Series objects. api.extensions.register_index_accessor (name) Register a custom accessor on Index objects. api.extensions.ExtensionDtype () A custom data type, to be paired with an ExtensionArray. api.extensions.ExtensionArray () Abstract base class for custom 1-D array types. arrays.NumpyExtensionArray (values[,Â copy]) A pandas ExtensionArray for NumPy data. Additionally, we have some utility methods for ensuring your object behaves correctly. api.indexers.check_array_indexer (array,Â indexer) Check if indexer is a valid array indexer for array . The sentinel pandas.api.extensions.no_default is used as the default value in some methods. Use an is comparison to check if the user provides a non-default value. previous pandas.set_eng_float_format next pandas.api.extensions.register_extension_dtype Show Source",
    "url": "https://pandas.pydata.org/docs/reference/extensions.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference Plotting pandas.plott... pandas.plotting.scatter_matrix # pandas.plotting. scatter_matrix ( frame , alpha = 0.5 , figsize = None , ax = None , grid = False , diagonal = 'hist' , marker = '.' , density_kwds = None , hist_kwds = None , range_padding = 0.05 , ** kwargs ) [source] # Draw a matrix of scatter plots. Parameters : frame DataFrame alpha float, optional Amount of transparency applied. figsize (float,float), optional A tuple (width, height) in inches. ax Matplotlib axis object, optional grid bool, optional Setting this to True will show the grid. diagonal {âhistâ, âkdeâ} Pick between âkdeâ and âhistâ for either Kernel Density Estimation or Histogram plot in the diagonal. marker str, optional Matplotlib marker type, default â.â. density_kwds keywords Keyword arguments to be passed to kernel density estimate plot. hist_kwds keywords Keyword arguments to be passed to hist function. range_padding float, default 0.05 Relative extension of axis range in x and y with respect to (x_max - x_min) or (y_max - y_min). **kwargs Keyword arguments to be passed to scatter function. Returns : numpy.ndarray A matrix of scatter plots. Examples >>> df = pd . DataFrame ( np . random . randn ( 1000 , 4 ), columns = [ 'A' , 'B' , 'C' , 'D' ]) >>> pd . plotting . scatter_matrix ( df , alpha = 0.2 ) array([[<Axes: xlabel='A', ylabel='A'>, <Axes: xlabel='B', ylabel='A'>, <Axes: xlabel='C', ylabel='A'>, <Axes: xlabel='D', ylabel='A'>], [<Axes: xlabel='A', ylabel='B'>, <Axes: xlabel='B', ylabel='B'>, <Axes: xlabel='C', ylabel='B'>, <Axes: xlabel='D', ylabel='B'>], [<Axes: xlabel='A', ylabel='C'>, <Axes: xlabel='B', ylabel='C'>, <Axes: xlabel='C', ylabel='C'>, <Axes: xlabel='D', ylabel='C'>], [<Axes: xlabel='A', ylabel='D'>, <Axes: xlabel='B', ylabel='D'>, <Axes: xlabel='C', ylabel='D'>, <Axes: xlabel='D', ylabel='D'>]], dtype=object) previous pandas.plotting.register_matplotlib_converters next pandas.plotting.table On this page scatter_matrix() Show Source",
    "url": "https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html"
  },
  {
    "library": "pandas",
    "source": "api_spec",
    "text": "API reference General functions General functions # Data manipulations # melt (frame[,Â id_vars,Â value_vars,Â var_name,Â ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. pivot (data,Â *,Â columns[,Â index,Â values]) Return reshaped DataFrame organized by given index / column values. pivot_table (data[,Â values,Â index,Â columns,Â ...]) Create a spreadsheet-style pivot table as a DataFrame. crosstab (index,Â columns[,Â values,Â rownames,Â ...]) Compute a simple cross tabulation of two (or more) factors. cut (x,Â bins[,Â right,Â labels,Â retbins,Â ...]) Bin values into discrete intervals. qcut (x,Â q[,Â labels,Â retbins,Â precision,Â ...]) Quantile-based discretization function. merge (left,Â right[,Â how,Â on,Â left_on,Â ...]) Merge DataFrame or named Series objects with a database-style join. merge_ordered (left,Â right[,Â on,Â left_on,Â ...]) Perform a merge for ordered data with optional filling/interpolation. merge_asof (left,Â right[,Â on,Â left_on,Â ...]) Perform a merge by key distance. concat (objs,Â *[,Â axis,Â join,Â ignore_index,Â ...]) Concatenate pandas objects along a particular axis. get_dummies (data[,Â prefix,Â prefix_sep,Â ...]) Convert categorical variable into dummy/indicator variables. from_dummies (data[,Â sep,Â default_category]) Create a categorical DataFrame from a DataFrame of dummy variables. factorize (values[,Â sort,Â use_na_sentinel,Â ...]) Encode the object as an enumerated type or categorical variable. unique (values) Return unique values based on a hash table. lreshape (data,Â groups[,Â dropna]) Reshape wide-format data to long. wide_to_long (df,Â stubnames,Â i,Â j[,Â sep,Â suffix]) Unpivot a DataFrame from wide to long format. Top-level missing data # isna (obj) Detect missing values for an array-like object. isnull (obj) Detect missing values for an array-like object. notna (obj) Detect non-missing values for an array-like object. notnull (obj) Detect non-missing values for an array-like object. Top-level dealing with numeric data # to_numeric (arg[,Â errors,Â downcast,Â ...]) Convert argument to a numeric type. Top-level dealing with datetimelike data # to_datetime (arg[,Â errors,Â dayfirst,Â ...]) Convert argument to datetime. to_timedelta (arg[,Â unit,Â errors]) Convert argument to timedelta. date_range ([start,Â end,Â periods,Â freq,Â tz,Â ...]) Return a fixed frequency DatetimeIndex. bdate_range ([start,Â end,Â periods,Â freq,Â tz,Â ...]) Return a fixed frequency DatetimeIndex with business day as the default. period_range ([start,Â end,Â periods,Â freq,Â name]) Return a fixed frequency PeriodIndex. timedelta_range ([start,Â end,Â periods,Â freq,Â ...]) Return a fixed frequency TimedeltaIndex with day as the default. infer_freq (index) Infer the most likely frequency given the input index. Top-level dealing with Interval data # interval_range ([start,Â end,Â periods,Â freq,Â ...]) Return a fixed frequency IntervalIndex. Top-level evaluation # eval (expr[,Â parser,Â engine,Â local_dict,Â ...]) Evaluate a Python expression as a string using various backends. Datetime formats # tseries.api.guess_datetime_format (dt_str[,Â ...]) Guess the datetime format of a given datetime string. Hashing # util.hash_array (vals[,Â encoding,Â hash_key,Â ...]) Given a 1d array, return an array of deterministic integers. util.hash_pandas_object (obj[,Â index,Â ...]) Return a data hash of the Index/Series/DataFrame. Importing from other DataFrame libraries # api.interchange.from_dataframe (df[,Â allow_copy]) Build a pd.DataFrame from any DataFrame supporting the interchange protocol. previous pandas.io.stata.StataWriter.write_file next pandas.melt On this page Data manipulations Top-level missing data Top-level dealing with numeric data Top-level dealing with datetimelike data Top-level dealing with Interval data Top-level evaluation Datetime formats Hashing Importing from other DataFrame libraries Show Source",
    "url": "https://pandas.pydata.org/docs/reference/general_functions.html"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Why do I get a SettingWithCopyWarning when using shift and dropna inside a function? In general, when I receive this warning\n/home/mo/mwe.py:7: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\n  Try using .loc[row_indexer,col_indexer] = value ...",
    "url": "https://stackoverflow.com/questions/79802076/why-do-i-get-a-settingwithcopywarning-when-using-shift-and-dropna-inside-a-funct"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Azure_Synapse - Fazzy match between column a and b in two tables in synapse [closed] I have two tables in synapse over a million records. I need to get column a and b from each table and find fuzzy match. But I face with less memory. Can you guide me how I can solve this problem?",
    "url": "https://stackoverflow.com/questions/79800378/azure-synapse-fazzy-match-between-column-a-and-b-in-two-tables-in-synapse"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Create a loop that gives the prices of gold for different dates from an API [closed] This is the API that I'm using: https://www.goldapi.io/dashboard\nBasically, what I want to achieve from this project is a Tkinter program that displays the prices of gold in different currencies on ...",
    "url": "https://stackoverflow.com/questions/79800010/create-a-loop-that-gives-the-prices-of-gold-for-different-dates-from-an-api"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Read public gsheet with python [closed] I am trying to get the data of the 1st sheet of this gsheet in python :\nhttps://docs.google.com/spreadsheets/d/e/2PACX-1vTN03UvJDm99byA6vQPZHKOCYVvfxLu1zkJAzdaKyROykzEKY2-Xl1rl1q5znZEf36m88dxMKsY2eaO/...",
    "url": "https://stackoverflow.com/questions/79799486/read-public-gsheet-with-python"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to create a column based on 2 other columns of the dataframe? I have a kivy app and at some point of the code a pandas dataframe loaded from excel and I managed to create already 2 columns filled with booleans.\nI need to create a third columns which content ...",
    "url": "https://stackoverflow.com/questions/79798995/how-to-create-a-column-based-on-2-other-columns-of-the-dataframe"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Combining two dataframes and keeping the average I'm new to coding, and I'm trying to combine the data from two weather stations into one new dataframe sorted by Datetime. I want this new dataframe to contain the average values of the two original ...",
    "url": "https://stackoverflow.com/questions/79798000/combining-two-dataframes-and-keeping-the-average"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How do you get specific data in unknown row from a csv file using known data from the same row? [duplicate] import geopy # used to get location\nfrom geopy.geocoders import Nominatim\nimport pandas as pd\nfrom pyproj import Transformer\n\ndef get_user_location(): # user location\n    geolocator = Nominatim(...",
    "url": "https://stackoverflow.com/questions/79797958/how-do-you-get-specific-data-in-unknown-row-from-a-csv-file-using-known-data-fro"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Pandas's to_datetime function and datetime format I have a .csv file with two columns (Date and Time). The time zone is \"Europe/Paris\" with a +02:00 hours shift. The file is structured in 2 parts with two datetime formats.\nDate\nTime\n08-11-...",
    "url": "https://stackoverflow.com/questions/79797838/pandass-to-datetime-function-and-datetime-format"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Creating a new pandas dataframe from shape I have information on total number of rows and number of columns for a new pandas dataframe\nimport pandas as pd\nnRow = 10\nnCol = 4\n\nBased on this information I want to create a new dataframe where ...",
    "url": "https://stackoverflow.com/questions/79796094/creating-a-new-pandas-dataframe-from-shape"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How can I write a matrix or a pandas DataFrame to an Excel file using openpyxl, without iterating cell by cell? [duplicate] I’d like to insert a 2D array (for example, a pandas DataFrame) into an existing Excel worksheet at a specific position (e.g., starting at cell M8), using openpyxl.\nIs there a way to assign the whole ...",
    "url": "https://stackoverflow.com/questions/79795953/how-can-i-write-a-matrix-or-a-pandas-dataframe-to-an-excel-file-using-openpyxl"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "pandas.read_csv uses only utf-8 encoding for django file upload I'm testing django using file uploads. Was facing a strange issue, when despide which encoding I choose, I'm always getting same error message that pandas is trying to decode with UTF-8\npd.read_csv(...",
    "url": "https://stackoverflow.com/questions/79795928/pandas-read-csv-uses-only-utf-8-encoding-for-django-file-upload"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "pandas crosstab with string as second parameter Is this code, which works, supposed to work?\nimport pandas as pd\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\npd.crosstab(penguins.species, \"count\")\nspecies\ncount\n...",
    "url": "https://stackoverflow.com/questions/79795855/pandas-crosstab-with-string-as-second-parameter"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Return only 3 numbers with duplicate values in each row from a dataframe [duplicate] From a dataframe I would like to return only the duplicates with 3 different numbers in each row:\ndf = pd.DataFrame([[4,6,10,21,30,4,6,21,33], # 4,6,21 this has 3 duplicate\n                  [1,2,4,16,...",
    "url": "https://stackoverflow.com/questions/79795483/return-only-3-numbers-with-duplicate-values-in-each-row-from-a-dataframe"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to drop duplicate values when merging dataframes I have a DataFrame that I want to merge and drop only duplicates values based on column name and row. For example, key_x and key_y has the\nsame values in the same row in row 0,3,10,12,15.\nMy DataFrame\n...",
    "url": "https://stackoverflow.com/questions/79794520/how-to-drop-duplicate-values-when-merging-dataframes"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "pywebview: error maximum recursion depth exceeded before pressing button when passing pandas/model objects in js_api I’m embedding a small UI with pywebview and want Python to JS live updates. I created a GPSSpoofingDetector class that loads a pickled sklearn model and a pandas test CSV. I want a JavaScript “Start” ...",
    "url": "https://stackoverflow.com/questions/79794345/pywebview-error-maximum-recursion-depth-exceeded-before-pressing-button-when-pa"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Combining Identically Indexed and Column Dataframes into 3d Dataframe I have 3 2D DataFrames, all with identical indexes (datetime range) and column names, but different data for these labels. I would like to combine these three 2D dataframes into 1 3D DataFrame with an ...",
    "url": "https://stackoverflow.com/questions/79793878/combining-identically-indexed-and-column-dataframes-into-3d-dataframe"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Calculating MultiIndex intersection to a given tolerance in an efficient way I have two DataFrames, data1 and data2, with 3-level multiindices. The first two levels are floats, and correspond to spatial coordinates (say longitude and latitude). The third level, time, is based ...",
    "url": "https://stackoverflow.com/questions/79793858/calculating-multiindex-intersection-to-a-given-tolerance-in-an-efficient-way"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Copying columns using loc or iloc, from one dataframe to another that is initialized with columns [closed] Copying columns from one dataframe to another empty dataframe that is initialized with columns.\nUsing loc the copy does not succeeds at all!\nUsing iloc the copy succeeds only if the source dataframe ...",
    "url": "https://stackoverflow.com/questions/79793690/copying-columns-using-loc-or-iloc-from-one-dataframe-to-another-that-is-initial"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Calculating mean for a column of arrays in pandas I have below pandas dataframe\nimport pandas as pd\nimport numpy as np\n\ndat = pd.DataFrame({\n    'A': [1,2,3],\n    'B': [[[np.nan, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], ...",
    "url": "https://stackoverflow.com/questions/79793610/calculating-mean-for-a-column-of-arrays-in-pandas"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "\"UserWarning: No artists with labels found to put in legend\" Error when trying to create a legend with labels from dataset I can't get legend labels to show up when I use 'CONDITION' (a longer string) as my x data and hue, however when I use CONDITION_N (a shorter string) as the hue then it appears. Why?\nWarning:\n/var/...",
    "url": "https://stackoverflow.com/questions/79793379/userwarning-no-artists-with-labels-found-to-put-in-legend-error-when-trying-t"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "convert a list of texts to numeric values [closed] fruits = ['apple', 'orange', 'banana']\n\nI want to convert each fruit to a numeric value or numeric array separately, not by LabelEncoder or OneHotEncoding the whole list. Can someone help fill in the ...",
    "url": "https://stackoverflow.com/questions/79792969/convert-a-list-of-texts-to-numeric-values"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Python/Pandas: Scaling baseline values across denomination blocks in a pricing table [closed] Develop a dynamic, Python-based table processor to normalize and populate incomplete financial/pricing data blocks within a structured grid. The goal is to enforce consistency across all denominations ...",
    "url": "https://stackoverflow.com/questions/79792769/python-pandas-scaling-baseline-values-across-denomination-blocks-in-a-pricing-t"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How can I use wildcard paths from a Pandas dataframe as required rule inputs and outputs in Snakemake? I have a Snakemake pipeline (https://github.com/V-Varga/SPOT-BGC/tree/main), where I generate input and output file names for various intermediate steps using wildcards that refer back to file and ...",
    "url": "https://stackoverflow.com/questions/79792470/how-can-i-use-wildcard-paths-from-a-pandas-dataframe-as-required-rule-inputs-and"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Error due to single-level dataframe merge with multi-level indexed dataframe # Read lookup file which only contains 5 columns.\ndf_lookup = pd.read_excel(\n    os.path.join(path, 'lookup.xlsx'),\n    index_col=[0, 1, 2, 3, 4])\n\n# sample df_lookup\n# |A |B |C |D |E |\n# |--|--|--|--|...",
    "url": "https://stackoverflow.com/questions/79792272/error-due-to-single-level-dataframe-merge-with-multi-level-indexed-dataframe"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to control the zorder values on superimposed bars in a histogram plot in matplotlib I have a list of three dataframes, each of them having four columns of interest. I want to create a figure with four subplots (one for each column). In each subplot, first, I want to create a ...",
    "url": "https://stackoverflow.com/questions/79791271/how-to-control-the-zorder-values-on-superimposed-bars-in-a-histogram-plot-in-mat"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Failing to fill empty date values with numpy nan I have below code\nimport pandas as pd\nimport numpy as np\ndat = pd.DataFrame({'A' : [1,2,3,4,5], 'B' : ['2002-01-01', '2003-01-01', '2004-01-01', '2004-01-01', '2005-01-01']})\ndat['B'] = pd.to_datetime(...",
    "url": "https://stackoverflow.com/questions/79790995/failing-to-fill-empty-date-values-with-numpy-nan"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to write a pandas-compatible, non-elementary expression in narwhals I'm working with the narwhals package and I'm trying to write an expression that is:\n\napplied over groups using .over()\nNon-elementary/chained (longer than a single operation)\nWorks when the native df ...",
    "url": "https://stackoverflow.com/questions/79790533/how-to-write-a-pandas-compatible-non-elementary-expression-in-narwhals"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Create an incremental suffix for values in a pandas column that have duplicate values in another column Setup\nI have a dataframe, df\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        'Name':['foo','foo','foo','bar','bar','bar','baz','baz','baz'],\n        'Color':['red','blue','red','green','green','...",
    "url": "https://stackoverflow.com/questions/79790517/create-an-incremental-suffix-for-values-in-a-pandas-column-that-have-duplicate-v"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "transformation logic for real-time inference service I have developed an XGBoost model, the data transformations where done using Pandas for training the model. For real-time inference the data comes from the HTTP request single object/record that ...",
    "url": "https://stackoverflow.com/questions/79790417/transformation-logic-for-real-time-inference-service"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Index in to two specific dates on Pandas dataframe [closed] I have a pandas dataframe where the index is datetime. I learned that I can index in to a specific date using this code:\nselected_date_df = df.loc['yyyy-mm-dd']\n\nI can also find data between two dates ...",
    "url": "https://stackoverflow.com/questions/79790365/index-in-to-two-specific-dates-on-pandas-dataframe"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to change names of pandas MultiIndex using Styler Let's assume we have the following:\nmidx = pd.MultiIndex.from_product(\n    [[0, 1], [0, 1], [0, 1]],\n    names=['L1', 'L2', 'L3'])\ndf = pd.DataFrame({\"col\": list(range(8))}, index=midx)\n\nNow,...",
    "url": "https://stackoverflow.com/questions/79790153/how-to-change-names-of-pandas-multiindex-using-styler"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Increase the date by number of months in pandas I have below pandas data frame\nimport pandas as pd\nimport numpy as np\ndat = pd.DataFrame({'A' : [1,2,3,4,5], 'B' : ['2002-01-01', '2003-01-01', '2004-01-01', '2004-01-01', '2005-01-01']})\ndat['A'] = ...",
    "url": "https://stackoverflow.com/questions/79789768/increase-the-date-by-number-of-months-in-pandas"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Why is there a duplicate index when using sort_index() in pandas? I am doing target mean mapping based on an external statistical table, where org_ is the external data and merged_data is the set of training data and test data. After processing, the features of ...",
    "url": "https://stackoverflow.com/questions/79789223/why-is-there-a-duplicate-index-when-using-sort-index-in-pandas"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Is there a way in Python to make a row of an HTML table multi-lined? I have a Python script that constructs a pandas DataFrame from API data, which I then convert to a pretty_html_table that will be the body of an email. In one of the rows, I have data containing an ...",
    "url": "https://stackoverflow.com/questions/79788916/is-there-a-way-in-python-to-make-a-row-of-an-html-table-multi-lined"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to efficiently denormalize a SQL DB to produce Parquet files I'm trying to create a parquet file from a heavily normalized SQL database with a snowflake schema. Some of the dimensions have very long text attributes so that a simply running a big set of joins to ...",
    "url": "https://stackoverflow.com/questions/79788168/how-to-efficiently-denormalize-a-sql-db-to-produce-parquet-files"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How do I calculate a relative time delta in Pandas? I have a column of datetimes and I want to get the difference between values in terms of years, months, etc, instead of timedeltas that only provide days. How do I do this in Pandas?\nPandas provides ...",
    "url": "https://stackoverflow.com/questions/79788108/how-do-i-calculate-a-relative-time-delta-in-pandas"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to combine multiple rows of Pandas dataframe into one row using a key [duplicate] I am trying to manipulate a CSV using Pandas and I need to get the data into the format of one row per ID.\nThis is an example of what I am trying to accomplish:\nFrom:\ndf = pd.DataFrame({\n'ID': [1, 1, ...",
    "url": "https://stackoverflow.com/questions/79787585/how-to-combine-multiple-rows-of-pandas-dataframe-into-one-row-using-a-key"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "python plotly scatter ols trendline has a kink in it I am using plotly express to model some data, and wanted to add a trendline = 'ols' to it.\nwhen I do, I obtain a kink in the result\n\nhere is the code used:\nd={'category': {63: 'test', 128: 'test', 192:...",
    "url": "https://stackoverflow.com/questions/79787051/python-plotly-scatter-ols-trendline-has-a-kink-in-it"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Using .loc to change a value in a pd.Dataframe with a variable as column name I need to change a value in a pd dataframe with .loc\nlet show with an example :\nimport pandas as pd\ndf = pd.DataFrame(data={\"A\":[\"bla\",\"bla2\"],\"B\":[1,2]})\n\nI ...",
    "url": "https://stackoverflow.com/questions/79786581/using-loc-to-change-a-value-in-a-pd-dataframe-with-a-variable-as-column-name"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Why does changing a DataFrame in one Jupyter cell also change another variable? [duplicate] I am working in Jupyter Notebook with pandas, and I noticed something strange.\nIn one cell , I did this:\nimport pandas as pd\n\ndf1 = pd.DataFrame({\"A\":[1,2,3]})\ndf2 = df1\n\nThen in another ...",
    "url": "https://stackoverflow.com/questions/79786055/why-does-changing-a-dataframe-in-one-jupyter-cell-also-change-another-variable"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Why doesn't Pandas concat do a copy when one of the dataframes is empty? Consider this example:\nimport pandas as pd\ndf_part1 = pd.DataFrame()\ndf_part2 = pd.DataFrame({'A': [1,1], 'B': [3,4]})\ndf_concat_out = pd.concat([df_part1, df_part2])\nprint(\"id(df_part2.values) ==...",
    "url": "https://stackoverflow.com/questions/79785937/why-doesnt-pandas-concat-do-a-copy-when-one-of-the-dataframes-is-empty"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Reconfigure a Pandas Dataframe [duplicate] Our old ERP system generates orphaned HTML reports with the following format which I import into Pandas\n   Work Order Item Type  Material  Labor\n0      552603    Budget     71119   4567\n1      552603  ...",
    "url": "https://stackoverflow.com/questions/79785896/reconfigure-a-pandas-dataframe"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Why does Pandas not recognise my sqlalchemy connection engine? I'm trying to connect to an IBM DB2 database from Python. I'm using Python 3.12.10, SQLAlchemy 1.4.54, and Pandas 2.3.2. This is what my code looks like:\nimport os\nimport sqlalchemy\nimport pandas as ...",
    "url": "https://stackoverflow.com/questions/79785397/why-does-pandas-not-recognise-my-sqlalchemy-connection-engine"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Importing a table from a webpage as a dataframe in Python I am trying to read in a specific table from the US Customs and Border Protection's Dashboard on Southwest Land Border Encounters as a dataframe.\nThe url is: https://www.cbp.gov/newsroom/stats/...",
    "url": "https://stackoverflow.com/questions/79784978/importing-a-table-from-a-webpage-as-a-dataframe-in-python"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to count pandas DataFrame cells using unique cell values as rows and columns [duplicate] Let's say I have a DataFrame df like this:\npd.DataFrame({'Planet':['Planet_1','Planet_1','Planet_2','Planet_2','Planet_3','Planet_3'],'FeatureType':['Lake','Lake','Crater','Volcano','Lake','Canyon'],'...",
    "url": "https://stackoverflow.com/questions/79784134/how-to-count-pandas-dataframe-cells-using-unique-cell-values-as-rows-and-columns"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to pivot a Pandas dataframe into the desired format? [closed] I have the following data in a dataframe:\n    Product t_Proj  CFType1 CFType2 CFType3\n0   Product1    0   270 193 130\n1   Product1    1   233 197 362\n2   Product1    2   130 278 375\n3   Product1    3  ...",
    "url": "https://stackoverflow.com/questions/79783950/how-to-pivot-a-pandas-dataframe-into-the-desired-format"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Plotly - add dropdown list or buttons to OHLC / Candlestick graph I have the following dataframe:\nlst = [['10/01/2025 8:30:00', 2.74, 2.87, 2.60, 2.65, 14, 'SPXW251001P06590000', 'P', 6590],\n       ['10/01/2025 8:31:00', 2.80, 2.80, 2.50, 2.53, 61, '...",
    "url": "https://stackoverflow.com/questions/79783318/plotly-add-dropdown-list-or-buttons-to-ohlc-candlestick-graph"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "How to efficiently handle lookups between resampled and original DataFrames in Pandas? I am building a backtesting project in Python using Pandas.\nI have:\n\nA large tick / 1-minute level DataFrame (df) with full market data.\n\nA 15-minute interval DataFrame (df_15) created from it using ...",
    "url": "https://stackoverflow.com/questions/79780994/how-to-efficiently-handle-lookups-between-resampled-and-original-dataframes-in-p"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Pandas dataFrame standard deviation issue I have this details.\nimport pandas as pd\nimport numpy as np\n\n# Sample dataset\ndata = {\n    \"date\": pd.date_range(\"2025-10-01\", periods=7),\n    \"sales\": [200, 220, 250, np....",
    "url": "https://stackoverflow.com/questions/79780371/pandas-dataframe-standard-deviation-issue"
  },
  {
    "library": "pandas",
    "source": "community",
    "text": "Different Date Formats In Same Column [closed] I've a dataset having a column: Joining_date with different date formats. I want to convert them in a one same date format\ndf['Joining_Date']\n0        2016-10-26\n2        16/08/2018\n3        2017/06/...",
    "url": "https://stackoverflow.com/questions/79780171/different-date-formats-in-same-column"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Speed of numpy.random.choice() against static dataset with differing probabilities [duplicate] I have a static data set I would like to sample from on the basis of differing probability distributions. I've found that using this in a for loop with numpy.random.choice() is quite slow, and I'm ...",
    "url": "https://stackoverflow.com/questions/79802197/speed-of-numpy-random-choice-against-static-dataset-with-differing-probabiliti"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Doing an assigment on image processing using python and google colab [closed] The first part of the assigment is to turn a picture of my face, edit it to grayscale and then seperatly add in a sobel, prewitt and canny edges. I have done the code and it runs but i would like any ...",
    "url": "https://stackoverflow.com/questions/79800271/doing-an-assigment-on-image-processing-using-python-and-google-colab"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Build numpy 2.3+ without accelerated libraries Related post: Compile numpy WITHOUT Intel MKL/BLAS/ATLAS/LAPACK\nRecent versions of numpy use meson for build configuration, I can build numpy from source but failed to exclude BLAS/LAPACK/... deps.\n...",
    "url": "https://stackoverflow.com/questions/79800025/build-numpy-2-3-without-accelerated-libraries"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "What does np.ones_like represent in a quiver plot of a differential equation? I found some similar questions about quiver plots and direction fields, but I’m still confused about the meaning of these two lines in many examples:\nWhat do dx = np.ones_like(f) and dy = f mean in a ...",
    "url": "https://stackoverflow.com/questions/79799821/what-does-np-ones-like-represent-in-a-quiver-plot-of-a-differential-equation"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Does `jax` compilation save runtime memory by recognizing array elements that are duplicated by indexing Consider the example code:\nfrom functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@partial(jit, static_argnums=(0,))\ndef my_function(n):\n\n    idx = jnp.tile(jnp.arange(n, dtype=int)...",
    "url": "https://stackoverflow.com/questions/79798175/does-jax-compilation-save-runtime-memory-by-recognizing-array-elements-that-ar"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "C++ app cannot find Numpy C API even though numpy is installed [closed] I am deploying a C++ application that at some point allows the user to load and run a Python script. For this, I am loading the C API of Python and Numpy. This is done by calling the function ...",
    "url": "https://stackoverflow.com/questions/79798166/c-app-cannot-find-numpy-c-api-even-though-numpy-is-installed"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to recover the component of mixtures after transforming the mixture to another distribution I have a mixture of generalized Pareto Distributions F(x) = 0.4 F1(x) + 0.6 F2(x) and another generalized extreme value distribution $G(x)$. I want to transform F(x) to match G(x) then obtain the ...",
    "url": "https://stackoverflow.com/questions/79796765/how-to-recover-the-component-of-mixtures-after-transforming-the-mixture-to-anoth"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Convert NETCDF files into TIF files I’m creating a Python program that converts NetCDF files into GeoTIFFs, allowing the user to select the variable and time step using a Tkinter interface.\nThe conversion works, but I’m having a ...",
    "url": "https://stackoverflow.com/questions/79796609/convert-netcdf-files-into-tif-files"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Why does NumPy load() take longer in a larger script than in another? I scan for a trigger and when I get it, load a .npy file and process it. It started to take almost 2 seconds to load the NumPy file from within the process but when I tried to load the same file from ...",
    "url": "https://stackoverflow.com/questions/79796042/why-does-numpy-load-take-longer-in-a-larger-script-than-in-another"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Obtaining cumulative product for a list elements I tried to calculate the cumulative products for my list elements as below. In my list, each element is two-dimensional list:\nimport numpy as np\n\nOrig_list = [[[1,2,3], [4,5,6], [7,8,9]], [[11,3,4], [...",
    "url": "https://stackoverflow.com/questions/79795854/obtaining-cumulative-product-for-a-list-elements"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to read a chunk of big tiff file in Python? [closed] I have a TIFF file with a size of (48000, 432000) and I would like to upload only a piece of the image, for example, pass a parameter like (X, Y, wid, hei), where X, Y are the coordinates of the upper-...",
    "url": "https://stackoverflow.com/questions/79794643/how-to-read-a-chunk-of-big-tiff-file-in-python"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to use sklearn imputation methods on numpy.void (record or structured array, I'm not sure) ndarray [closed] Code:\nimport numpy as np\nimport sklearn as skl\n\ndata = np.genfromtxt(\"water_potability.csv\", delimiter = \",\", names = True)\nprint(data)\nprint(data.shape)\nprint(type(data[0]))\n...",
    "url": "https://stackoverflow.com/questions/79794144/how-to-use-sklearn-imputation-methods-on-numpy-void-record-or-structured-array"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "NumPy high-dimensional array automatically rearranges after subsetting Today I wanted to use the following command to subset a numpy high-dimensional array, but was surprised to find that the two methods were completely different.I'm very curious why numpy reorders the ...",
    "url": "https://stackoverflow.com/questions/79794093/numpy-high-dimensional-array-automatically-rearranges-after-subsetting"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Implement a Convolution(Conv2D) in Numpy [closed] what I have  just selecting a kernel size (3X3,5X5) and computing a (elementwise) product of the outcome from padded matrix with respect to kernel like if kernel size = 3*3 then we have output[0][0]=...",
    "url": "https://stackoverflow.com/questions/79794072/implement-a-convolutionconv2d-in-numpy"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Image stacking using xarray [closed] I am trying to stack images of different bands i.e SAR+RGB+slope+ MNDWI. I am working in Python. When I stack all images then normalize it the mean values of rgb.nir and slope come out to very large ...",
    "url": "https://stackoverflow.com/questions/79793629/image-stacking-using-xarray"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to prevent overlapping contours for periodic values in matplotlib The example code creates a 2D visualization of angles relative to the center. Contour lines with labels are added to show lines of constant angle.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn ...",
    "url": "https://stackoverflow.com/questions/79792925/how-to-prevent-overlapping-contours-for-periodic-values-in-matplotlib"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Divide by zero encountered in matmul on MacOS M4 with numpy v2.0.0 I'm encountering a strange RuntimeWarning: divide by zero encountered in matmul when performing a simple matrix multiplication on my new Apple M4 machine.\nThe most peculiar part is that this warning ...",
    "url": "https://stackoverflow.com/questions/79792627/divide-by-zero-encountered-in-matmul-on-macos-m4-with-numpy-v2-0-0"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Numpy vectorising n-variable function by index Given a closed-form function f(x,y,z), what is the most efficient way to fill up a 3D ndarray arr[z][y][x] with the values of f(x,y,z)?\nFor example, if I wanted a 3D ndarray arr[z][y][x] with each ...",
    "url": "https://stackoverflow.com/questions/79787909/numpy-vectorising-n-variable-function-by-index"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Python multiprocessing shared memory seems to hang or crash when interacting with small numpy arrays I've been trying to parallelize some code that I wrote in python. The actual work is embarrassingly parallel, but I don't have much experience with multiprocessing in Python.\nThe actual code I'm ...",
    "url": "https://stackoverflow.com/questions/79787708/python-multiprocessing-shared-memory-seems-to-hang-or-crash-when-interacting-wit"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "FFT-based quasi-steady detection issue I am trying to detect the beginning of a quasi-steady regime in time series data representing drag (Fx) and lift (Fy) forces after an initial transient.\nInitially, I used a slope-based method, but it ...",
    "url": "https://stackoverflow.com/questions/79787338/fft-based-quasi-steady-detection-issue"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Shape of sliced array from shape of array and slice If I know the shape of a numpy array like (1000, 50), and I have an arbitrary selection expressed as an IndexExpression, let's say np.s_[:200, :], how can I evaluate the shape of the sliced array (in ...",
    "url": "https://stackoverflow.com/questions/79785678/shape-of-sliced-array-from-shape-of-array-and-slice"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "What types are required for numpy.abs()? [closed] I am trying to make an app in python that takes in equations and outputs them in a graph or solves the equation(s). However in the equation-parsing function, I cannot work out which types are required ...",
    "url": "https://stackoverflow.com/questions/79785289/what-types-are-required-for-numpy-abs"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Numpy random normal test shows unexpected results with low p-value I am performing some tests on the normality of the numpy random generator. Running the following code, the stats.normaltest shows some seeds with low pvalue (which highlights non-normal distribution).\n...",
    "url": "https://stackoverflow.com/questions/79784932/numpy-random-normal-test-shows-unexpected-results-with-low-p-value"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "RuntimeError: Numpy is not available when running Streamlit + PyTorch + Torchvision app on Streamlit Cloud `I’m deploying a Streamlit app on Streamlit Cloud that uses PyTorch and Torchvision for image segmentation.\nLocally, everything works fine, but when I deploy on Streamlit Cloud I get this error:\nFile &...",
    "url": "https://stackoverflow.com/questions/79779849/runtimeerror-numpy-is-not-available-when-running-streamlit-pytorch-torchvis"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to make index into exponential notation? I've tried to put in the NumPy scientific notation function in different places but that didn't work, other than that I haven't been able to find anything that works\ndef optimal_resistor(name=\"&...",
    "url": "https://stackoverflow.com/questions/79779335/how-to-make-index-into-exponential-notation"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Use-case for interpolation (pandas) and linear space (numpy) What is the difference between using an interpolation in the pandas package and the linear space of the numpy package? For instance in the following:\nimport numpy as np\nimport pandas as pd\nIn [12]: np....",
    "url": "https://stackoverflow.com/questions/79777562/use-case-for-interpolation-pandas-and-linear-space-numpy"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to save violin plot object as npy or npz files I am plotting multiple violin plots for a large dataset and it takes so long to generate one figure. I would like to save the violin plotting objects as npy or npz, and plot them separately. Is this ...",
    "url": "https://stackoverflow.com/questions/79776925/how-to-save-violin-plot-object-as-npy-or-npz-files"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Tuples index when using Ellipsis in a numpy array I have a code that loops on tuples indexes:\nfor _ip in np.ndindex(params_shape):\n    ...\n    loglik[_ip, ...] = ...\n\nIf my tuple is (0, 1), I would like the last line to access loglik[0, 1, ...]. ...",
    "url": "https://stackoverflow.com/questions/79773893/tuples-index-when-using-ellipsis-in-a-numpy-array"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Error serializing numpy.int64 into JSON with Python I’m developing a Python script that collects “snapshots” of my data at different points in time and saves them into JSON files for later analysis. I want to store each snapshot as a single line in a ...",
    "url": "https://stackoverflow.com/questions/79770654/error-serializing-numpy-int64-into-json-with-python"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Why is my code for generating a curve fit of a cosine to a sum of gaussians working so poorly? I have this code in which I'm trying to use scipy.optimize.curve_fit() to take a cos^2(x) function and approximate it as a sum of gaussian peaks. However, it is generating a very poor fit and I can't ...",
    "url": "https://stackoverflow.com/questions/79770032/why-is-my-code-for-generating-a-curve-fit-of-a-cosine-to-a-sum-of-gaussians-work"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Saving numpy arrays to postgresql efficiently I need a fast and simple way of storing numpy arrays to a postgresql database, but I'm not sure if this a reasonable way of doing this. Preferably to several different specified tables. Is there a ...",
    "url": "https://stackoverflow.com/questions/79769803/saving-numpy-arrays-to-postgresql-efficiently"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Why does my log transformation output only black? [closed] Here is my original code:\n# Formula: s = c * log(1 + r)\nc = 255 / np.log(1 + np.max(image))\nlog_transformed = c * np.log(1 + image.astype(np.float32))\nlog_transformed = np.clip(log_transformed, 0, 255)...",
    "url": "https://stackoverflow.com/questions/79768280/why-does-my-log-transformation-output-only-black"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Converting 2d tiff into 3d tif files - stack error I'm working in WSL Ubuntu, writing this script in SPAM Jupyter Lab. I want to convert my 2d TIFF files into a 3d TIFF file. After stating the directory where the 2d TIFF files are, this is my code:\n...",
    "url": "https://stackoverflow.com/questions/79767179/converting-2d-tiff-into-3d-tif-files-stack-error"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Get last non-zero index in a vector I have a 1D NumPy array, and I want to find the index of the last non-zero element. The vector consists only of 1s and 0s, not sure if this is relevant information.\nFor example:\nimport numpy as np\n\n...",
    "url": "https://stackoverflow.com/questions/79766380/get-last-non-zero-index-in-a-vector"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Type hint two function parameters to constrain them both to the same type and reject types like numpy.ndarray which do not support boolean comparison I would like to annotate both parameters of a function – supposed to perform simple comparison between them ­– in order to indicate that their types should be the same and should support simple ...",
    "url": "https://stackoverflow.com/questions/79765976/type-hint-two-function-parameters-to-constrain-them-both-to-the-same-type-and-re"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Extreme trends in GSL (Growing Season Length) due to missing years in ERA5-Land based calculation I calculated the Growing Season Length (GSL) index for the 1950–2023 period in Turkey using ERA5-Land daily mean temperature data. According to the definition:\n\nFirst, find the first occurrence of at ...",
    "url": "https://stackoverflow.com/questions/79765956/extreme-trends-in-gsl-growing-season-length-due-to-missing-years-in-era5-land"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Why isn't my Hermitian covariance matrix invertible? I am following a paper that uses a Hermitian covariance matrix\n\nand inverts it to produce a Fisher matrix. I construct my covariance as Gamma[i,m,n], stored inside a larger array of shape (n_z, n_k, ...",
    "url": "https://stackoverflow.com/questions/79765646/why-isnt-my-hermitian-covariance-matrix-invertible"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to do batched matrix vector multiplication with np.matvec I have rotation matrix, for the sake of a simple example, np.ndarray(..., shape=(3, 2)), and I want to multiply it by an nd-array of vectors, say np.mgrid[:4, :5], how do I do that with the new np....",
    "url": "https://stackoverflow.com/questions/79762332/how-to-do-batched-matrix-vector-multiplication-with-np-matvec"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "SC mapping simple connected region I am trying to use Schwarz-Christoffel formula to map a n-polygon onto a unit circle. To do this, the parameter problem needs to be solved, which is non linear for which I am using least squares. The ...",
    "url": "https://stackoverflow.com/questions/79761417/sc-mapping-simple-connected-region"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Best way to assign a scalar to a new DataFrame column with a specific dtype I am writing a routine to load a large dataset into a Pandas DataFrame from a bespoke text format.\nAs part of this process, I need to add new columns to a DataFrame. Sometimes I need to broadcast a ...",
    "url": "https://stackoverflow.com/questions/79761269/best-way-to-assign-a-scalar-to-a-new-dataframe-column-with-a-specific-dtype"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How does Jax turn Python code into Jaxpr? I want to know how Jax turns Python functions into Jaxprs when doing JIT and other operations. Some simple operations seem straight forward, but how does it compile more advanced NumPy functions like ...",
    "url": "https://stackoverflow.com/questions/79760386/how-does-jax-turn-python-code-into-jaxpr"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "How to efficiently create an array from a list containing arrays of different lengths I have a list containing 2D arrays with same number of rows but different number of columns. I need to create a padded array of arrays with same shape. My current code is below but due to the for loop,...",
    "url": "https://stackoverflow.com/questions/79758933/how-to-efficiently-create-an-array-from-a-list-containing-arrays-of-different-le"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Manipulating a large dataframe most efficiently Imagine I have this dataframe called temp:\ntemp = pd.DataFrame(index = [x for x in range(0, 10)], columns = list('abcd'))\nfor row in temp.index:\n        temp.loc[row] = default_rng().choice(10, size=4,...",
    "url": "https://stackoverflow.com/questions/79757853/manipulating-a-large-dataframe-most-efficiently"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "f2py from numpy 2.3.2: Linking a fortran 77 code with many subroutines in separate files fails I have a large fortran 77 code, \"NSCool\", distributed among many files which I compile into a python package using f2py. It used to work nicely with python 3.7, then it took me weeks to have ...",
    "url": "https://stackoverflow.com/questions/79757716/f2py-from-numpy-2-3-2-linking-a-fortran-77-code-with-many-subroutines-in-separa"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Weird behavior in large complex128 NumPy arrays, imaginary part only [closed] I'm working on numerical simulations. I ran into an issue with large NumPy arrays (~ 26 GB) on Linux with 128 GB of RAM. The arrays are of type complex128.\n\nArrays are instantiated without errors (if ...",
    "url": "https://stackoverflow.com/questions/79757282/weird-behavior-in-large-complex128-numpy-arrays-imaginary-part-only"
  },
  {
    "library": "numpy",
    "source": "community",
    "text": "Making numpy array of positions [duplicate] Given a shape, how do I make a numpy array whose value at each position/location/index is the coordinates of that position/location/index?\nFor example, if I had shape = (5,8,7) I want out_array where ...",
    "url": "https://stackoverflow.com/questions/79757263/making-numpy-array-of-positions"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "MLflow doesn’t log or show model artifacts after training run I’m working on a machine learning project using MLflow for experiment tracking (on macOS, Python 3.12, scikit-learn, and DagsHub as the tracking server). The experiment runs successfully — I see the ...",
    "url": "https://stackoverflow.com/questions/79796081/mlflow-doesn-t-log-or-show-model-artifacts-after-training-run"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Shape of tree_.value According to the sklearn docs the shape of tree_.value is  [n_nodes, n_classes, n_outputs]. I just wanted to ask if this is still correct.\nI think the correct shape is  [n_nodes, n_outputs, n_classes] ...",
    "url": "https://stackoverflow.com/questions/79783003/shape-of-tree-value"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "PCA with arpack returns different values when the order of observations change, but why? I have recently noticed that when I change the order of the observations in a sparse array, scikit-learn PCA with svd_solver=\"arpack\" returns different floating point numbers. Is this an ...",
    "url": "https://stackoverflow.com/questions/79779783/pca-with-arpack-returns-different-values-when-the-order-of-observations-change"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Qiskit Problem: Why does this not work with COBYLA and how do i optimize it? I'm doing a small program that is supposed to classify the data of the Wisconsin Breast Cancer database contained in sklearn.datasets using Quantum Neural Networks (specifically EstimatorQNN).\nI think ...",
    "url": "https://stackoverflow.com/questions/79767006/qiskit-problem-why-does-this-not-work-with-cobyla-and-how-do-i-optimize-it"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "How to correct 3rd party sphinx ambiguous cross-reference warnings? I'm trying to document a variety of classes that use scikit-learn bases BaseEstimator and TransformerMixin.  Sphinx builds with a warning that,\n/home/jake/github/proj/pkg/__init__.py:docstring of \n...",
    "url": "https://stackoverflow.com/questions/79763327/how-to-correct-3rd-party-sphinx-ambiguous-cross-reference-warnings"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "n_jobs>=2 breaks reproducibility I am facing a problem in maintaining the reproducibility in the ML project. I believe the core snippet of my issue is\nclf = Clf(random_state=cfg.seed)\n    # instantiate the K-fold cross-validation ...",
    "url": "https://stackoverflow.com/questions/79759086/n-jobs-2-breaks-reproducibility"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Flask ML App Stuck on \"Loading\" Status Despite Successful Model Training I'm deploying a Flask ML application with book recommendations to Render, but I'm experiencing a persistent issue where my health endpoint always returns \"model_loaded\": false, \"status&...",
    "url": "https://stackoverflow.com/questions/79750977/flask-ml-app-stuck-on-loading-status-despite-successful-model-training"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Brier Skill Score returns NaN in cross_val_score with imbalanced dataset I’m trying to evaluate classification models on a highly imbalanced fraud dataset using the Brier Skill Score (BSS) as the evaluation metric.\n\nThe dataset has ~2133 rows and the target Fraud_Flag is ...",
    "url": "https://stackoverflow.com/questions/79749078/brier-skill-score-returns-nan-in-cross-val-score-with-imbalanced-dataset"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "how to pass pre-computed folds to successiveHalving in sklearn I want to undersample 3 cross-validation folds from a dataset, using say, RandomUnderSampler from imblearn, and then, optimize the hyperparameters of various gbms using those undersampled folds as ...",
    "url": "https://stackoverflow.com/questions/79748461/how-to-pass-pre-computed-folds-to-successivehalving-in-sklearn"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Evaluate transformations with the same model in scikit-learn I would like to perform a regression analysis and test different transformations of the input variables for the same model. To accomplish this, I created a dictionary with the different pipelines, ...",
    "url": "https://stackoverflow.com/questions/79748223/evaluate-transformations-with-the-same-model-in-scikit-learn"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why am I getting ModuleNotFoundError: No module named 'numpy._utils' when importing scikit-learn or TensorFlow on Python 3.12? I’m trying to run Python scripts that import scikit-learn and TensorFlow, but I keep running into a NumPy-related error.\n\ncode example:\nfrom packaging import version\nimport sklearn\n\nassert version....",
    "url": "https://stackoverflow.com/questions/79743297/why-am-i-getting-modulenotfounderror-no-module-named-numpy-utils-when-import"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "ValueError when fitting model: Expected 2D array, got 1D array instead import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nX = np.array([1, 2, 3, 4, 5])  # Features\ny = np.array([2, 4, 6, 8, 10])  # Target\nmodel = LinearRegression()\nmodel.fit(X, y)  # &...",
    "url": "https://stackoverflow.com/questions/79733937/valueerror-when-fitting-model-expected-2d-array-got-1d-array-instead"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Version unsupported imbalanced-learn I am trying to install scikit-learn and imbalanced-learn for ML project using poetry.\n# File pyproject.toml\n\n[project]\nname = \"hello\"\nversion = \"0.1.0\"\ndescription = \"\"\n...",
    "url": "https://stackoverflow.com/questions/79730533/version-unsupported-imbalanced-learn"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Plot confusion matrix in black and white I currently have the following code:\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.show()\n\nThis results into something like:\n\nHowever, I want the diagonal to be depicted with a ...",
    "url": "https://stackoverflow.com/questions/79696741/plot-confusion-matrix-in-black-and-white"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Reproduce a particular tree from the random forest using DecisionTreeRegressor I am trying to replicate a specific decision tree trained by a RandomForestRegressor class, using DecisionTreeRegressor.\nHowever, I cannot get the exact results, even with using the exact same ...",
    "url": "https://stackoverflow.com/questions/79696701/reproduce-a-particular-tree-from-the-random-forest-using-decisiontreeregressor"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Ridge Polynomial Regression: How to get parameters for equation found I've used sklearn for polynomial ridge regression.  Using grid search, I am happy with the results.  Now I would like to render it as a simple polynomial equation to run in a small python module.  The ...",
    "url": "https://stackoverflow.com/questions/79691953/ridge-polynomial-regression-how-to-get-parameters-for-equation-found"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Building a sklearn compatible estimator: 'dict' object has no attribute 'requires_fit' I am trying to build a scikit-learn compatible estimator. I have built a custom class that inherits from BaseEstimator and RegressorMixin. However, when I try to use this, I run into an AttributeError:...",
    "url": "https://stackoverflow.com/questions/79678325/building-a-sklearn-compatible-estimator-dict-object-has-no-attribute-require"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why does the y-intercept change when all features are used in a linear regression model vs only one feature? [closed] I'm training a linear regression on the \"advertising sales dataset\"\nWhen I train all of the features(columns) of the dataset together, I get a different y-intercept than if I trained the ...",
    "url": "https://stackoverflow.com/questions/79677154/why-does-the-y-intercept-change-when-all-features-are-used-in-a-linear-regressio"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Rancom Forest Classifier model returns all zeroes I'm trying to train a RandomForestClassifier Model. However, when I train it, it gives me all zeroes. And, I really can't seem to understand why. The dataset is HUGE (close to like 75,0000 rows), so, ...",
    "url": "https://stackoverflow.com/questions/79669783/rancom-forest-classifier-model-returns-all-zeroes"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why does sklearn GaussianMixtures spawn a cmd window when run from IDLE? When I run this code using IDLE in windows, it spawns a temporary cmd window that disappears shortly thereafter.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import ...",
    "url": "https://stackoverflow.com/questions/79657196/why-does-sklearn-gaussianmixtures-spawn-a-cmd-window-when-run-from-idle"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Runtime warning in sklearn KMeans I am running k-means using sklearn but has been getting runtime warning. Can you please explain what's happening? Below is a sample code for reproducibility:\nimport pandas as pd\nimport numpy as np\n...",
    "url": "https://stackoverflow.com/questions/79630857/runtime-warning-in-sklearn-kmeans"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Grid search scikeras error : 'super' object has no attribute '__sklearn_tags__' I am trying to run a gridsearch in my pycharm jupyter notebook.\nGetting 'super' object has no attribute '__sklearn_tags__'   error. This question was asked for RandomizedSearch previously, but none of ...",
    "url": "https://stackoverflow.com/questions/79628711/grid-search-scikeras-error-super-object-has-no-attribute-sklearn-tags"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "I am unable to import scikit module, using jupyter notebook on VScode I am doing an assignment whose one part asks me to implement OLS using scikit. In VScode, whenever i run the code, it asks for a kernel, i chose the one which is installed in my original anaconda ...",
    "url": "https://stackoverflow.com/questions/79626619/i-am-unable-to-import-scikit-module-using-jupyter-notebook-on-vscode"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Is it possible to kill specific coeffients in polynomial regression model? I need to make a multivariate polynomial regression. The code is based on https://saturncloud.io/blog/multivariate-polynomial-regression-with-python/.\nSo for my specific task I need to \"kill\"...",
    "url": "https://stackoverflow.com/questions/79622840/is-it-possible-to-kill-specific-coeffients-in-polynomial-regression-model"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "How to set a fixed random state in RandomizedSearchCV? I'm using RandomizedSearchCV with RandomForestClassifier in scikit-learn. I want to make sure my results are reproducible across runs. Where should I set the random_state—in the classifier, in ...",
    "url": "https://stackoverflow.com/questions/79603555/how-to-set-a-fixed-random-state-in-randomizedsearchcv"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why does RandomizedSearchCV sometimes return worse results than manual tuning in scikit-learn? I'm working on a classification problem using scikit-learn's RandomForestClassifier. I tried using RandomizedSearchCV for hyperparameter tuning, but the results were worse than when I manually set the ...",
    "url": "https://stackoverflow.com/questions/79603404/why-does-randomizedsearchcv-sometimes-return-worse-results-than-manual-tuning-in"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why is DecisionTree using same feature and same condition twice When trying to fit scikit-learn DecisionTreeClassifier on my data, I am observing some weird behavior.\nx[54] (a boolan feature) is used to break the 19 samples into 2 and 17 on top left node. Then ...",
    "url": "https://stackoverflow.com/questions/79602705/why-is-decisiontree-using-same-feature-and-same-condition-twice"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "do i need to scale the rf model while creating voting ensemble model? [closed] I'm building a classification model for sleep disorders using Voting Ensemble and I have three base models: Logistic Regression, Random Forest and SVM.\nNow I want to combine these models using a ...",
    "url": "https://stackoverflow.com/questions/79601928/do-i-need-to-scale-the-rf-model-while-creating-voting-ensemble-model"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Stratification fails in train_test_split Please consider the following code:\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# step 1\nids = list(range(1000))\nlabel = 500 * [1.0] + 500 * [0.0]\ndf = pd.DataFrame({&...",
    "url": "https://stackoverflow.com/questions/79595277/stratification-fails-in-train-test-split"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "HalvingGridSearchCV cannot fit multi label DecisionTreeClassifier I'm trying to use HalvingGridSearch to find the best DecisionTree model. My model performs a multi-label prediction on a single example, it is trained on a batch of data of size (n_samples x ...",
    "url": "https://stackoverflow.com/questions/79592409/halvinggridsearchcv-cannot-fit-multi-label-decisiontreeclassifier"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Python Sklearn.Model_Selection giving error numpy.dtype size changed I have a train test split code\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(new_cleaned_df, test_size=0.05, random_state=42, shuffle=True)\ntrain_df....",
    "url": "https://stackoverflow.com/questions/79586934/python-sklearn-model-selection-giving-error-numpy-dtype-size-changed"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Linear regression prediction does not display properly I want to make 2 different linear regressions for 2 diferent plots, but on the same figure. I have a problem with the y1_pred because it does not go for all the y axis where are scatters.\nmodel1 = ...",
    "url": "https://stackoverflow.com/questions/79586660/linear-regression-prediction-does-not-display-properly"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Keras SKLearnClassifier wrapper can't fit MNIST data I'm trying to use the SKLearnClassifier Keras wrapper to do some grid searching and cross validation using the sklearn library but I'm unable to get the model to work properly.\ndef build_model(X, y, ...",
    "url": "https://stackoverflow.com/questions/79583640/keras-sklearnclassifier-wrapper-cant-fit-mnist-data"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "How to fit scaler for different subsets of rows depending on group variable and include it in a Pipeline? I have a data set like the following and want to scale the data using any of the scalers in sklearn.preprocessing.\nIs there an easy way to fit this scaler not over the whole data set, but per group? ...",
    "url": "https://stackoverflow.com/questions/79577490/how-to-fit-scaler-for-different-subsets-of-rows-depending-on-group-variable-and"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Confirm understanding of decision_function in Isolation Forest I am looking to better understand sklearn IsolationForest decision_function. My understanding from this previous stack overflow post,  What is the difference between decision function and ...",
    "url": "https://stackoverflow.com/questions/79576028/confirm-understanding-of-decision-function-in-isolation-forest"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why does RandomForestClassifier in scikit-learn predict even on all-NaN input? I am training a random forest classifier in python sklearn, see code below-\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X = df.drop(\"...",
    "url": "https://stackoverflow.com/questions/79575941/why-does-randomforestclassifier-in-scikit-learn-predict-even-on-all-nan-input"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "reg.predict is telling me I am not providing an array It seems I have an issue with an array that I thought I coded correctly. When I ask for reg.score or reg.coef_ the code works great, but when I try to predict it throws an error that is saying it is ...",
    "url": "https://stackoverflow.com/questions/79570774/reg-predict-is-telling-me-i-am-not-providing-an-array"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Get analytical equation of RF regressor model [duplicate] I have the following dataset:\n         X1        X2        X3         y\n0  0.548814  0.715189  0.602763  0.264556\n1  0.544883  0.423655  0.645894  0.774234\n2  0.437587  0.891773  0.963663  0.456150\n3  ...",
    "url": "https://stackoverflow.com/questions/79559492/get-analytical-equation-of-rf-regressor-model"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Different Feature Selection Results Between Local (Ubuntu VM) and Databricks Using sklearn's SequentialFeatureSelector I am migrating from running my machine learning pipeline in VS Code with Ubuntu on a VM into Databricks. When I test the same dataset using the same code, I get different selected features from ...",
    "url": "https://stackoverflow.com/questions/79541286/different-feature-selection-results-between-local-ubuntu-vm-and-databricks-usi"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "TabPFN feature selection raises KeyError(f\"None of [{key}] are in the [{axis_name}]\") I trained a tabPFN model, which I then tried applying a sequential feature selector for important feature selection. I've been getting this error\nKeyError(f\"None of [{key}] are in the [{axis_name}...",
    "url": "https://stackoverflow.com/questions/79529836/tabpfn-feature-selection-raises-keyerrorfnone-of-key-are-in-the-axis-nam"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why does SequentialFeatureSelector return at most \"n_features_in_ - 1\" predictors? I have a training dataset with six features and I am using SequentialFeatureSelector to find an \"optimal\" subset of the features for a linear regression model. The following code returns ...",
    "url": "https://stackoverflow.com/questions/79528929/why-does-sequentialfeatureselector-return-at-most-n-features-in-1-predictor"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "How to load a Neural Network Model along with MinMaxScalar? [duplicate] I have a simple neural network model, of 4 layers, that I trained on a numerical dataset of 25K data points.\nIt takes a good time to load the data, whenever I want to evaluate new features to python ...",
    "url": "https://stackoverflow.com/questions/79518764/how-to-load-a-neural-network-model-along-with-minmaxscalar"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Define a custom tree splitter from sklearn I'm trying to define a custom splitter using sklearn Classification Trees classes, but I'm getting no results so far. I got no errors but the tree is not developed. How to achieve this?\nMy strategy is ...",
    "url": "https://stackoverflow.com/questions/79517202/define-a-custom-tree-splitter-from-sklearn"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Length of features is not equal to the length of SHAP Values Im running a random forest model and to get some feature importance and Im trying to run a SHAP analysis. The problem is that every time I try to plot the shap values, I keep getting this error:\n...",
    "url": "https://stackoverflow.com/questions/79515542/length-of-features-is-not-equal-to-the-length-of-shap-values"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "How is each tree within DecisionTreeClassifier calculating probability of a class? According to the sklearn docs, if you apply predict_proba to DecisionTreeClassifier:\n\nThe predicted class probability is the fraction of samples of the same\nclass in a leaf.\n\nLet's say that the rows ...",
    "url": "https://stackoverflow.com/questions/79504108/how-is-each-tree-within-decisiontreeclassifier-calculating-probability-of-a-clas"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Nested parallelism with GridSearchCV causes infinite hang I'm running a GridSearchCV optimization into a parallelized function. The pseudocode looks like this\nfrom tqdm.contrib.concurrent import process_map\nfrom sklearn.model_selection import GridSearchCV\n\n...",
    "url": "https://stackoverflow.com/questions/79489137/nested-parallelism-with-gridsearchcv-causes-infinite-hang"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "'numpy.ndarray' object has no attribute 'groupby' I am trying to apply target encoding to categorical features using the category_encoders.TargetEncoder in Python. However, I keep getting the following error:\nAttributeError: 'numpy.ndarray' object ...",
    "url": "https://stackoverflow.com/questions/79483002/numpy-ndarray-object-has-no-attribute-groupby"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Pipeline FutureWarning: This Pipeline instance is not fitted yet [closed] I am working on a fairly simple machine learning problem in the form of a practicum. I am using the following code to preprocess the data:\nfrom preprocess.date_converter import DateConverter\nfrom ...",
    "url": "https://stackoverflow.com/questions/79475986/pipeline-futurewarning-this-pipeline-instance-is-not-fitted-yet"
  },
  {
    "library": "scikit-learn",
    "source": "community",
    "text": "Why VotingClassifer performance with voting set to \"hard\" is different with different weights? I wanted to test VotingClassifier from sklearn and comparae performance with different parameters. I used param grid and then I notice something unintelligible.\nI prepared three classifiers\ngnb = ...",
    "url": "https://stackoverflow.com/questions/79474361/why-votingclassifer-performance-with-voting-set-to-hard-is-different-with-diff"
  }
]